{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f506f8a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-unnecessary-columns-from-datasets-for-easier-manipulation.\" data-toc-modified-id=\"Remove-unnecessary-columns-from-datasets-for-easier-manipulation.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Remove unnecessary columns from datasets for easier manipulation.</a></span></li><li><span><a href=\"#Clean-movie-data-using-the-clean_movie_data()-function,-which-includes:\" data-toc-modified-id=\"Clean-movie-data-using-the-clean_movie_data()-function,-which-includes:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Clean movie data using the clean_movie_data() function, which includes:</a></span></li><li><span><a href=\"#Clean-monetary-columns-using-the-clean_money_columns()-function,-which:\" data-toc-modified-id=\"Clean-monetary-columns-using-the-clean_money_columns()-function,-which:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Clean monetary columns using the clean_money_columns() function, which:</a></span></li></ul></li><li><span><a href=\"#Title-Key-Creation-and-Application\" data-toc-modified-id=\"Title-Key-Creation-and-Application-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Title Key Creation and Application</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-title-key-using-the-create_title_key_file()-function,-which:\" data-toc-modified-id=\"Create-a-title-key-using-the-create_title_key_file()-function,-which:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Create a title key using the create_title_key_file() function, which:</a></span></li><li><span><a href=\"#Add-corresponding-title-IDs-to-each-dataset-using-the-add_title_ids()-function,-which:\" data-toc-modified-id=\"Add-corresponding-title-IDs-to-each-dataset-using-the-add_title_ids()-function,-which:-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Add corresponding title IDs to each dataset using the add_title_ids() function, which:</a></span></li></ul></li><li><span><a href=\"#Dataset-Compare/Update\" data-toc-modified-id=\"Dataset-Compare/Update-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dataset Compare/Update</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compare-and-update-datasets-with-corresponding-title-IDs-using-the-add_title_id()-function,-which:\" data-toc-modified-id=\"Compare-and-update-datasets-with-corresponding-title-IDs-using-the-add_title_id()-function,-which:-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Compare and update datasets with corresponding title IDs using the add_title_id() function, which:</a></span></li><li><span><a href=\"#Delete-files,-drop-columns,-rename-columns\" data-toc-modified-id=\"Delete-files,-drop-columns,-rename-columns-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Delete files, drop columns, rename columns</a></span></li></ul></li><li><span><a href=\"#Create-reference-files-for-easier-comparison\" data-toc-modified-id=\"Create-reference-files-for-easier-comparison-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create reference files for easier comparison</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-a-file-for-directors,-actors,-and-genres-will-make-the-analysis-easier\" data-toc-modified-id=\"Creating-a-file-for-directors,-actors,-and-genres-will-make-the-analysis-easier-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Creating a file for directors, actors, and genres will make the analysis easier</a></span></li><li><span><a href=\"#Removing-rows-where-there-is-only-1-title_id\" data-toc-modified-id=\"Removing-rows-where-there-is-only-1-title_id-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Removing rows where there is only 1 title_id</a></span></li></ul></li><li><span><a href=\"#Create-an-updated-title_id_key.csv\" data-toc-modified-id=\"Create-an-updated-title_id_key.csv-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create an updated title_id_key.csv</a></span></li><li><span><a href=\"#Data-File-Split\" data-toc-modified-id=\"Data-File-Split-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Data File Split</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-each-dataset-split-the-files-to-prepare-for-completed-datasets:\" data-toc-modified-id=\"For-each-dataset-split-the-files-to-prepare-for-completed-datasets:-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>For each dataset split the files to prepare for completed datasets:</a></span></li></ul></li><li><span><a href=\"#Cleaning-groups-of-data-from-csv-files-date-columns-and-formats\" data-toc-modified-id=\"Cleaning-groups-of-data-from-csv-files-date-columns-and-formats-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Cleaning groups of data from csv files date columns and formats</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#fixing-dates-and-addressing-money-and-percent-columns-for-the-summary-files.\" data-toc-modified-id=\"fixing-dates-and-addressing-money-and-percent-columns-for-the-summary-files.-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>fixing dates and addressing money and percent columns for the summary files.</a></span></li></ul></li></ul></li><li><span><a href=\"#Final-Preprocessing\" data-toc-modified-id=\"Final-Preprocessing-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Final Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Create-Review-Summary-Key-(review_summary_key.csv):\" data-toc-modified-id=\"1.-Create-Review-Summary-Key-(review_summary_key.csv):-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>1. Create Review Summary Key (review_summary_key.csv):</a></span></li><li><span><a href=\"#2.-Create-Movie-Data-Summary-Key-(movie_data_summary_key.csv):\" data-toc-modified-id=\"2.-Create-Movie-Data-Summary-Key-(movie_data_summary_key.csv):-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>2. Create Movie Data Summary Key (movie_data_summary_key.csv):</a></span></li><li><span><a href=\"#Filter-directors,-actors,-and-genres-reference-datasets\" data-toc-modified-id=\"Filter-directors,-actors,-and-genres-reference-datasets-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Filter directors, actors, and genres reference datasets</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d09468",
   "metadata": {},
   "source": [
    "The Jupyter notebook is aimed at cleaning and merging movie datasets to create a more cohesive dataset. The cleaning process is done using the 'clean_movie_data()' function that applies several cleaning steps to each file, including converting column names to snake_case, updating columns with spaces, and creating release year/month/day columns.\n",
    "\n",
    "The 'clean_money_columns()' and 'clean_money_string()' functions are helper functions used to clean currency values in the dataset. Once the data is cleaned, the 'add_title_ids()' function is used to merge the datasets using a title ID key to minimize the number of datasets and rows being worked with for better computation.\n",
    "\n",
    "The notebook also includes a section on creating a title key to ensure consistency across multiple datasets. The 'create_title_key_file()' function creates a DataFrame with the title and the number of files it appears in and filters it to keep only titles that appear in at least two files.\n",
    "\n",
    "The 'add_title_id()' function merges two datasets with their respective main datasets by adding a 'title_id' column to the extras datasets. The function is called twice, once for 'movies.csv' and 'movies_extras.csv' and the second time for 'movie_data.csv' and 'movie_data_extras.csv'. The output is saved in the 'cl_extras_reviews' folder.\n",
    "\n",
    "In conclusion, the Jupyter notebook provides an end-to-end solution to cleaning and merging movie datasets to create a cohesive dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163f87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common_imports.ipynb\n",
      "importing Jupyter notebook from capstone_functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from common_imports import *\n",
    "from capstone_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62c514",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Remove unnecessary columns from datasets for easier manipulation.\n",
    "\n",
    "### Clean movie data using the clean_movie_data() function, which includes:\n",
    "- Converting column names to snake_case\n",
    "- Cleaning 'title' columns to lowercase with no punctuation or special characters\n",
    "- Creating 'release_year', 'release_month', and 'release_day' columns from 'release_date'\n",
    "- Removing rows with duplicate titles and years, keeping the row with fewest null values\n",
    "- Dropping rows where the release date or year is outside the range of 2002-2022\n",
    "\n",
    "### Clean monetary columns using the clean_money_columns() function, which:\n",
    "- Cleans values in specified columns to remove non-numeric characters and apply multipliers\n",
    "\n",
    "Save cleaned data to new CSV files in output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e02a3f",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns for movies_dataset.csv: ['Id', 'title', 'Creative Type', 'Domestic Box Office', 'Franchise', 'Genre', 'International Box Office', 'Keywords', 'Languages', 'MPAA Rating', 'MPAA Rating for', 'Opening Weekend', 'Production Budget', 'Production Companies', 'Production Countries', 'Production Method', 'Running Time Min', 'Source', 'Opening Theater Counts', 'Max Theater Counts', 'Avg Weeks Per Theater', 'Worldwide Box Office', 'release_year', 'release_month', 'release_day', 'release_date']\n",
      "Columns to keep for movies_dataset.csv: ['title', 'Domestic Box Office', 'International Box Office', 'Genre', 'Worldwide Box Office', 'release_year', 'release_month', 'release_day', 'release_date', 'Id', 'Languages']\n",
      "Remaining columns for movies_dataset.csv: ['title', 'Domestic Box Office', 'International Box Office', 'Genre', 'Worldwide Box Office', 'release_year', 'release_month', 'release_day', 'release_date', 'Id', 'Languages']\n",
      "Original columns for metacritic-reviews.csv: ['Title', 'Release Date', 'Rating', 'summary', 'User rating', 'Website rating']\n",
      "Columns to keep for metacritic-reviews.csv: ['Title', 'Rating', 'User rating', 'Website rating', 'Release Date']\n",
      "Remaining columns for metacritic-reviews.csv: ['Title', 'Rating', 'User rating', 'Website rating', 'Release Date']\n",
      "Original columns for letterboxd-reviews.csv: ['Title', 'Release Year', 'Reviewer name', 'Review date', 'Review', 'Comment count', 'Like count']\n",
      "Columns to keep for letterboxd-reviews.csv: ['Title', 'Review', 'Review date', 'Comment count', 'Like count', 'Release Year']\n",
      "Remaining columns for letterboxd-reviews.csv: ['Title', 'Review', 'Review date', 'Comment count', 'Like count', 'Release Year']\n",
      "Original columns for rotten_tomatoes_movies.csv: ['id', 'title', 'audience_score', 'tomato_meter', 'rating', 'rating_contents', 'release_date_theaters', 'runtime_minutes', 'genre', 'original_language', 'director', 'writer', 'box_office', 'distributor', 'sound_mix']\n",
      "Columns to keep for rotten_tomatoes_movies.csv: ['title', 'audience_score', 'tomato_meter', 'director', 'id', 'box_office', 'release_date_theaters', 'original_language']\n",
      "Remaining columns for rotten_tomatoes_movies.csv: ['title', 'audience_score', 'tomato_meter', 'director', 'id', 'box_office', 'release_date_theaters', 'original_language']\n",
      "Original columns for movies_extras.csv: ['id', 'tagline', 'credits', 'keywords', 'poster_path', 'backdrop_path']\n",
      "Columns to keep for movies_extras.csv: ['id', 'credits']\n",
      "Remaining columns for movies_extras.csv: ['id', 'credits']\n",
      "Original columns for rotten_tomatoes_movie_reviews.csv: ['id', 'reviewId', 'creationDate', 'criticName', 'isTopCritic', 'originalScore', 'reviewState', 'publicatioName', 'reviewText', 'scoreSentiment', 'reviewUrl']\n",
      "Columns to keep for rotten_tomatoes_movie_reviews.csv: ['reviewText', 'scoreSentiment', 'id', 'isTopCritic', 'originalScore', 'creationDate']\n",
      "Remaining columns for rotten_tomatoes_movie_reviews.csv: ['reviewText', 'scoreSentiment', 'id', 'isTopCritic', 'originalScore', 'creationDate']\n",
      "Original columns for 25k IMDb movie Dataset.csv: ['movie title', 'Rating', 'User Rating', 'Generes', 'Overview', 'Plot Kyeword', 'Director', 'Top 5 Casts', 'Writer', 'year', 'path']\n",
      "Columns to keep for 25k IMDb movie Dataset.csv: ['movie title', 'Rating', 'User Rating', 'Director', 'Top 5 Casts', 'year']\n",
      "Remaining columns for 25k IMDb movie Dataset.csv: ['movie title', 'Rating', 'User Rating', 'Director', 'Top 5 Casts', 'year']\n",
      "Original columns for movies.csv: ['id', 'title', 'genres', 'original_language', 'overview', 'popularity', 'production_companies', 'release_date', 'budget', 'revenue', 'runtime', 'status', 'tagline', 'vote_average', 'vote_count', 'credits', 'keywords', 'poster_path', 'backdrop_path', 'recommendations']\n",
      "Columns to keep for movies.csv: ['title', 'genres', 'popularity', 'budget', 'revenue', 'vote_average', 'vote_count', 'id', 'release_date', 'original_language']\n",
      "Remaining columns for movies.csv: ['title', 'genres', 'popularity', 'budget', 'revenue', 'vote_average', 'vote_count', 'id', 'release_date', 'original_language']\n",
      "Dropping columns completed!\n"
     ]
    }
   ],
   "source": [
    "# Remove unneeded columns to make for easier processing\n",
    "folder_path = '/Users/toniwork/Desktop/AUBEC - 3 Projects/to_clean'\n",
    "output_folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "create_folder(output_folder_path)\n",
    "\n",
    "# Dictionary to map file names to the columns to keep\n",
    "columns_to_keep = {\n",
    "    'movies_dataset.csv': ['title', 'Domestic Box Office', 'International Box Office', 'Genre', 'Worldwide Box Office', 'release_year', 'release_month', 'release_day', 'release_date', 'Id', 'Languages'],\n",
    "    'metacritic-reviews.csv': ['Title', 'Rating', 'User rating', 'Website rating', 'Release Date'],\n",
    "    'letterboxd-reviews.csv': ['Title', 'Review', 'Review date', 'Comment count', 'Like count', 'Release Year'],\n",
    "    'rotten_tomatoes_movies.csv': ['title', 'audience_score', 'tomato_meter', 'director', 'id', 'box_office', 'release_date_theaters', 'original_language'],\n",
    "    'rotten_tomatoes_movie_reviews.csv': ['reviewText', 'scoreSentiment', 'id', 'isTopCritic', 'originalScore', 'creationDate'],\n",
    "    '25k IMDb movie Dataset.csv': ['movie title', 'Rating', 'User Rating', 'Director', 'Top 5 Casts', 'year'],\n",
    "    'movies.csv': ['title', 'genres', 'popularity', 'budget', 'revenue', 'vote_average', 'vote_count', 'id', 'release_date', 'original_language'],\n",
    "    'movies_extras.csv': ['id','credits']\n",
    "}\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        output_file_path = os.path.join(output_folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Print the original columns\n",
    "        print(f\"Original columns for {filename}: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Get the columns to keep for this file\n",
    "        keep_columns = columns_to_keep.get(filename, [])\n",
    "        \n",
    "        # Print the columns to keep\n",
    "        print(f\"Columns to keep for {filename}: {keep_columns}\")\n",
    "        \n",
    "        # Drop all other columns\n",
    "        df = df[keep_columns]\n",
    "        \n",
    "        # Print the remaining columns\n",
    "        print(f\"Remaining columns for {filename}: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Save the cleaned file to the output folder\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Dropping columns completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5939294",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion of files under 50KB completed!\n"
     ]
    }
   ],
   "source": [
    "# Delete files with no data \n",
    "\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Get the file size in bytes\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    # Check if the file size is less than 50KB (50KB = 50 * 1024 bytes)\n",
    "    if file_size < 50 * 1024:\n",
    "        print(f\"Deleting {filename} (size: {file_size} bytes)\")\n",
    "        os.remove(file_path)\n",
    "\n",
    "print(\"Deletion of files under 50KB completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956aefcd",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13195 entries, 0 to 13194\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     13195 non-null  object \n",
      " 1   Domestic Box Office       13194 non-null  float64\n",
      " 2   International Box Office  8095 non-null   float64\n",
      " 3   Genre                     12643 non-null  object \n",
      " 4   Worldwide Box Office      8095 non-null   float64\n",
      " 5   release_year              13187 non-null  float64\n",
      " 6   release_month             13185 non-null  float64\n",
      " 7   release_day               13180 non-null  float64\n",
      " 8   release_date              13193 non-null  object \n",
      " 9   Id                        13195 non-null  int64  \n",
      " 10  Languages                 10464 non-null  object \n",
      "dtypes: float64(6), int64(1), object(4)\n",
      "memory usage: 1.1+ MB\n",
      "movies_dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9345 entries, 0 to 9344\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Title           9345 non-null   object \n",
      " 1   Rating          8560 non-null   object \n",
      " 2   User rating     7674 non-null   float64\n",
      " 3   Website rating  9345 non-null   int64  \n",
      " 4   Release Date    9345 non-null   object \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 365.2+ KB\n",
      "metacritic-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4596 entries, 0 to 4595\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Title          4596 non-null   object \n",
      " 1   Review         3560 non-null   object \n",
      " 2   Review date    4370 non-null   object \n",
      " 3   Comment count  4273 non-null   object \n",
      " 4   Like count     3318 non-null   float64\n",
      " 5   Release Year   4578 non-null   float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 215.6+ KB\n",
      "letterboxd-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143258 entries, 0 to 143257\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   title                  142891 non-null  object \n",
      " 1   audience_score         73248 non-null   float64\n",
      " 2   tomato_meter           33877 non-null   float64\n",
      " 3   director               139064 non-null  object \n",
      " 4   id                     143258 non-null  object \n",
      " 5   box_office             14743 non-null   object \n",
      " 6   release_date_theaters  30773 non-null   object \n",
      " 7   original_language      129400 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 8.7+ MB\n",
      "rotten_tomatoes_movies.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249325 entries, 0 to 249324\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   id       249325 non-null  int64 \n",
      " 1   credits  151157 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "movies_extras.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1444963 entries, 0 to 1444962\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   reviewText      1375738 non-null  object\n",
      " 1   scoreSentiment  1444963 non-null  object\n",
      " 2   id              1444963 non-null  object\n",
      " 3   isTopCritic     1444963 non-null  bool  \n",
      " 4   originalScore   1009745 non-null  object\n",
      " 5   creationDate    1444963 non-null  object\n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 56.5+ MB\n",
      "rotten_tomatoes_movie_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14057 entries, 0 to 14056\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   movie title  14057 non-null  object \n",
      " 1   Rating       12763 non-null  float64\n",
      " 2   User Rating  14057 non-null  object \n",
      " 3   Director     14057 non-null  object \n",
      " 4   Top 5 Casts  14057 non-null  object \n",
      " 5   year         14057 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 659.0+ KB\n",
      "25k IMDb movie Dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 722917 entries, 0 to 722916\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   title              722913 non-null  object \n",
      " 1   genres             511960 non-null  object \n",
      " 2   popularity         722917 non-null  float64\n",
      " 3   budget             722917 non-null  float64\n",
      " 4   revenue            722917 non-null  float64\n",
      " 5   vote_average       722917 non-null  float64\n",
      " 6   vote_count         722917 non-null  float64\n",
      " 7   id                 722917 non-null  int64  \n",
      " 8   release_date       670290 non-null  object \n",
      " 9   original_language  722917 non-null  object \n",
      "dtypes: float64(5), int64(1), object(4)\n",
      "memory usage: 55.2+ MB\n",
      "movies.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print all file info from step_1 folder, this is to review the column names and data\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823c9380",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Intital clean_movie function to clean up for data consistency\n",
    "\n",
    "titles_with_removed_zeros = set()\n",
    "\n",
    "def clean_movie_data(folder_path, output_folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {filename} not found.\")\n",
    "                continue\n",
    "\n",
    "        # Call the cleaning functions\n",
    "        df = convert_column_names(df)\n",
    "        df = clean_title_columns(df)\n",
    "        df = handle_release_dates(df)\n",
    "        df = remove_duplicates(df, columns_to_check=['release_year','title', 'Title', 'movie title'])\n",
    "        df = handle_date_ranges(df,\n",
    "            date_columns=['release_date'],\n",
    "            year_column='year',\n",
    "            start_date='2002-01-01',\n",
    "            end_date='2022-12-31',\n",
    "            start_year=2002,\n",
    "            end_year=2022)\n",
    "        df = convert_string_columns_to_lowercase(df)\n",
    "        df = filter_by_language(df)\n",
    "\n",
    "        # Save cleaned DataFrame\n",
    "        new_filename = filename\n",
    "        df.to_csv(os.path.join(output_folder_path, new_filename), index=False)\n",
    "        print(f\"{filename} cleaned and saved as {new_filename}.\")\n",
    "    \n",
    "    output_folder_path0 = '/Users/toniwork/Desktop/Capstone/'\n",
    "    save_titles_with_removed_zeros(output_folder_path0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8700f7a6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 2nd initial clean_movie function to clean up non-numeric to numeric types\n",
    "def clean_movie_data2(folder_path, output_folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {filename} not found.\")\n",
    "                continue\n",
    "\n",
    "        # Call the cleaning functions\n",
    "        df = clean_money_columns(df, columns=['user_rating', 'box_office', 'comment_count'])       \n",
    "\n",
    "        # Save cleaned DataFrame\n",
    "        new_filename = filename\n",
    "        df.to_csv(os.path.join(output_folder_path, new_filename), index=False)\n",
    "        print(f\"{filename} cleaned and saved as {new_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01ca5ee",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'release_date' column not in 'YYYY-MM-DD' format. Cannot convert non-finite values (NA or inf) to integer\n",
      "movies_dataset.csv cleaned and saved as movies_dataset.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metacritic-reviews.csv cleaned and saved as metacritic-reviews.csv.\n",
      "letterboxd-reviews.csv cleaned and saved as letterboxd-reviews.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotten_tomatoes_movies.csv cleaned and saved as rotten_tomatoes_movies.csv.\n",
      "movies_extras.csv cleaned and saved as movies_extras.csv.\n",
      "rotten_tomatoes_movie_reviews.csv cleaned and saved as rotten_tomatoes_movie_reviews.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25k IMDb movie Dataset.csv cleaned and saved as 25k IMDb movie Dataset.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'release_date' column not in 'YYYY-MM-DD' format. Cannot convert non-finite values (NA or inf) to integer\n",
      "movies.csv cleaned and saved as movies.csv.\n",
      "Titles with removed zeros saved as 'titles_with_removed_zeros.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Run clean_movie_data function\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "output_folder_path = '/Users/toniwork/Desktop/Capstone/step_1'  \n",
    "\n",
    "clean_movie_data(folder_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e74873e",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column user_rating not found in the DataFrame. Skipping.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "movies_dataset.csv cleaned and saved as movies_dataset.csv.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "metacritic-reviews.csv cleaned and saved as metacritic-reviews.csv.\n",
      "Column user_rating not found in the DataFrame. Skipping.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "letterboxd-reviews.csv cleaned and saved as letterboxd-reviews.csv.\n",
      "Column user_rating not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "rotten_tomatoes_movies.csv cleaned and saved as rotten_tomatoes_movies.csv.\n",
      "Column user_rating not found in the DataFrame. Skipping.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "movies_extras.csv cleaned and saved as movies_extras.csv.\n",
      "Column user_rating not found in the DataFrame. Skipping.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "rotten_tomatoes_movie_reviews.csv cleaned and saved as rotten_tomatoes_movie_reviews.csv.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "25k IMDb movie Dataset.csv cleaned and saved as 25k IMDb movie Dataset.csv.\n",
      "Column user_rating not found in the DataFrame. Skipping.\n",
      "Column box_office not found in the DataFrame. Skipping.\n",
      "Column comment_count not found in the DataFrame. Skipping.\n",
      "movies.csv cleaned and saved as movies.csv.\n"
     ]
    }
   ],
   "source": [
    "# Run clean_movie_data2 function\n",
    "clean_movie_data2(folder_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94861bd",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_dataset.csv :              title  domestic_box_office  international_box_office  \\\n",
      "0   silent trigger              76382.0                       NaN   \n",
      "1         touch me               5000.0                       NaN   \n",
      "2         impostor            6114237.0                 1860370.0   \n",
      "3        spiderman          403706375.0               418000000.0   \n",
      "4  die another day          160942139.0               271000000.0   \n",
      "\n",
      "               genre  worldwide_box_office  release_year  release_month  \\\n",
      "0  thriller/suspense                   NaN           NaN            NaN   \n",
      "1              drama                   NaN           NaN            NaN   \n",
      "2  thriller/suspense             7974607.0        2002.0            1.0   \n",
      "3          adventure           821706375.0        2002.0            5.0   \n",
      "4             action           431942139.0        2002.0           11.0   \n",
      "\n",
      "   release_day release_date    id languages  \n",
      "0          NaN          NaN   577   english  \n",
      "1          NaN          NaN  1014   english  \n",
      "2          4.0   2002-01-04  2789   english  \n",
      "3          3.0   2002-05-03  3280   english  \n",
      "4         22.0   2002-11-22  3281   english  \n",
      "metacritic-reviews.csv :                                                title     rating  user_rating  \\\n",
      "0  rivers and tides andy goldsworthy working with...       tv-g         76.0   \n",
      "1                                           impostor          r         76.0   \n",
      "2                         what time is it over there  not rated         67.0   \n",
      "3                            brotherhood of the wolf          r         73.0   \n",
      "4                                      orange county      pg-13         71.0   \n",
      "\n",
      "   website_rating release_date  release_year  release_month  release_day  \n",
      "0              82   2002-01-02          2002              1            2  \n",
      "1              33   2002-01-04          2002              1            4  \n",
      "2              79   2002-01-11          2002              1           11  \n",
      "3              57   2002-01-11          2002              1           11  \n",
      "4              48   2002-01-11          2002              1           11  \n",
      "letterboxd-reviews.csv :                                 title  \\\n",
      "0                           aftersun    \n",
      "1                              joker    \n",
      "2        puss in boots the last wish    \n",
      "3          the banshees of inisherin    \n",
      "4  everything everywhere all at once    \n",
      "\n",
      "                                              review review_date  \\\n",
      "0                  this review may contain spoilers.  2020-01-12   \n",
      "1  if youв??ve never swam in the ocean then of co...  2022-12-20   \n",
      "2                puss in boots: into the pussy-verse  2022-09-15   \n",
      "3  i will not leave my donkey outside when iв??m sad  2022-04-08   \n",
      "4  watch it and have fun before film twitter tell...  2019-08-14   \n",
      "\n",
      "   comment_count  like_count  release_year  \n",
      "0          130.0     22446.0        2022.0  \n",
      "1            NaN     22032.0        2019.0  \n",
      "2           62.0     21666.0        2022.0  \n",
      "3            NaN     21609.0        2022.0  \n",
      "4          355.0     20688.0        2022.0  \n",
      "rotten_tomatoes_movies.csv :                 title  audience_score  tomato_meter               director  \\\n",
      "0  space zombie bingo            50.0           NaN          george ormrod   \n",
      "1     the green grass             NaN           NaN        tiffany edwards   \n",
      "2         sore losers            60.0           NaN  john michael mccarthy   \n",
      "3     dinosaur island            70.0           NaN          will meugniot   \n",
      "4              adrift            65.0          69.0      baltasar kormákur   \n",
      "\n",
      "                     id  box_office release_date_theaters original_language  \n",
      "0    space-zombie-bingo         NaN                   NaN           english  \n",
      "1       the_green_grass         NaN                   NaN           english  \n",
      "2  the_sore_losers_1997         NaN                   NaN           english  \n",
      "3  dinosaur_island_2002         NaN                   NaN           english  \n",
      "4           adrift_2018         NaN            2018-06-01           english  \n",
      "movies_extras.csv :       id                                            credits\n",
      "0  44351  scott coffey-rebekah del rio-laura harring-nao...\n",
      "1  23964  stephen baldwin-callum keith rennie-jessica st...\n",
      "2  36174  martin clunes-victoria hamilton-conleth hill-j...\n",
      "3  31460  sarah jessica parker-harry connick jr.-johnny ...\n",
      "4  26449  jay michael ferguson-allison lange-michael bow...\n",
      "rotten_tomatoes_movie_reviews.csv :                                           reviewtext scoresentiment  \\\n",
      "0  timed to be just long enough for most youngste...       positive   \n",
      "1  it doesn't matter if a movie costs 300 million...       negative   \n",
      "2  the choreography is so precise and lifelike at...       positive   \n",
      "3  the film's out-of-touch attempts at humor may ...       negative   \n",
      "4  its clumsy determination is endearing and some...       positive   \n",
      "\n",
      "                                  id  istopcritic originalscore creationdate  \n",
      "0                            beavers        False         3.5/4   2003-05-23  \n",
      "1                         blood_mask        False           1/5   2007-06-02  \n",
      "2  city_hunter_shinjuku_private_eyes        False           NaN   2019-05-28  \n",
      "3  city_hunter_shinjuku_private_eyes        False         2.5/5   2019-02-14  \n",
      "4                 dangerous_men_2015        False           NaN   2018-08-29  \n",
      "25k IMDb movie Dataset.csv :              movie_title  rating  user_rating           director  \\\n",
      "0  all things fall apart     5.3          NaN  mario van peebles   \n",
      "1                    gun     3.8          NaN      jessy terrero   \n",
      "2       what alice found     6.4          NaN       a. dean bell   \n",
      "3                  happy     7.1          NaN     a. karunakaran   \n",
      "4                   balu     6.0          NaN     a. karunakaran   \n",
      "\n",
      "                                         top_5_casts  year  \n",
      "0  ['brian a. miller', '50 cent', 'ray liotta', '...  2011  \n",
      "1  ['50 cent', 'val kilmer', 'annalynne mccord', ...  2010  \n",
      "2  ['emily grace', 'judith ivey', 'bill raymond',...  2003  \n",
      "3  ['radha mohan', 'darling swamy', 'allu arjun',...  2006  \n",
      "4  ['kona venkat', 'pawan kalyan', 'shriya saran'...  2005  \n",
      "movies.csv :                            title                             genres  \\\n",
      "0        avatar the way of water   science fiction-adventure-action   \n",
      "1  black panther wakanda forever   action-adventure-science fiction   \n",
      "2    puss in boots the last wish  animation-adventure-comedy-family   \n",
      "3            lord of the streets                             action   \n",
      "4              a man called otto                       comedy-drama   \n",
      "\n",
      "   popularity       budget       revenue  vote_average  vote_count      id  \\\n",
      "0    9366.788  350000000.0  2.312336e+09         7.751      6748.0   76600   \n",
      "1    2525.408  250000000.0  8.585356e+08         7.338      3922.0  505642   \n",
      "2    2078.280   90000000.0  4.630876e+08         8.363      4671.0  315162   \n",
      "3    1691.825    1000000.0  0.000000e+00         4.900        29.0  965839   \n",
      "4    1545.382   50000000.0  1.038423e+08         7.811       540.0  937278   \n",
      "\n",
      "  release_date original_language  \n",
      "0   2022-12-14                en  \n",
      "1   2022-11-09                en  \n",
      "2   2022-12-07                en  \n",
      "3   2022-04-22                en  \n",
      "4   2022-12-28                en  \n"
     ]
    }
   ],
   "source": [
    "# Print file header informaition to verify cleaning steps were correctly processed\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56cdb0",
   "metadata": {},
   "source": [
    "## Title Key Creation and Application\n",
    "\n",
    "### Create a title key using the create_title_key_file() function, which:\n",
    "- Loops over all CSV files in a folder and counts the number of times each movie title appears across all files\n",
    "- Creates a DataFrame with the title and the number of files it appears in and filters it to keep only titles that appear in at least two files\n",
    "- Saves the DataFrame to a new CSV file called 'title_key.csv' in the specified output folder path\n",
    "\n",
    "### Add corresponding title IDs to each dataset using the add_title_ids() function, which:\n",
    "- Reads in the 'title_key.csv' file and loops over each file in the specified folder path\n",
    "- Adds a new column called 'title_id' to the DataFrame and loops through each row to add the corresponding title ID from the 'title_key.csv' file\n",
    "- If a title ID is not found, the row is dropped from the DataFrame\n",
    "- Saves the updated CSV file to the specified output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5567d6",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate titles in movies_dataset.csv: 1308\n",
      "Duplicate titles in metacritic-reviews.csv: 143\n",
      "Duplicate titles in letterboxd-reviews.csv: 3349\n",
      "Duplicate titles in rotten_tomatoes_movies.csv: 8633\n",
      "Duplicate titles in 25k IMDb movie Dataset.csv: 269\n",
      "Duplicate titles in movies.csv: 50912\n",
      "Title key file saved as 'title_key.csv'.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 215824 entries, 0 to 215823\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   title          215795 non-null  object\n",
      " 1   release_date   215824 non-null  object\n",
      " 2   release_year   215824 non-null  int64 \n",
      " 3   release_month  215824 non-null  int64 \n",
      " 4   release_day    215824 non-null  int64 \n",
      " 5   title_id       215824 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 9.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create title key using create_title_key_file function\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "output_folder_path = '/Users/toniwork/Desktop/Capstone/keys'\n",
    "specific_files = ['rotten_tomatoes_movies.csv','letterboxd-reviews.csv','metacritic-reviews.csv',\n",
    "                  '25k IMDb movie Dataset.csv', 'rotten_tomatoes_movie_reviews.csv']\n",
    "columns_to_check = [col for col in df.columns if 'title' in col.lower()]\n",
    "\n",
    "create_folder(output_folder_path)\n",
    "\n",
    "create_title_key_file(folder_path, output_folder_path, specific_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62ada5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: title_key_final.csv, Title duplicates: 0\n",
      "File: title_key_final.csv, Title_id duplicates: 0\n",
      "File: title_key.csv, Title duplicates: 49064\n",
      "File: title_key.csv, Title_id duplicates: 0\n",
      "File: movie_data_summary_key.csv, does not have a 'title' column.\n",
      "File: movie_data_summary_key.csv, Title_id duplicates: 0\n",
      "File: review_summary_key.csv, does not have a 'title' column.\n",
      "File: review_summary_key.csv, Title_id duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/Users/toniwork/Desktop/Capstone/keys\"\n",
    "file_pattern = \".csv\" \n",
    "drop_duplicate_rows_and_report(folder_path, file_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ed0cc7",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sorting:\n",
      "           title release_date  release_year  release_month  release_day  \\\n",
      "57220          1   2006-01-01          2006              1            1   \n",
      "83376          1   2011-05-09          2011              5            9   \n",
      "145618         1   2013-09-20          2013              9           20   \n",
      "89868          1   2013-09-30          2013              9           30   \n",
      "178450         1   2019-05-11          2019              5           11   \n",
      "162732         1   2019-07-14          2019              7           14   \n",
      "181267         1   2020-09-14          2020              9           14   \n",
      "14791          1   2020-12-07          2020             12            7   \n",
      "178367         1   2021-11-20          2021             11           20   \n",
      "154842  1  1  11   2015-06-21          2015              6           21   \n",
      "\n",
      "        title_id  \n",
      "57220      57221  \n",
      "83376      83377  \n",
      "145618    145619  \n",
      "89868      89869  \n",
      "178450    178451  \n",
      "162732    162733  \n",
      "181267    181268  \n",
      "14791      14792  \n",
      "178367    178368  \n",
      "154842    154843  \n",
      "After dropping duplicates:\n",
      "                       title release_date  release_year  release_month  \\\n",
      "57220                      1   2006-01-01          2006              1   \n",
      "154842              1  1  11   2015-06-21          2015              6   \n",
      "155976                   1 2   2007-01-01          2007              1   \n",
      "206566                 1 2 3   2015-10-05          2015             10   \n",
      "153166  1 2 3 all eyes on me   2020-10-24          2020             10   \n",
      "29585         1 2 3 knock up   2006-01-01          2006              1   \n",
      "128291            1 a minute   2010-10-06          2010             10   \n",
      "137255           1 and 0 nly   2008-10-16          2008             10   \n",
      "131193     1 angry black man   2019-10-12          2019             10   \n",
      "90080      1 billion orgasms   2018-09-21          2018              9   \n",
      "\n",
      "        release_day  title_id  \n",
      "57220             1     57221  \n",
      "154842           21    154843  \n",
      "155976            1    155977  \n",
      "206566            5    206567  \n",
      "153166           24    153167  \n",
      "29585             1     29586  \n",
      "128291            6    128292  \n",
      "137255           16    137256  \n",
      "131193           12    131194  \n",
      "90080            21     90081  \n",
      "Null titles and duplicates dropped, and file saved.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 184432 entries, 57220 to 158338\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   title          184432 non-null  object\n",
      " 1   release_date   184432 non-null  object\n",
      " 2   release_year   184432 non-null  int64 \n",
      " 3   release_month  184432 non-null  int64 \n",
      " 4   release_day    184432 non-null  int64 \n",
      " 5   title_id       184432 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 9.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/toniwork/Desktop/Capstone/keys/title_key.csv\"\n",
    "# Read the file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows where the 'title' is null or empty\n",
    "df = df[df['title'].notna() & (df['title'] != '')]\n",
    "\n",
    "# Sort the DataFrame by 'title' and 'release_date'\n",
    "df.sort_values(by=['title', 'release_date'], ascending=[True, True], inplace=True)\n",
    "\n",
    "# Print first few rows to confirm sorting\n",
    "print(\"After sorting:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Drop duplicate rows based on 'title', keeping the first (earliest 'release_date') occurrence\n",
    "df.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "\n",
    "# Print first few rows to confirm duplicates are dropped\n",
    "print(\"After dropping duplicates:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Save the DataFrame back to the file or a new file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Null titles and duplicates dropped, and file saved.\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d37bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential duplicates saved for manual review.\n"
     ]
    }
   ],
   "source": [
    "# Read the file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Find titles that contain only numbers\n",
    "numeric_titles = df[df['title'].str.isnumeric()]\n",
    "\n",
    "# Group by 'title' and count the occurrences\n",
    "grouped_titles = numeric_titles.groupby('title').size().reset_index(name='counts')\n",
    "\n",
    "# Filter out titles that appear more than once (potential duplicates)\n",
    "potential_duplicates = grouped_titles[grouped_titles['counts'] > 1]['title'].tolist()\n",
    "\n",
    "# Save these titles to a text file or another format for manual review\n",
    "with open('potential_duplicates.txt', 'w') as f:\n",
    "    for title in potential_duplicates:\n",
    "        f.write(f\"{title}\\n\")\n",
    "\n",
    "print(\"Potential duplicates saved for manual review.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b5c51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title_ids_main_test(input_folder, title_key_path, split_folder, output_folder):\n",
    "    # Read in the title key CSV file as a DataFrame\n",
    "    title_key_df = pd.read_csv(os.path.join(title_key_path, 'title_key.csv'))\n",
    "\n",
    "    # Loop through each file in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv') and 'extra' not in file_name:\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "            # Read in the CSV file as a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Get the title columns\n",
    "            title_cols = [col for col in df.columns if 'title' in col.lower()]\n",
    "\n",
    "            # Check if the file has any title columns\n",
    "            if len(title_cols) == 0:\n",
    "                continue\n",
    "\n",
    "            # Split the file if it has more than 50,000 rows\n",
    "            if len(df) > 25000:\n",
    "                split_files = []\n",
    "                split_num = 1\n",
    "                split_size = 25000\n",
    "                for i in range(0, len(df), split_size):\n",
    "                    split_file_name = f\"{os.path.splitext(file_name)[0]}_split{split_num}.csv\"\n",
    "                    split_num += 1\n",
    "                    split_file_path = os.path.join(split_folder, split_file_name)\n",
    "                    split_files.append(split_file_path)\n",
    "                    df.iloc[i:i+split_size].to_csv(split_file_path, index=False)\n",
    "\n",
    "                # Loop through each split file\n",
    "                for split_file_path in split_files:\n",
    "                    # Read in the CSV file as a DataFrame\n",
    "                    df_split = pd.read_csv(split_file_path)\n",
    "\n",
    "                    # Add a new column for the title ID\n",
    "                    df_split['title_id'] = None\n",
    "\n",
    "                    # Loop through each row in the DataFrame and add the title ID\n",
    "                    for i, row in df_split.iterrows():\n",
    "                        title = ' '.join([str(row[col]) for col in title_cols]).strip()\n",
    "                        title_id = title_key_df.loc[title_key_df['title'] == title, 'title_id'].values\n",
    "\n",
    "                        # If a title ID was found, add it to the DataFrame\n",
    "                        if len(title_id) > 0:\n",
    "                            df_split.at[i, 'title_id'] = title_id[0]\n",
    "                        else:\n",
    "                            # If no title ID was found, drop the row\n",
    "                            df_split.drop(i, inplace=True)\n",
    "\n",
    "                    # Save the updated CSV file to the split folder\n",
    "                    output_file = os.path.join(split_folder, os.path.basename(split_file_path))\n",
    "                    df_split.to_csv(output_file, index=False)\n",
    "                    print(f\"{split_file_path} cleaned and 'title_id' added. On {output_file}\")\n",
    "\n",
    "                # Concatenate the split files and save to the output_folder\n",
    "                split_files_df = [pd.read_csv(split_file) for split_file in split_files]\n",
    "                concat_df = pd.concat(split_files_df)\n",
    "                output_file = os.path.join(output_folder, file_name)\n",
    "                concat_df.to_csv(output_file, index=False)\n",
    "                print(f\"{len(split_files)} files concatenated and saved to {output_file}\")\n",
    "\n",
    "            else:\n",
    "                # Add a new column for the title ID\n",
    "                df['title_id'] = None\n",
    "\n",
    "                # Loop through each row in the DataFrame and add the title ID\n",
    "                for i, row in df.iterrows():\n",
    "                    title = ' '.join([str(row[col]) for col in title_cols]).strip()\n",
    "                    title_id = title_key_df.loc[title_key_df['title'] == title, 'title_id'].values\n",
    "                    # If a title ID was found, add it to the DataFrame\n",
    "                    if len(title_id) > 0:\n",
    "                        df.at[i, 'title_id'] = title_id[0]\n",
    "                    else:\n",
    "                        # If no title ID was found, drop the row\n",
    "                        df.drop(i, inplace=True)\n",
    "                \n",
    "                # Save the updated CSV file to the output path\n",
    "                output_file = os.path.join(output_folder, file_name)\n",
    "                output_file2 = os.path.join(input_folder, file_name)\n",
    "                df.to_csv(output_file, index=False)\n",
    "                #df.to_csv(output_file2, index=False)\n",
    "                print(f\"{file_name} cleaned and 'title_id' added.\",df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c306bfc0",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_dataset.csv cleaned and 'title_id' added.       title  domestic_box_office  international_box_office              genre  \\\n",
      "1  touch me               5000.0                       NaN              drama   \n",
      "2  impostor            6114237.0                 1860370.0  thriller/suspense   \n",
      "\n",
      "   worldwide_box_office  release_year  release_month  release_day  \\\n",
      "1                   NaN           NaN            NaN          NaN   \n",
      "2             7974607.0        2002.0            1.0          4.0   \n",
      "\n",
      "  release_date    id languages title_id  \n",
      "1          NaN  1014   english   152944  \n",
      "2   2002-01-04  2789   english   210194  \n",
      "metacritic-reviews.csv cleaned and 'title_id' added.                                                title rating  user_rating  \\\n",
      "0  rivers and tides andy goldsworthy working with...   tv-g         76.0   \n",
      "1                                           impostor      r         76.0   \n",
      "\n",
      "   website_rating release_date  release_year  release_month  release_day  \\\n",
      "0              82   2002-01-02          2002              1            2   \n",
      "1              33   2002-01-04          2002              1            4   \n",
      "\n",
      "  title_id  \n",
      "0   179353  \n",
      "1   210194  \n",
      "letterboxd-reviews.csv cleaned and 'title_id' added.        title                                             review review_date  \\\n",
      "0  aftersun                   this review may contain spoilers.  2020-01-12   \n",
      "1     joker   if youв??ve never swam in the ocean then of co...  2022-12-20   \n",
      "\n",
      "   comment_count  like_count  release_year title_id  \n",
      "0          130.0     22446.0        2022.0   201780  \n",
      "1            NaN     22032.0        2019.0    30951  \n",
      "/Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split1.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split1.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split2.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split2.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split3.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split3.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split4.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/rotten_tomatoes_movies_split4.csv\n",
      "4 files concatenated and saved to /Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movies.csv\n",
      "25k IMDb movie Dataset.csv cleaned and 'title_id' added.              movie_title  rating  user_rating           director  \\\n",
      "0  all things fall apart     5.3          NaN  mario van peebles   \n",
      "1                    gun     3.8          NaN      jessy terrero   \n",
      "\n",
      "                                         top_5_casts  year title_id  \n",
      "0  ['brian a. miller', '50 cent', 'ray liotta', '...  2011    78775  \n",
      "1  ['50 cent', 'val kilmer', 'annalynne mccord', ...  2010   188000  \n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split1.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split1.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split2.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split2.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split3.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split3.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split4.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split4.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split5.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split5.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split6.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split6.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split7.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split7.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split8.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split8.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split9.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split9.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split10.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split10.csv\n",
      "/Users/toniwork/Desktop/Capstone/split_1/movies_split11.csv cleaned and 'title_id' added. On /Users/toniwork/Desktop/Capstone/split_1/movies_split11.csv\n",
      "11 files concatenated and saved to /Users/toniwork/Desktop/Capstone/step_2/movies.csv\n"
     ]
    }
   ],
   "source": [
    "# Loop through file in folder to add the new title_id from the title_key.csv using add_title_ids_main function\n",
    "input_folder = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "title_key_path = '/Users/toniwork/Desktop/Capstone/keys/'\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "split_folder = '/Users/toniwork/Desktop/Capstone/split_1'\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "create_folder(output_folder)\n",
    "    \n",
    "# Create the split folder if it does not exist\n",
    "create_folder(split_folder)\n",
    "\n",
    "add_title_ids_main(input_folder, title_key_path, split_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f8a9d",
   "metadata": {},
   "source": [
    "## Dataset Compare/Update\n",
    "\n",
    "### Compare and update datasets with corresponding title IDs using the add_title_id() function, which:\n",
    "- Takes in two filenames, the name of the column to match on, the folder path where the files are located, and the output path where the merged files will be saved\n",
    "- Loads the data from the main dataset and the extras dataset, checks if the matching column exists in both datasets, creates a new column for the 'title_id' in the extras dataset, loops through each row in the extras dataset, and checks if the value in the matching column exists in the main dataset\n",
    "- If it does, it adds the corresponding 'title_id' to the new column in the extras dataset\n",
    "- If it doesn't, it drops the row\n",
    "\n",
    "Saves the updated extras dataset to a new CSV file in the specified output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dcd6922",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_extras.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/step_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movies_extras.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through files in folder to add the new title_id from the title_key.csv using add_title_ids_2 function, this compares its counter part file with the title_id to the other\n",
    "\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "compare_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "output_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "movies = 'movies.csv'\n",
    "movies_extras = 'movies_extras.csv'\n",
    "on_col = 'id'\n",
    "\n",
    "# Compare movies.csv and movies_extras.csv, drop rows and add 'title_id' column\n",
    "add_title_ids_2(movies, movies_extras, on_col, folder_path, output_path, compare_path)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac689dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title_ids_2(df1_file, df2_file, on_col, folder_path, output_path, compare_path):\n",
    "    # Load the data from df1\n",
    "    df1 = pd.read_csv(os.path.join(compare_path, df1_file))\n",
    "\n",
    "    # Load the data from df2\n",
    "    df2 = pd.read_csv(os.path.join(folder_path, df2_file))\n",
    "    \n",
    "    # Check if on_col exists in df1\n",
    "    if on_col not in df1.columns:\n",
    "        raise ValueError(f\"{on_col} not found in {df1_file}\")\n",
    "    \n",
    "    # Check if on_col exists in df2\n",
    "    if on_col not in df2.columns:\n",
    "        raise ValueError(f\"{on_col} not found in {df2_file}\")\n",
    "    \n",
    "    # Check for missing IDs\n",
    "    missing_ids = set(df2[on_col]) - set(df1[on_col])\n",
    "    if missing_ids:\n",
    "        print(f\"Missing IDs Count: {len(missing_ids)}\")\n",
    "           \n",
    "    # Create a new column for the title_id in df2\n",
    "    df2['title_id'] = ''\n",
    "    \n",
    "    # Keep only the rows in df2 that have a matching value in df1[on_col]\n",
    "    df2 = df2[df2[on_col].isin(df1[on_col])]\n",
    "    \n",
    "    # Loop through each row in df2 and add the corresponding title_id from df1\n",
    "    for index, row in df2.iterrows():\n",
    "        title_id = df1.loc[df1[on_col] == row[on_col], 'title_id'].iloc[0]\n",
    "        df2.loc[index, 'title_id'] = title_id\n",
    "    \n",
    "    # Write the updated df2 to a new CSV file in the specified output path\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    output_file = os.path.join(output_path, df2_file)\n",
    "    df2.to_csv(output_file, index=False)\n",
    "    print(f\"{df2_file} cleaned and 'title_id' added to {output_path}\")\n",
    "    \n",
    "    # Return the updated df2\n",
    "    return df2_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e619d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "compare_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "split_path = '/Users/toniwork/Desktop/Capstone/split_1'\n",
    "\n",
    "rotten_movie_file = 'rotten_tomatoes_movies.csv'\n",
    "rotten_movie_review_file = 'rotten_tomatoes_movie_reviews.csv'\n",
    "on_col = 'id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df701f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire DataFrame to get its length\n",
    "df_reviews = pd.read_csv(os.path.join(folder_path, rotten_movie_review_file))\n",
    "total_length = len(df_reviews)\n",
    "\n",
    "# Define the number of chunks\n",
    "num_chunks = 100\n",
    "\n",
    "# Calculate the chunk size\n",
    "chunk_size = total_length // num_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb63065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 4545\n",
      "Error in line 4546\n",
      "Error in line 4589\n",
      "Error in line 4590\n",
      "Error in line 4591\n",
      "Error in line 4592\n",
      "Error in line 4708\n",
      "Error in line 4736\n",
      "Error in line 4738\n",
      "Error in line 4891\n",
      "Error in line 4895\n",
      "Error in line 4897\n",
      "Error in line 8883\n",
      "Error in line 8884\n",
      "Error in line 8885\n",
      "Error in line 9334\n",
      "Error in line 9336\n",
      "Error in line 9746\n",
      "Error in line 9747\n",
      "Error in line 9748\n",
      "Error in line 9749\n",
      "Error in line 10231\n",
      "Error in line 10652\n",
      "Error in line 10653\n",
      "Error in line 13814\n",
      "Error in line 13815\n",
      "Error in line 15573\n",
      "Error in line 15574\n",
      "Error in line 16197\n",
      "Error in line 16199\n",
      "Error in line 19868\n",
      "Error in line 19869\n",
      "Error in line 20267\n",
      "Error in line 20523\n",
      "Error in line 20524\n",
      "Error in line 20843\n",
      "Error in line 20844\n",
      "Error in line 20845\n",
      "Error in line 21934\n",
      "Error in line 21935\n",
      "Error in line 23492\n",
      "Error in line 23493\n",
      "Error in line 23494\n",
      "Error in line 23495\n",
      "Error in line 23496\n",
      "Error in line 28255\n",
      "Error in line 29777\n",
      "Error in line 29778\n",
      "Error in line 30719\n",
      "Error in line 30723\n",
      "Error in line 30725\n",
      "Error in line 30727\n",
      "Error in line 30744\n",
      "Error in line 30745\n",
      "Error in line 30796\n",
      "Error in line 30898\n",
      "Error in line 30899\n",
      "Error in line 30900\n",
      "Error in line 30904\n",
      "Error in line 30905\n",
      "Error in line 30906\n",
      "Error in line 30961\n",
      "Error in line 30962\n",
      "Error in line 30963\n",
      "Error in line 32638\n",
      "Error in line 32639\n",
      "Error in line 37250\n",
      "Error in line 37256\n",
      "Error in line 37258\n",
      "Error in line 37323\n",
      "Error in line 37324\n",
      "Error in line 37362\n",
      "Error in line 37363\n",
      "Error in line 37390\n",
      "Error in line 37391\n",
      "Error in line 37509\n",
      "Error in line 37510\n",
      "Error in line 37527\n",
      "Error in line 37528\n",
      "Error in line 37532\n",
      "Error in line 37533\n",
      "Error in line 37570\n",
      "Error in line 37571\n",
      "Error in line 37572\n",
      "Error in line 38209\n",
      "Error in line 38255\n",
      "Error in line 38256\n",
      "Error in line 39324\n",
      "Error in line 39325\n",
      "Error in line 39326\n",
      "Error in line 42567\n",
      "Error in line 42568\n",
      "Error in line 42822\n",
      "Error in line 42823\n",
      "Error in line 42824\n",
      "Error in line 42830\n",
      "Error in line 42831\n",
      "Error in line 43101\n",
      "Error in line 43102\n",
      "Error in line 43103\n",
      "Error in line 43668\n",
      "Error in line 43669\n",
      "Error in line 43670\n",
      "Error in line 45360\n",
      "Error in line 45361\n",
      "Error in line 47838\n",
      "Error in line 47839\n",
      "Error in line 47840\n",
      "Error in line 47847\n",
      "Error in line 47848\n",
      "Error in line 47849\n",
      "Error in line 49436\n",
      "Error in line 51108\n",
      "Error in line 51109\n",
      "Error in line 54876\n",
      "Error in line 54877\n",
      "Error in line 57880\n",
      "Error in line 58084\n",
      "Error in line 58086\n",
      "Error in line 58087\n",
      "Error in line 58803\n",
      "Error in line 58804\n",
      "Error in line 66818\n",
      "Error in line 66829\n",
      "Error in line 66830\n",
      "Error in line 66884\n",
      "Error in line 66885\n",
      "Error in line 68279\n",
      "Error in line 68280\n",
      "Error in line 71620\n",
      "Error in line 71621\n",
      "Error in line 74084\n",
      "Error in line 74085\n",
      "Error in line 74520\n",
      "Error in line 74521\n",
      "Error in line 75311\n",
      "Error in line 75312\n",
      "Error in line 76741\n",
      "Error in line 76742\n",
      "Error in line 77260\n",
      "Error in line 77261\n",
      "Error in line 77297\n",
      "Error in line 77298\n",
      "Error in line 80077\n",
      "Error in line 80078\n",
      "Error in line 80079\n",
      "Error in line 80082\n",
      "Error in line 80083\n",
      "Error in line 80084\n",
      "Error in line 83427\n",
      "Error in line 83428\n",
      "Error in line 83429\n",
      "Error in line 83430\n",
      "Error in line 84965\n",
      "Error in line 84966\n",
      "Error in line 85500\n",
      "Error in line 85501\n",
      "Error in line 85502\n",
      "Error in line 90873\n",
      "Error in line 90874\n",
      "Error in line 92035\n",
      "Error in line 92036\n",
      "Error in line 92037\n",
      "Error in line 93567\n",
      "Error in line 93568\n",
      "Error in line 93904\n",
      "Error in line 93905\n",
      "Error in line 96196\n",
      "Error in line 96197\n",
      "Error in line 98137\n",
      "Error in line 98138\n",
      "Error in line 102903\n",
      "Error in line 103208\n",
      "Error in line 103209\n",
      "Error in line 104541\n",
      "Error in line 104542\n",
      "Error in line 107477\n",
      "Error in line 107478\n",
      "Error in line 107731\n",
      "Error in line 107732\n",
      "Error in line 107733\n",
      "Error in line 107977\n",
      "Error in line 107978\n",
      "Error in line 109836\n",
      "Error in line 109837\n",
      "Error in line 110499\n",
      "Error in line 110500\n",
      "Error in line 110574\n",
      "Error in line 110575\n",
      "Error in line 112846\n",
      "Error in line 112847\n",
      "Error in line 119307\n",
      "Error in line 119308\n",
      "Error in line 120092\n",
      "Error in line 120093\n",
      "Error in line 124517\n",
      "Error in line 124519\n",
      "Error in line 124520\n",
      "Error in line 128228\n",
      "Error in line 128229\n",
      "Error in line 132506\n",
      "Error in line 132507\n",
      "Error in line 132508\n",
      "Error in line 132509\n",
      "Error in line 133072\n",
      "Error in line 133073\n",
      "Error in line 140773\n",
      "Error in line 140775\n",
      "Error in line 140927\n",
      "Error in line 140928\n",
      "Error in line 140929\n",
      "Error in line 145549\n",
      "Error in line 145550\n",
      "Error in line 145758\n",
      "Error in line 145759\n",
      "Error in line 147365\n",
      "Error in line 147367\n",
      "Error in line 151361\n",
      "Error in line 151362\n",
      "Error in line 153470\n",
      "Error in line 153473\n",
      "Error in line 153477\n",
      "Error in line 153478\n",
      "Error in line 156007\n",
      "Error in line 156008\n",
      "Error in line 157415\n",
      "Error in line 157416\n",
      "Error in line 158829\n",
      "Error in line 158830\n",
      "Error in line 158831\n",
      "Error in line 158883\n",
      "Error in line 158884\n",
      "Error in line 163505\n",
      "Error in line 163506\n",
      "Error in line 163507\n",
      "Error in line 165127\n",
      "Error in line 165128\n",
      "Error in line 166395\n",
      "Error in line 166396\n",
      "Error in line 166868\n",
      "Error in line 167803\n",
      "Error in line 168694\n",
      "Error in line 171169\n",
      "Error in line 173346\n",
      "Error in line 173347\n",
      "Error in line 173376\n",
      "Error in line 173377\n",
      "Error in line 173378\n",
      "Error in line 173379\n",
      "Error in line 173380\n",
      "Error in line 176064\n",
      "Error in line 176066\n",
      "Error in line 176334\n",
      "Error in line 176335\n",
      "Error in line 177081\n",
      "Error in line 177082\n",
      "Error in line 179425\n",
      "Error in line 179427\n",
      "Error in line 181587\n",
      "Error in line 181588\n",
      "Error in line 182412\n",
      "Error in line 182417\n",
      "Error in line 182428\n",
      "Error in line 182429\n",
      "Error in line 182430\n",
      "Error in line 186611\n",
      "Error in line 186612\n",
      "Error in line 187054\n",
      "Error in line 188015\n",
      "Error in line 188016\n",
      "Error in line 188017\n",
      "Error in line 188308\n",
      "Error in line 188309\n",
      "Error in line 188330\n",
      "Error in line 188363\n",
      "Error in line 188364\n",
      "Error in line 192203\n",
      "Error in line 192204\n",
      "Error in line 192205\n",
      "Error in line 192911\n",
      "Error in line 192912\n",
      "Error in line 192913\n",
      "Error in line 195881\n",
      "Error in line 195882\n",
      "Error in line 197340\n",
      "Error in line 197341\n",
      "Error in line 197429\n",
      "Error in line 197430\n",
      "Error in line 197431\n",
      "Error in line 198531\n",
      "Error in line 199711\n",
      "Error in line 199712\n",
      "Error in line 200621\n",
      "Error in line 202149\n",
      "Error in line 202150\n",
      "Error in line 202973\n",
      "Error in line 202974\n",
      "Error in line 202975\n",
      "Error in line 207853\n",
      "Error in line 211367\n",
      "Error in line 211368\n",
      "Error in line 215222\n",
      "Error in line 216225\n",
      "Error in line 216226\n",
      "Error in line 216227\n",
      "Error in line 216253\n",
      "Error in line 218757\n",
      "Error in line 219585\n",
      "Error in line 219586\n",
      "Error in line 219591\n",
      "Error in line 220310\n",
      "Error in line 220311\n",
      "Error in line 220916\n",
      "Error in line 221391\n",
      "Error in line 221392\n",
      "Error in line 221393\n",
      "Error in line 221696\n",
      "Error in line 221697\n",
      "Error in line 221706\n",
      "Error in line 221707\n",
      "Error in line 225415\n",
      "Error in line 225416\n",
      "Error in line 225417\n",
      "Error in line 228384\n",
      "Error in line 228385\n",
      "Error in line 230276\n",
      "Error in line 231519\n",
      "Error in line 231521\n",
      "Error in line 231561\n",
      "Error in line 233111\n",
      "Error in line 233163\n",
      "Error in line 233164\n",
      "Error in line 233175\n",
      "Error in line 233176\n",
      "Error in line 233210\n",
      "Error in line 233211\n",
      "Error in line 233220\n",
      "Error in line 233221\n",
      "Error in line 234991\n",
      "Error in line 234993\n",
      "Error in line 238021\n",
      "Error in line 238022\n",
      "Error in line 238023\n",
      "Error in line 238042\n",
      "Error in line 238043\n",
      "Error in line 248935\n",
      "Error in line 248936\n",
      "Error in line 254379\n",
      "Error in line 254380\n",
      "Error in line 254852\n",
      "Error in line 254853\n",
      "Error in line 255232\n",
      "Error in line 255233\n",
      "Error in line 255321\n",
      "Error in line 255322\n",
      "Error in line 255382\n",
      "Error in line 255383\n",
      "Error in line 255384\n",
      "Error in line 256781\n",
      "Error in line 261491\n",
      "Error in line 261492\n",
      "Error in line 263144\n",
      "Error in line 263145\n",
      "Error in line 264399\n",
      "Error in line 264401\n",
      "Error in line 264412\n",
      "Error in line 264413\n",
      "Error in line 264414\n",
      "Error in line 264520\n",
      "Error in line 264521\n",
      "Error in line 264579\n",
      "Error in line 264580\n",
      "Error in line 264581\n",
      "Error in line 265232\n",
      "Error in line 265233\n",
      "Error in line 267051\n",
      "Error in line 267052\n",
      "Error in line 274846\n",
      "Error in line 274847\n",
      "Error in line 274849\n",
      "Error in line 274851\n",
      "Error in line 274923\n",
      "Error in line 274924\n",
      "Error in line 278395\n",
      "Error in line 278396\n",
      "Error in line 278772\n",
      "Error in line 278773\n",
      "Error in line 278774\n",
      "Error in line 279599\n",
      "Error in line 279600\n",
      "Error in line 281711\n",
      "Error in line 281712\n",
      "Error in line 282188\n",
      "Error in line 282453\n",
      "Error in line 282454\n",
      "Error in line 282455\n",
      "Error in line 283114\n",
      "Error in line 283116\n",
      "Error in line 283339\n",
      "Error in line 283495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 292581\n",
      "Error in line 292582\n",
      "Error in line 303609\n",
      "Error in line 303610\n",
      "Error in line 307025\n",
      "Error in line 307027\n",
      "Error in line 307029\n",
      "Error in line 307030\n",
      "Error in line 307493\n",
      "Error in line 307494\n",
      "Error in line 307495\n",
      "Error in line 307496\n",
      "Error in line 307497\n",
      "Error in line 307498\n",
      "Error in line 307509\n",
      "Error in line 307510\n",
      "Error in line 307511\n",
      "Error in line 307537\n",
      "Error in line 307538\n",
      "Error in line 307539\n",
      "Error in line 307639\n",
      "Error in line 307640\n",
      "Error in line 308929\n",
      "Error in line 308930\n",
      "Error in line 311557\n",
      "Error in line 311558\n",
      "Error in line 311559\n",
      "Error in line 315108\n",
      "Error in line 315109\n",
      "Error in line 315110\n",
      "Error in line 315599\n",
      "Error in line 315600\n",
      "Error in line 316478\n",
      "Error in line 316480\n",
      "Error in line 316491\n",
      "Error in line 316492\n",
      "Error in line 319299\n",
      "Error in line 320192\n",
      "Error in line 320193\n",
      "Error in line 320194\n",
      "Error in line 321372\n",
      "Error in line 321374\n",
      "Error in line 324359\n",
      "Error in line 324361\n",
      "Error in line 325659\n",
      "Error in line 325660\n",
      "Error in line 325661\n",
      "Error in line 325752\n",
      "Error in line 327341\n",
      "Error in line 327342\n",
      "Error in line 328384\n",
      "Error in line 328385\n",
      "Error in line 328743\n",
      "Error in line 328744\n",
      "Error in line 330883\n",
      "Error in line 330884\n",
      "Error in line 330901\n",
      "Error in line 330952\n",
      "Error in line 330953\n",
      "Error in line 330969\n",
      "Error in line 330972\n",
      "Error in line 330973\n",
      "Error in line 331002\n",
      "Error in line 331003\n",
      "Error in line 331052\n",
      "Error in line 331054\n",
      "Error in line 331055\n",
      "Error in line 331056\n",
      "Error in line 331379\n",
      "Error in line 331381\n",
      "Error in line 331880\n",
      "Error in line 331881\n",
      "Error in line 331882\n",
      "Error in line 333736\n",
      "Error in line 333737\n",
      "Error in line 336578\n",
      "Error in line 336579\n",
      "Error in line 336580\n",
      "Error in line 337241\n",
      "Error in line 337242\n",
      "Error in line 337796\n",
      "Error in line 337813\n",
      "Error in line 337814\n",
      "Error in line 337815\n",
      "Error in line 337816\n",
      "Error in line 337849\n",
      "Error in line 337850\n",
      "Error in line 337875\n",
      "Error in line 337911\n",
      "Error in line 337912\n",
      "Error in line 337913\n",
      "Error in line 337924\n",
      "Error in line 337925\n",
      "Error in line 337926\n",
      "Error in line 337941\n",
      "Error in line 337942\n",
      "Error in line 337976\n",
      "Error in line 337977\n",
      "Error in line 340362\n",
      "Error in line 340363\n",
      "Error in line 341730\n",
      "Error in line 341731\n",
      "Error in line 344385\n",
      "Error in line 344389\n",
      "Error in line 344608\n",
      "Error in line 344609\n",
      "Error in line 344610\n",
      "Error in line 344611\n",
      "Error in line 348215\n",
      "Error in line 348216\n",
      "Error in line 349795\n",
      "Error in line 356028\n",
      "Error in line 356029\n",
      "Error in line 359538\n",
      "Error in line 359539\n",
      "Error in line 359540\n",
      "Error in line 359541\n",
      "Error in line 360888\n",
      "Error in line 360889\n",
      "Error in line 361558\n",
      "Error in line 361559\n",
      "Error in line 361560\n",
      "Error in line 361568\n",
      "Error in line 361569\n",
      "Error in line 361585\n",
      "Error in line 361586\n",
      "Error in line 365595\n",
      "Error in line 365597\n",
      "Error in line 365599\n",
      "Error in line 370071\n",
      "Error in line 370072\n",
      "Error in line 370141\n",
      "Error in line 370143\n",
      "Error in line 370145\n",
      "Error in line 370163\n",
      "Error in line 370335\n",
      "Error in line 370336\n",
      "Error in line 370337\n",
      "Error in line 370349\n",
      "Error in line 370350\n",
      "Error in line 370351\n",
      "Error in line 372863\n",
      "Error in line 372943\n",
      "Error in line 374241\n",
      "Error in line 374242\n",
      "Error in line 374703\n",
      "Error in line 374704\n",
      "Error in line 374708\n",
      "Error in line 374709\n",
      "Error in line 374717\n",
      "Error in line 375187\n",
      "Error in line 375188\n",
      "Error in line 375785\n",
      "Error in line 375786\n",
      "Error in line 375787\n",
      "Error in line 375788\n",
      "Error in line 375789\n",
      "Error in line 379271\n",
      "Error in line 379272\n",
      "Error in line 379280\n",
      "Error in line 379281\n",
      "Error in line 379282\n",
      "Error in line 380346\n",
      "Error in line 380347\n",
      "Error in line 382587\n",
      "Error in line 382594\n",
      "Error in line 382595\n",
      "Error in line 382613\n",
      "Error in line 382614\n",
      "Error in line 390832\n",
      "Error in line 390833\n",
      "Error in line 392538\n",
      "Error in line 392539\n",
      "Error in line 394198\n",
      "Error in line 394199\n",
      "Error in line 394527\n",
      "Error in line 394528\n",
      "Error in line 394529\n",
      "Error in line 396058\n",
      "Error in line 397717\n",
      "Error in line 397718\n",
      "Error in line 399611\n",
      "Error in line 399612\n",
      "Error in line 400212\n",
      "Error in line 400213\n",
      "Error in line 400214\n",
      "Error in line 400216\n",
      "Error in line 402683\n",
      "Error in line 402684\n",
      "Error in line 402889\n",
      "Error in line 402890\n",
      "Error in line 402966\n",
      "Error in line 402967\n",
      "Error in line 402987\n",
      "Error in line 405014\n",
      "Error in line 405016\n",
      "Error in line 408294\n",
      "Error in line 408295\n",
      "Error in line 408296\n",
      "Error in line 417078\n",
      "Error in line 417079\n",
      "Error in line 423385\n",
      "Error in line 423387\n",
      "Error in line 423388\n",
      "Error in line 428327\n",
      "Error in line 428328\n",
      "Error in line 428501\n",
      "Error in line 429123\n",
      "Error in line 429124\n",
      "Error in line 430107\n",
      "Error in line 433597\n",
      "Error in line 433598\n",
      "Error in line 435197\n",
      "Error in line 435198\n",
      "Error in line 436035\n",
      "Error in line 436036\n",
      "Error in line 437766\n",
      "Error in line 437767\n",
      "Error in line 437768\n",
      "Error in line 438679\n",
      "Error in line 438680\n",
      "Error in line 438681\n",
      "Error in line 438682\n",
      "Error in line 438683\n",
      "Error in line 440775\n",
      "Error in line 440776\n",
      "Error in line 441217\n",
      "Error in line 441244\n",
      "Error in line 441245\n",
      "Error in line 441246\n",
      "Error in line 441249\n",
      "Error in line 441250\n",
      "Error in line 441251\n",
      "Error in line 441341\n",
      "Error in line 441342\n",
      "Error in line 443401\n",
      "Error in line 443402\n",
      "Error in line 446044\n",
      "Error in line 446045\n",
      "Error in line 448404\n",
      "Error in line 448405\n",
      "Error in line 448406\n",
      "Error in line 448407\n",
      "Error in line 448607\n",
      "Error in line 448609\n",
      "Error in line 451165\n",
      "Error in line 451166\n",
      "Error in line 455815\n",
      "Error in line 455816\n",
      "Error in line 457325\n",
      "Error in line 457326\n",
      "Error in line 457617\n",
      "Error in line 457619\n",
      "Error in line 459696\n",
      "Error in line 459697\n",
      "Error in line 460234\n",
      "Error in line 460235\n",
      "Error in line 463513\n",
      "Error in line 466020\n",
      "Error in line 466021\n",
      "Error in line 469862\n",
      "Error in line 469863\n",
      "Error in line 470275\n",
      "Error in line 470276\n",
      "Error in line 473096\n",
      "Error in line 473097\n",
      "Error in line 474538\n",
      "Error in line 474539\n",
      "Error in line 474540\n",
      "Error in line 480533\n",
      "Error in line 490427\n",
      "Error in line 495850\n",
      "Error in line 495851\n",
      "Error in line 499279\n",
      "Error in line 499280\n",
      "Error in line 503646\n",
      "Error in line 503647\n",
      "Error in line 503774\n",
      "Error in line 503775\n",
      "Error in line 503776\n",
      "Error in line 505236\n",
      "Error in line 505238\n",
      "Error in line 505239\n",
      "Error in line 505727\n",
      "Error in line 505728\n",
      "Error in line 507230\n",
      "Error in line 507232\n",
      "Error in line 507234\n",
      "Error in line 510908\n",
      "Error in line 510948\n",
      "Error in line 510949\n",
      "Error in line 512952\n",
      "Error in line 512953\n",
      "Error in line 512954\n",
      "Error in line 514970\n",
      "Error in line 514971\n",
      "Error in line 515036\n",
      "Error in line 515037\n",
      "Error in line 516440\n",
      "Error in line 516441\n",
      "Error in line 516442\n",
      "Error in line 517481\n",
      "Error in line 517483\n",
      "Error in line 517484\n",
      "Error in line 518122\n",
      "Error in line 518123\n",
      "Error in line 519945\n",
      "Error in line 519946\n",
      "Error in line 519949\n",
      "Error in line 519950\n",
      "Error in line 521124\n",
      "Error in line 521125\n",
      "Error in line 523563\n",
      "Error in line 527779\n",
      "Error in line 527780\n",
      "Error in line 527974\n",
      "Error in line 527976\n",
      "Error in line 528016\n",
      "Error in line 530667\n",
      "Error in line 531640\n",
      "Error in line 531641\n",
      "Error in line 534193\n",
      "Error in line 534195\n",
      "Error in line 534243\n",
      "Error in line 534244\n",
      "Error in line 543232\n",
      "Error in line 545726\n",
      "Error in line 545727\n",
      "Error in line 547542\n",
      "Error in line 547543\n",
      "Error in line 547942\n",
      "Error in line 547943\n",
      "Error in line 548107\n",
      "Error in line 548108\n",
      "Error in line 548391\n",
      "Error in line 548392\n",
      "Error in line 548411\n",
      "Error in line 548418\n",
      "Error in line 548420\n",
      "Error in line 548422\n",
      "Error in line 548423\n",
      "Error in line 548513\n",
      "Error in line 548514\n",
      "Error in line 548528\n",
      "Error in line 548529\n",
      "Error in line 548536\n",
      "Error in line 548537\n",
      "Error in line 548538\n",
      "Error in line 548542\n",
      "Error in line 548543\n",
      "Error in line 548544\n",
      "Error in line 548606\n",
      "Error in line 548607\n",
      "Error in line 548608\n",
      "Error in line 548609\n",
      "Error in line 548612\n",
      "Error in line 548613\n",
      "Error in line 555019\n",
      "Error in line 555020\n",
      "Error in line 555021\n",
      "Error in line 561083\n",
      "Error in line 561084\n",
      "Error in line 569679\n",
      "Error in line 569680\n",
      "Error in line 570753\n",
      "Error in line 570754\n",
      "Error in line 570774\n",
      "Error in line 570775\n",
      "Error in line 574552\n",
      "Error in line 574553\n",
      "Error in line 574554\n",
      "Error in line 575435\n",
      "Error in line 575436\n",
      "Error in line 575437\n",
      "Error in line 575518\n",
      "Error in line 577465\n",
      "Error in line 578846\n",
      "Error in line 578847\n",
      "Error in line 578850\n",
      "Error in line 578851\n",
      "Error in line 578852\n",
      "Error in line 580544\n",
      "Error in line 580545\n",
      "Error in line 580546\n",
      "Error in line 584505\n",
      "Error in line 584506\n",
      "Error in line 585380\n",
      "Error in line 585381\n",
      "Error in line 585415\n",
      "Error in line 585416\n",
      "Error in line 585962\n",
      "Error in line 585963\n",
      "Error in line 585964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 586258\n",
      "Error in line 586259\n",
      "Error in line 587609\n",
      "Error in line 587610\n",
      "Error in line 587611\n",
      "Error in line 594382\n",
      "Error in line 594383\n",
      "Error in line 597067\n",
      "Error in line 597073\n",
      "Error in line 597074\n",
      "Error in line 597135\n",
      "Error in line 597136\n",
      "Error in line 597159\n",
      "Error in line 597160\n",
      "Error in line 599739\n",
      "Error in line 599740\n",
      "Error in line 599741\n",
      "Error in line 599756\n",
      "Error in line 599757\n",
      "Error in line 602421\n",
      "Error in line 602422\n",
      "Error in line 602423\n",
      "Error in line 607181\n",
      "Error in line 607182\n",
      "Error in line 607480\n",
      "Error in line 607481\n",
      "Error in line 610516\n",
      "Error in line 610517\n",
      "Error in line 610518\n",
      "Error in line 610536\n",
      "Error in line 610537\n",
      "Error in line 610538\n",
      "Error in line 610594\n",
      "Error in line 610595\n",
      "Error in line 611127\n",
      "Error in line 611128\n",
      "Error in line 612027\n",
      "Error in line 612355\n",
      "Error in line 612532\n",
      "Error in line 612533\n",
      "Error in line 613450\n",
      "Error in line 613451\n",
      "Error in line 615790\n",
      "Error in line 615791\n",
      "Error in line 615995\n",
      "Error in line 615996\n",
      "Error in line 615997\n",
      "Error in line 616006\n",
      "Error in line 616007\n",
      "Error in line 616008\n",
      "Error in line 616009\n",
      "Error in line 616011\n",
      "Error in line 616012\n",
      "Error in line 617948\n",
      "Error in line 617949\n",
      "Error in line 619525\n",
      "Error in line 619526\n",
      "Error in line 620183\n",
      "Error in line 620184\n",
      "Error in line 620185\n",
      "Error in line 620989\n",
      "Error in line 621564\n",
      "Error in line 621565\n",
      "Error in line 621566\n",
      "Error in line 626372\n",
      "Error in line 626812\n",
      "Error in line 626813\n",
      "Error in line 629321\n",
      "Error in line 629322\n",
      "Error in line 634175\n",
      "Error in line 634176\n",
      "Error in line 634864\n",
      "Error in line 636650\n",
      "Error in line 641053\n",
      "Error in line 641054\n",
      "Error in line 643277\n",
      "Error in line 643278\n",
      "Error in line 643306\n",
      "Error in line 643307\n",
      "Error in line 647042\n",
      "Error in line 647043\n",
      "Error in line 647046\n",
      "Error in line 647047\n",
      "Error in line 650542\n",
      "Error in line 650543\n",
      "Error in line 651818\n",
      "Error in line 651820\n",
      "Error in line 653329\n",
      "Error in line 653330\n",
      "Error in line 653780\n",
      "Error in line 653781\n",
      "Error in line 655021\n",
      "Error in line 655022\n",
      "Error in line 667119\n",
      "Error in line 667120\n",
      "Error in line 667121\n",
      "Error in line 671112\n",
      "Error in line 671113\n",
      "Error in line 672525\n",
      "Error in line 672526\n",
      "Error in line 673964\n",
      "Error in line 673965\n",
      "Error in line 678400\n",
      "Error in line 678401\n",
      "Error in line 679694\n",
      "Error in line 679695\n",
      "Error in line 692582\n",
      "Error in line 692583\n",
      "Error in line 694726\n",
      "Error in line 694727\n",
      "Error in line 695568\n",
      "Error in line 695569\n",
      "Error in line 696446\n",
      "Error in line 697263\n",
      "Error in line 697264\n",
      "Error in line 697265\n",
      "Error in line 697299\n",
      "Error in line 697300\n",
      "Error in line 698149\n",
      "Error in line 698151\n",
      "Error in line 698152\n",
      "Error in line 700133\n",
      "Error in line 700134\n",
      "Error in line 702148\n",
      "Error in line 702149\n",
      "Error in line 704102\n",
      "Error in line 704103\n",
      "Error in line 704527\n",
      "Error in line 704528\n",
      "Error in line 705092\n",
      "Error in line 705093\n",
      "Error in line 707011\n",
      "Error in line 708176\n",
      "Error in line 708177\n",
      "Error in line 711734\n",
      "Error in line 711735\n",
      "Error in line 711746\n",
      "Error in line 711747\n",
      "Error in line 711752\n",
      "Error in line 711753\n",
      "Error in line 711754\n",
      "Error in line 711894\n",
      "Error in line 711895\n",
      "Error in line 711905\n",
      "Error in line 711907\n",
      "Error in line 712168\n",
      "Error in line 712169\n",
      "Error in line 712170\n",
      "Error in line 712321\n",
      "Error in line 715109\n",
      "Error in line 716867\n",
      "Error in line 716869\n",
      "Error in line 717667\n",
      "Error in line 717668\n",
      "Error in line 720778\n",
      "Error in line 720779\n",
      "Error in line 721563\n",
      "Error in line 721564\n",
      "Error in line 722100\n",
      "Error in line 722101\n",
      "Error in line 722107\n",
      "Error in line 722108\n",
      "Error in line 722164\n",
      "Error in line 722228\n",
      "Error in line 722230\n",
      "Error in line 722232\n",
      "Error in line 722233\n",
      "Error in line 722234\n",
      "Error in line 722280\n",
      "Error in line 722319\n",
      "Error in line 722365\n",
      "Error in line 722383\n",
      "Error in line 723392\n",
      "Error in line 723393\n",
      "Error in line 723978\n",
      "Error in line 723979\n",
      "Error in line 724460\n",
      "Error in line 724461\n",
      "Error in line 730960\n",
      "Error in line 730961\n",
      "Error in line 730987\n",
      "Error in line 730988\n",
      "Error in line 732972\n",
      "Error in line 732973\n",
      "Error in line 733197\n",
      "Error in line 733198\n",
      "Error in line 733199\n",
      "Error in line 733236\n",
      "Error in line 733237\n",
      "Error in line 733238\n",
      "Error in line 733966\n",
      "Error in line 733967\n",
      "Error in line 733968\n",
      "Error in line 736592\n",
      "Error in line 736593\n",
      "Error in line 736594\n",
      "Error in line 736618\n",
      "Error in line 736619\n",
      "Error in line 736800\n",
      "Error in line 736801\n",
      "Error in line 739178\n",
      "Error in line 739179\n",
      "Error in line 739186\n",
      "Error in line 739187\n",
      "Error in line 739358\n",
      "Error in line 743200\n",
      "Error in line 743201\n",
      "Error in line 743202\n",
      "Error in line 744192\n",
      "Error in line 744193\n",
      "Error in line 746377\n",
      "Error in line 746378\n",
      "Error in line 746379\n",
      "Error in line 747382\n",
      "Error in line 747383\n",
      "Error in line 747384\n",
      "Error in line 749317\n",
      "Error in line 749318\n",
      "Error in line 749431\n",
      "Error in line 749432\n",
      "Error in line 756546\n",
      "Error in line 756547\n",
      "Error in line 760477\n",
      "Error in line 760478\n",
      "Error in line 766474\n",
      "Error in line 766483\n",
      "Error in line 766484\n",
      "Error in line 766501\n",
      "Error in line 766502\n",
      "Error in line 766503\n",
      "Error in line 770716\n",
      "Error in line 770717\n",
      "Error in line 771720\n",
      "Error in line 771721\n",
      "Error in line 774599\n",
      "Error in line 774601\n",
      "Error in line 774623\n",
      "Error in line 774624\n",
      "Error in line 774625\n",
      "Error in line 774674\n",
      "Error in line 774675\n",
      "Error in line 774733\n",
      "Error in line 774734\n",
      "Error in line 774751\n",
      "Error in line 774752\n",
      "Error in line 774773\n",
      "Error in line 774774\n",
      "Error in line 774796\n",
      "Error in line 774797\n",
      "Error in line 774798\n",
      "Error in line 777530\n",
      "Error in line 777531\n",
      "Error in line 777532\n",
      "Error in line 777533\n",
      "Error in line 777534\n",
      "Error in line 778214\n",
      "Error in line 778215\n",
      "Error in line 778279\n",
      "Error in line 778280\n",
      "Error in line 780917\n",
      "Error in line 781341\n",
      "Error in line 781342\n",
      "Error in line 784005\n",
      "Error in line 784006\n",
      "Error in line 785714\n",
      "Error in line 785715\n",
      "Error in line 785722\n",
      "Error in line 785723\n",
      "Error in line 786813\n",
      "Error in line 786814\n",
      "Error in line 788229\n",
      "Error in line 788230\n",
      "Error in line 797711\n",
      "Error in line 797712\n",
      "Error in line 797713\n",
      "Error in line 797974\n",
      "Error in line 797975\n",
      "Error in line 797976\n",
      "Error in line 803954\n",
      "Error in line 803955\n",
      "Error in line 803956\n",
      "Error in line 807719\n",
      "Error in line 807720\n",
      "Error in line 810008\n",
      "Error in line 810009\n",
      "Error in line 810010\n",
      "Error in line 810011\n",
      "Error in line 810012\n",
      "Error in line 811323\n",
      "Error in line 811324\n",
      "Error in line 813783\n",
      "Error in line 813784\n",
      "Error in line 813789\n",
      "Error in line 813790\n",
      "Error in line 815375\n",
      "Error in line 815376\n",
      "Error in line 815400\n",
      "Error in line 815401\n",
      "Error in line 817369\n",
      "Error in line 820498\n",
      "Error in line 820499\n",
      "Error in line 820500\n",
      "Error in line 821477\n",
      "Error in line 824787\n",
      "Error in line 824789\n",
      "Error in line 825201\n",
      "Error in line 825202\n",
      "Error in line 825863\n",
      "Error in line 825866\n",
      "Error in line 825919\n",
      "Error in line 825920\n",
      "Error in line 826051\n",
      "Error in line 827463\n",
      "Error in line 827464\n",
      "Error in line 833839\n",
      "Error in line 833841\n",
      "Error in line 834764\n",
      "Error in line 834765\n",
      "Error in line 834766\n",
      "Error in line 834954\n",
      "Error in line 834956\n",
      "Error in line 834973\n",
      "Error in line 834974\n",
      "Error in line 835057\n",
      "Error in line 835058\n",
      "Error in line 835089\n",
      "Error in line 835090\n",
      "Error in line 835111\n",
      "Error in line 835112\n",
      "Error in line 835151\n",
      "Error in line 835152\n",
      "Error in line 838360\n",
      "Error in line 838399\n",
      "Error in line 838400\n",
      "Error in line 840456\n",
      "Error in line 840457\n",
      "Error in line 842203\n",
      "Error in line 842204\n",
      "Error in line 843242\n",
      "Error in line 843243\n",
      "Error in line 843244\n",
      "Error in line 844640\n",
      "Error in line 844641\n",
      "Error in line 844642\n",
      "Error in line 844689\n",
      "Error in line 844690\n",
      "Error in line 844691\n",
      "Error in line 846522\n",
      "Error in line 846523\n",
      "Error in line 850228\n",
      "Error in line 850229\n",
      "Error in line 853080\n",
      "Error in line 853082\n",
      "Error in line 853083\n",
      "Error in line 853152\n",
      "Error in line 853153\n",
      "Error in line 853213\n",
      "Error in line 853214\n",
      "Error in line 855881\n",
      "Error in line 855882\n",
      "Error in line 856362\n",
      "Error in line 856363\n",
      "Error in line 860320\n",
      "Error in line 860321\n",
      "Error in line 860322\n",
      "Error in line 860333\n",
      "Error in line 860334\n",
      "Error in line 860567\n",
      "Error in line 862483\n",
      "Error in line 865650\n",
      "Error in line 865651\n",
      "Error in line 866598\n",
      "Error in line 866599\n",
      "Error in line 866661\n",
      "Error in line 866662\n",
      "Error in line 867187\n",
      "Error in line 867188\n",
      "Error in line 867189\n",
      "Error in line 867190\n",
      "Error in line 867911\n",
      "Error in line 867912\n",
      "Error in line 869615\n",
      "Error in line 869616\n",
      "Error in line 869793\n",
      "Error in line 869794\n",
      "Error in line 870189\n",
      "Error in line 870190\n",
      "Error in line 870465\n",
      "Error in line 870466\n",
      "Error in line 870467\n",
      "Error in line 870468\n",
      "Error in line 870469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 872473\n",
      "Error in line 872474\n",
      "Error in line 873089\n",
      "Error in line 873090\n",
      "Error in line 873091\n",
      "Error in line 873112\n",
      "Error in line 873116\n",
      "Error in line 873117\n",
      "Error in line 873119\n",
      "Error in line 873120\n",
      "Error in line 873121\n",
      "Error in line 873130\n",
      "Error in line 873131\n",
      "Error in line 873146\n",
      "Error in line 879263\n",
      "Error in line 879264\n",
      "Error in line 879265\n",
      "Error in line 880670\n",
      "Error in line 880671\n",
      "Error in line 880691\n",
      "Error in line 880692\n",
      "Error in line 886596\n",
      "Error in line 886597\n",
      "Error in line 886598\n",
      "Error in line 887373\n",
      "Error in line 887374\n",
      "Error in line 887375\n",
      "Error in line 889892\n",
      "Error in line 889893\n",
      "Error in line 889894\n",
      "Error in line 890251\n",
      "Error in line 890252\n",
      "Error in line 890253\n",
      "Error in line 890816\n",
      "Error in line 890817\n",
      "Error in line 890818\n",
      "Error in line 891486\n",
      "Error in line 891488\n",
      "Error in line 891489\n",
      "Error in line 891879\n",
      "Error in line 891880\n",
      "Error in line 891881\n",
      "Error in line 891898\n",
      "Error in line 891899\n",
      "Error in line 895064\n",
      "Error in line 895065\n",
      "Error in line 895066\n",
      "Error in line 898726\n",
      "Error in line 898727\n",
      "Error in line 903093\n",
      "Error in line 903094\n",
      "Error in line 903561\n",
      "Error in line 903563\n",
      "Error in line 903707\n",
      "Error in line 903709\n",
      "Error in line 906756\n",
      "Error in line 906757\n",
      "Error in line 906758\n",
      "Error in line 909717\n",
      "Error in line 909718\n",
      "Error in line 910105\n",
      "Error in line 910106\n",
      "Error in line 912111\n",
      "Error in line 912112\n",
      "Error in line 912521\n",
      "Error in line 912522\n",
      "Error in line 912523\n",
      "Error in line 913781\n",
      "Error in line 913782\n",
      "Error in line 914986\n",
      "Error in line 914987\n",
      "Error in line 915792\n",
      "Error in line 915794\n",
      "Error in line 917557\n",
      "Error in line 917558\n",
      "Error in line 918075\n",
      "Error in line 918076\n",
      "Error in line 918805\n",
      "Error in line 918806\n",
      "Error in line 920707\n",
      "Error in line 920708\n",
      "Error in line 922707\n",
      "Error in line 922708\n",
      "Error in line 924225\n",
      "Error in line 924226\n",
      "Error in line 924227\n",
      "Error in line 924239\n",
      "Error in line 924240\n",
      "Error in line 925049\n",
      "Error in line 925050\n",
      "Error in line 925051\n",
      "Error in line 925549\n",
      "Error in line 925550\n",
      "Error in line 925551\n",
      "Error in line 926229\n",
      "Error in line 926230\n",
      "Error in line 926231\n",
      "Error in line 927551\n",
      "Error in line 927554\n",
      "Error in line 927556\n",
      "Error in line 927558\n",
      "Error in line 927560\n",
      "Error in line 927600\n",
      "Error in line 927601\n",
      "Error in line 927602\n",
      "Error in line 927611\n",
      "Error in line 927612\n",
      "Error in line 927650\n",
      "Error in line 927651\n",
      "Error in line 927652\n",
      "Error in line 927653\n",
      "Error in line 927700\n",
      "Error in line 927701\n",
      "Error in line 927776\n",
      "Error in line 927777\n",
      "Error in line 927778\n",
      "Error in line 927820\n",
      "Error in line 927821\n",
      "Error in line 927822\n",
      "Error in line 927865\n",
      "Error in line 927866\n",
      "Error in line 927867\n",
      "Error in line 927919\n",
      "Error in line 927920\n",
      "Error in line 927986\n",
      "Error in line 927987\n",
      "Error in line 927988\n",
      "Error in line 928024\n",
      "Error in line 928025\n",
      "Error in line 928643\n",
      "Error in line 931492\n",
      "Error in line 932603\n",
      "Error in line 932604\n",
      "Error in line 932623\n",
      "Error in line 932624\n",
      "Error in line 934118\n",
      "Error in line 934119\n",
      "Error in line 935154\n",
      "Error in line 935155\n",
      "Error in line 936410\n",
      "Error in line 936411\n",
      "Error in line 940972\n",
      "Error in line 940973\n",
      "Error in line 943258\n",
      "Error in line 943260\n",
      "Error in line 943298\n",
      "Error in line 943299\n",
      "Error in line 943300\n",
      "Error in line 943312\n",
      "Error in line 943313\n",
      "Error in line 943327\n",
      "Error in line 943470\n",
      "Error in line 943471\n",
      "Error in line 943472\n",
      "Error in line 943757\n",
      "Error in line 943758\n",
      "Error in line 944504\n",
      "Error in line 946996\n",
      "Error in line 946997\n",
      "Error in line 946998\n",
      "Error in line 948679\n",
      "Error in line 948680\n",
      "Error in line 951834\n",
      "Error in line 951835\n",
      "Error in line 951836\n",
      "Error in line 951837\n",
      "Error in line 954174\n",
      "Error in line 954175\n",
      "Error in line 954176\n",
      "Error in line 954180\n",
      "Error in line 954181\n",
      "Error in line 960288\n",
      "Error in line 960289\n",
      "Error in line 960651\n",
      "Error in line 960652\n",
      "Error in line 963074\n",
      "Error in line 963075\n",
      "Error in line 963076\n",
      "Error in line 963144\n",
      "Error in line 963145\n",
      "Error in line 963146\n",
      "Error in line 963254\n",
      "Error in line 963255\n",
      "Error in line 963256\n",
      "Error in line 965347\n",
      "Error in line 965348\n",
      "Error in line 966838\n",
      "Error in line 966839\n",
      "Error in line 968180\n",
      "Error in line 968181\n",
      "Error in line 968182\n",
      "Error in line 969684\n",
      "Error in line 969685\n",
      "Error in line 969743\n",
      "Error in line 969744\n",
      "Error in line 969745\n",
      "Error in line 969749\n",
      "Error in line 969750\n",
      "Error in line 969760\n",
      "Error in line 969761\n",
      "Error in line 969898\n",
      "Error in line 969910\n",
      "Error in line 969911\n",
      "Error in line 971024\n",
      "Error in line 971025\n",
      "Error in line 971026\n",
      "Error in line 972263\n",
      "Error in line 972264\n",
      "Error in line 982326\n",
      "Error in line 982327\n",
      "Error in line 982924\n",
      "Error in line 982925\n",
      "Error in line 989964\n",
      "Error in line 989965\n",
      "Error in line 990377\n",
      "Error in line 992469\n",
      "Error in line 992470\n",
      "Error in line 992471\n",
      "Error in line 994562\n",
      "Error in line 994563\n",
      "Error in line 994564\n",
      "Error in line 996550\n",
      "Error in line 996551\n",
      "Error in line 999770\n",
      "Error in line 999771\n",
      "Error in line 1005223\n",
      "Error in line 1005224\n",
      "Error in line 1010402\n",
      "Error in line 1010403\n",
      "Error in line 1010404\n",
      "Error in line 1010445\n",
      "Error in line 1010446\n",
      "Error in line 1011378\n",
      "Error in line 1011379\n",
      "Error in line 1016283\n",
      "Error in line 1017965\n",
      "Error in line 1017966\n",
      "Error in line 1017967\n",
      "Error in line 1025067\n",
      "Error in line 1025068\n",
      "Error in line 1025069\n",
      "Error in line 1027293\n",
      "Error in line 1027294\n",
      "Error in line 1027295\n",
      "Error in line 1029583\n",
      "Error in line 1029585\n",
      "Error in line 1034118\n",
      "Error in line 1034119\n",
      "Error in line 1034120\n",
      "Error in line 1034206\n",
      "Error in line 1034207\n",
      "Error in line 1037871\n",
      "Error in line 1037872\n",
      "Error in line 1038505\n",
      "Error in line 1038506\n",
      "Error in line 1039054\n",
      "Error in line 1039055\n",
      "Error in line 1040800\n",
      "Error in line 1040801\n",
      "Error in line 1042622\n",
      "Error in line 1044000\n",
      "Error in line 1044124\n",
      "Error in line 1044125\n",
      "Error in line 1054118\n",
      "Error in line 1054119\n",
      "Error in line 1054137\n",
      "Error in line 1054138\n",
      "Error in line 1054157\n",
      "Error in line 1056105\n",
      "Error in line 1056106\n",
      "Error in line 1056162\n",
      "Error in line 1056163\n",
      "Error in line 1056463\n",
      "Error in line 1056464\n",
      "Error in line 1056465\n",
      "Error in line 1057691\n",
      "Error in line 1057692\n",
      "Error in line 1060760\n",
      "Error in line 1060761\n",
      "Error in line 1061540\n",
      "Error in line 1061541\n",
      "Error in line 1061726\n",
      "Error in line 1061727\n",
      "Error in line 1061728\n",
      "Error in line 1062236\n",
      "Error in line 1062237\n",
      "Error in line 1062277\n",
      "Error in line 1062278\n",
      "Error in line 1062468\n",
      "Error in line 1067280\n",
      "Error in line 1067289\n",
      "Error in line 1067290\n",
      "Error in line 1074063\n",
      "Error in line 1074064\n",
      "Error in line 1074145\n",
      "Error in line 1074146\n",
      "Error in line 1074147\n",
      "Error in line 1075397\n",
      "Error in line 1075398\n",
      "Error in line 1075399\n",
      "Error in line 1077512\n",
      "Error in line 1077513\n",
      "Error in line 1078741\n",
      "Error in line 1078742\n",
      "Error in line 1081465\n",
      "Error in line 1081466\n",
      "Error in line 1081843\n",
      "Error in line 1081844\n",
      "Error in line 1083923\n",
      "Error in line 1084895\n",
      "Error in line 1084896\n",
      "Error in line 1087362\n",
      "Error in line 1087363\n",
      "Error in line 1087364\n",
      "Error in line 1087371\n",
      "Error in line 1087372\n",
      "Error in line 1087853\n",
      "Error in line 1087854\n",
      "Error in line 1090546\n",
      "Error in line 1090548\n",
      "Error in line 1093188\n",
      "Error in line 1093189\n",
      "Error in line 1093190\n",
      "Error in line 1094430\n",
      "Error in line 1094431\n",
      "Error in line 1094432\n",
      "Error in line 1096678\n",
      "Error in line 1096679\n",
      "Error in line 1097271\n",
      "Error in line 1097272\n",
      "Error in line 1099471\n",
      "Error in line 1099472\n",
      "Error in line 1099476\n",
      "Error in line 1099477\n",
      "Error in line 1099478\n",
      "Error in line 1099491\n",
      "Error in line 1099492\n",
      "Error in line 1099493\n",
      "Error in line 1099497\n",
      "Error in line 1099498\n",
      "Error in line 1099982\n",
      "Error in line 1099983\n",
      "Error in line 1100366\n",
      "Error in line 1100367\n",
      "Error in line 1101923\n",
      "Error in line 1101924\n",
      "Error in line 1107285\n",
      "Error in line 1107286\n",
      "Error in line 1110108\n",
      "Error in line 1110109\n",
      "Error in line 1110110\n",
      "Error in line 1110834\n",
      "Error in line 1110835\n",
      "Error in line 1115142\n",
      "Error in line 1115143\n",
      "Error in line 1121166\n",
      "Error in line 1121167\n",
      "Error in line 1121578\n",
      "Error in line 1121579\n",
      "Error in line 1132671\n",
      "Error in line 1132672\n",
      "Error in line 1132705\n",
      "Error in line 1132706\n",
      "Error in line 1132900\n",
      "Error in line 1132901\n",
      "Error in line 1138705\n",
      "Error in line 1138707\n",
      "Error in line 1140044\n",
      "Error in line 1140045\n",
      "Error in line 1140046\n",
      "Error in line 1141026\n",
      "Error in line 1141027\n",
      "Error in line 1142429\n",
      "Error in line 1145182\n",
      "Error in line 1145183\n",
      "Error in line 1149465\n",
      "Error in line 1149466\n",
      "Error in line 1149467\n",
      "Error in line 1149468\n",
      "Error in line 1149469\n",
      "Error in line 1150679\n",
      "Error in line 1150680\n",
      "Error in line 1151322\n",
      "Error in line 1151324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 1153658\n",
      "Error in line 1153659\n",
      "Error in line 1153660\n",
      "Error in line 1154612\n",
      "Error in line 1154613\n",
      "Error in line 1154995\n",
      "Error in line 1154996\n",
      "Error in line 1158137\n",
      "Error in line 1158138\n",
      "Error in line 1158139\n",
      "Error in line 1158354\n",
      "Error in line 1158990\n",
      "Error in line 1158991\n",
      "Error in line 1159960\n",
      "Error in line 1159961\n",
      "Error in line 1159962\n",
      "Error in line 1161217\n",
      "Error in line 1161218\n",
      "Error in line 1161219\n",
      "Error in line 1161734\n",
      "Error in line 1161735\n",
      "Error in line 1162171\n",
      "Error in line 1162172\n",
      "Error in line 1162173\n",
      "Error in line 1165073\n",
      "Error in line 1165074\n",
      "Error in line 1165118\n",
      "Error in line 1166302\n",
      "Error in line 1166303\n",
      "Error in line 1167433\n",
      "Error in line 1167437\n",
      "Error in line 1167824\n",
      "Error in line 1167825\n",
      "Error in line 1169282\n",
      "Error in line 1169283\n",
      "Error in line 1170963\n",
      "Error in line 1170964\n",
      "Error in line 1171726\n",
      "Error in line 1171728\n",
      "Error in line 1171729\n",
      "Error in line 1171731\n",
      "Error in line 1172540\n",
      "Error in line 1172541\n",
      "Error in line 1172817\n",
      "Error in line 1172818\n",
      "Error in line 1174641\n",
      "Error in line 1174642\n",
      "Error in line 1177489\n",
      "Error in line 1177490\n",
      "Error in line 1178938\n",
      "Error in line 1178939\n",
      "Error in line 1178952\n",
      "Error in line 1178953\n",
      "Error in line 1178972\n",
      "Error in line 1178973\n",
      "Error in line 1184504\n",
      "Error in line 1187173\n",
      "Error in line 1187174\n",
      "Error in line 1187354\n",
      "Error in line 1187356\n",
      "Error in line 1187357\n",
      "Error in line 1187358\n",
      "Error in line 1190783\n",
      "Error in line 1191059\n",
      "Error in line 1191060\n",
      "Error in line 1195115\n",
      "Error in line 1195116\n",
      "Error in line 1199768\n",
      "Error in line 1199769\n",
      "Error in line 1199853\n",
      "Error in line 1209090\n",
      "Error in line 1209091\n",
      "Error in line 1209092\n",
      "Error in line 1209922\n",
      "Error in line 1209923\n",
      "Error in line 1209924\n",
      "Error in line 1211256\n",
      "Error in line 1211257\n",
      "Error in line 1211285\n",
      "Error in line 1212672\n",
      "Error in line 1212673\n",
      "Error in line 1214105\n",
      "Error in line 1214106\n",
      "Error in line 1215736\n",
      "Error in line 1215742\n",
      "Error in line 1215743\n",
      "Error in line 1216442\n",
      "Error in line 1216443\n",
      "Error in line 1217755\n",
      "Error in line 1217756\n",
      "Error in line 1220574\n",
      "Error in line 1220576\n",
      "Error in line 1220578\n",
      "Error in line 1222580\n",
      "Error in line 1222581\n",
      "Error in line 1224906\n",
      "Error in line 1231135\n",
      "Error in line 1231136\n",
      "Error in line 1232510\n",
      "Error in line 1232511\n",
      "Error in line 1233571\n",
      "Error in line 1239484\n",
      "Error in line 1239485\n",
      "Error in line 1239486\n",
      "Error in line 1239491\n",
      "Error in line 1239492\n",
      "Error in line 1239915\n",
      "Error in line 1239916\n",
      "Error in line 1239944\n",
      "Error in line 1239946\n",
      "Error in line 1239963\n",
      "Error in line 1239964\n",
      "Error in line 1243204\n",
      "Error in line 1243205\n",
      "Error in line 1243225\n",
      "Error in line 1243226\n",
      "Error in line 1246328\n",
      "Error in line 1246329\n",
      "Error in line 1247277\n",
      "Error in line 1247278\n",
      "Error in line 1247279\n",
      "Error in line 1248615\n",
      "Error in line 1254677\n",
      "Error in line 1254678\n",
      "Error in line 1255243\n",
      "Error in line 1255244\n",
      "Error in line 1258007\n",
      "Error in line 1258008\n",
      "Error in line 1258009\n",
      "Error in line 1258010\n",
      "Error in line 1258011\n",
      "Error in line 1258012\n",
      "Error in line 1258014\n",
      "Error in line 1258015\n",
      "Error in line 1258016\n",
      "Error in line 1258025\n",
      "Error in line 1258026\n",
      "Error in line 1258170\n",
      "Error in line 1258171\n",
      "Error in line 1258653\n",
      "Error in line 1258654\n",
      "Error in line 1258975\n",
      "Error in line 1258976\n",
      "Error in line 1258977\n",
      "Error in line 1261250\n",
      "Error in line 1261254\n",
      "Error in line 1261256\n",
      "Error in line 1261268\n",
      "Error in line 1261269\n",
      "Error in line 1261270\n",
      "Error in line 1262482\n",
      "Error in line 1262483\n",
      "Error in line 1262484\n",
      "Error in line 1266646\n",
      "Error in line 1266647\n",
      "Error in line 1266648\n",
      "Error in line 1267550\n",
      "Error in line 1267551\n",
      "Error in line 1267552\n",
      "Error in line 1270081\n",
      "Error in line 1270082\n",
      "Error in line 1270083\n",
      "Error in line 1273084\n",
      "Error in line 1273085\n",
      "Error in line 1276316\n",
      "Error in line 1276317\n",
      "Error in line 1277130\n",
      "Error in line 1277142\n",
      "Error in line 1277143\n",
      "Error in line 1277157\n",
      "Error in line 1277158\n",
      "Error in line 1277159\n",
      "Error in line 1279418\n",
      "Error in line 1279419\n",
      "Error in line 1279960\n",
      "Error in line 1279961\n",
      "Error in line 1282402\n",
      "Error in line 1282403\n",
      "Error in line 1282404\n",
      "Error in line 1285067\n",
      "Error in line 1285068\n",
      "Error in line 1291425\n",
      "Error in line 1291426\n",
      "Error in line 1291427\n",
      "Error in line 1291698\n",
      "Error in line 1291699\n",
      "Error in line 1291700\n",
      "Error in line 1292900\n",
      "Error in line 1292901\n",
      "Error in line 1295636\n",
      "Error in line 1295637\n",
      "Error in line 1295639\n",
      "Error in line 1295640\n",
      "Error in line 1295641\n",
      "Error in line 1297364\n",
      "Error in line 1297365\n",
      "Error in line 1297499\n",
      "Error in line 1297500\n",
      "Error in line 1297569\n",
      "Error in line 1297570\n",
      "Error in line 1297637\n",
      "Error in line 1297639\n",
      "Error in line 1297640\n",
      "Error in line 1301735\n",
      "Error in line 1301736\n",
      "Error in line 1301738\n",
      "Error in line 1304243\n",
      "Error in line 1304245\n",
      "Error in line 1304246\n",
      "Error in line 1309409\n",
      "Error in line 1309410\n",
      "Error in line 1309412\n",
      "Error in line 1310758\n",
      "Error in line 1310759\n",
      "Error in line 1310764\n",
      "Error in line 1310765\n",
      "Error in line 1310766\n",
      "Error in line 1311228\n",
      "Error in line 1311229\n",
      "Error in line 1311234\n",
      "Error in line 1311235\n",
      "Error in line 1311236\n",
      "Error in line 1311508\n",
      "Error in line 1311509\n",
      "Error in line 1311510\n",
      "Error in line 1311572\n",
      "Error in line 1311573\n",
      "Error in line 1312694\n",
      "Error in line 1312695\n",
      "Error in line 1314987\n",
      "Error in line 1314988\n",
      "Error in line 1314989\n",
      "Error in line 1315051\n",
      "Error in line 1315052\n",
      "Error in line 1315320\n",
      "Error in line 1315321\n",
      "Error in line 1315945\n",
      "Error in line 1315947\n",
      "Error in line 1317020\n",
      "Error in line 1317021\n",
      "Error in line 1321189\n",
      "Error in line 1321190\n",
      "Error in line 1323903\n",
      "Error in line 1323904\n",
      "Error in line 1324173\n",
      "Error in line 1324174\n",
      "Error in line 1325075\n",
      "Error in line 1325076\n",
      "Error in line 1325077\n",
      "Error in line 1325939\n",
      "Error in line 1325940\n",
      "Error in line 1326784\n",
      "Error in line 1326785\n",
      "Error in line 1330530\n",
      "Error in line 1330531\n",
      "Error in line 1331556\n",
      "Error in line 1331557\n",
      "Error in line 1331586\n",
      "Error in line 1331587\n",
      "Error in line 1331588\n",
      "Error in line 1331589\n",
      "Error in line 1331590\n",
      "Error in line 1331591\n",
      "Error in line 1337176\n",
      "Error in line 1337177\n",
      "Error in line 1337878\n",
      "Error in line 1337879\n",
      "Error in line 1339634\n",
      "Error in line 1339635\n",
      "Error in line 1339636\n",
      "Error in line 1339756\n",
      "Error in line 1339757\n",
      "Error in line 1342976\n",
      "Error in line 1342977\n",
      "Error in line 1344391\n",
      "Error in line 1344393\n",
      "Error in line 1345567\n",
      "Error in line 1345568\n",
      "Error in line 1345569\n",
      "Error in line 1345575\n",
      "Error in line 1345576\n",
      "Error in line 1345577\n",
      "Error in line 1346160\n",
      "Error in line 1346161\n",
      "Error in line 1346162\n",
      "Error in line 1347416\n",
      "Error in line 1347417\n",
      "Error in line 1347418\n",
      "Error in line 1347668\n",
      "Error in line 1347669\n",
      "Error in line 1347670\n",
      "Error in line 1347912\n",
      "Error in line 1347913\n",
      "Error in line 1347914\n",
      "Error in line 1347915\n",
      "Error in line 1347916\n",
      "Error in line 1347917\n",
      "Error in line 1347918\n",
      "Error in line 1347919\n",
      "Error in line 1347920\n",
      "Error in line 1347921\n",
      "Error in line 1347922\n",
      "Error in line 1347923\n",
      "Error in line 1347924\n",
      "Error in line 1347925\n",
      "Error in line 1347926\n",
      "Error in line 1347927\n",
      "Error in line 1347928\n",
      "Error in line 1347929\n",
      "Error in line 1347930\n",
      "Error in line 1347931\n",
      "Error in line 1347932\n",
      "Error in line 1347933\n",
      "Error in line 1347934\n",
      "Error in line 1347935\n",
      "Error in line 1347936\n",
      "Error in line 1347937\n",
      "Error in line 1347938\n",
      "Error in line 1347939\n",
      "Error in line 1347940\n",
      "Error in line 1347941\n",
      "Error in line 1347942\n",
      "Error in line 1347943\n",
      "Error in line 1347944\n",
      "Error in line 1347945\n",
      "Error in line 1347946\n",
      "Error in line 1347947\n",
      "Error in line 1347948\n",
      "Error in line 1347949\n",
      "Error in line 1347950\n",
      "Error in line 1347951\n",
      "Error in line 1347952\n",
      "Error in line 1347953\n",
      "Error in line 1347954\n",
      "Error in line 1347955\n",
      "Error in line 1347956\n",
      "Error in line 1347957\n",
      "Error in line 1347958\n",
      "Error in line 1347959\n",
      "Error in line 1347960\n",
      "Error in line 1347961\n",
      "Error in line 1347962\n",
      "Error in line 1347963\n",
      "Error in line 1347964\n",
      "Error in line 1347965\n",
      "Error in line 1347966\n",
      "Error in line 1347967\n",
      "Error in line 1347968\n",
      "Error in line 1347969\n",
      "Error in line 1347970\n",
      "Error in line 1347971\n",
      "Error in line 1347972\n",
      "Error in line 1347973\n",
      "Error in line 1347974\n",
      "Error in line 1347975\n",
      "Error in line 1347976\n",
      "Error in line 1347977\n",
      "Error in line 1347978\n",
      "Error in line 1347979\n",
      "Error in line 1347980\n",
      "Error in line 1347981\n",
      "Error in line 1347982\n",
      "Error in line 1347983\n",
      "Error in line 1347984\n",
      "Error in line 1347985\n",
      "Error in line 1347986\n",
      "Error in line 1347987\n",
      "Error in line 1347988\n",
      "Error in line 1347989\n",
      "Error in line 1347990\n",
      "Error in line 1347991\n",
      "Error in line 1347992\n",
      "Error in line 1347993\n",
      "Error in line 1347994\n",
      "Error in line 1347995\n",
      "Error in line 1347996\n",
      "Error in line 1347997\n",
      "Error in line 1347998\n",
      "Error in line 1347999\n",
      "Error in line 1348000\n",
      "Error in line 1348001\n",
      "Error in line 1348002\n",
      "Error in line 1348003\n",
      "Error in line 1348004\n",
      "Error in line 1348005\n",
      "Error in line 1348006\n",
      "Error in line 1348007\n",
      "Error in line 1348008\n",
      "Error in line 1348009\n",
      "Error in line 1348010\n",
      "Error in line 1348011\n",
      "Error in line 1348012\n",
      "Error in line 1348013\n",
      "Error in line 1348014\n",
      "Error in line 1348015\n",
      "Error in line 1348016\n",
      "Error in line 1348017\n",
      "Error in line 1348018\n",
      "Error in line 1348019\n",
      "Error in line 1348020\n",
      "Error in line 1348021\n",
      "Error in line 1348022\n",
      "Error in line 1348023\n",
      "Error in line 1348024\n",
      "Error in line 1348025\n",
      "Error in line 1348026\n",
      "Error in line 1348027\n",
      "Error in line 1348028\n",
      "Error in line 1348029\n",
      "Error in line 1348030\n",
      "Error in line 1348031\n",
      "Error in line 1348032\n",
      "Error in line 1348033\n",
      "Error in line 1348034\n",
      "Error in line 1348035\n",
      "Error in line 1348036\n",
      "Error in line 1348038\n",
      "Error in line 1348039\n",
      "Error in line 1348040\n",
      "Error in line 1348041\n",
      "Error in line 1348042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in line 1349550\n",
      "Error in line 1349551\n",
      "Error in line 1350187\n",
      "Error in line 1352026\n",
      "Error in line 1352027\n",
      "Error in line 1355018\n",
      "Error in line 1355019\n",
      "Error in line 1355443\n",
      "Error in line 1355444\n",
      "Error in line 1355512\n",
      "Error in line 1355513\n",
      "Error in line 1355515\n",
      "Error in line 1355516\n",
      "Error in line 1358193\n",
      "Error in line 1359646\n",
      "Error in line 1359647\n",
      "Error in line 1361558\n",
      "Error in line 1361559\n",
      "Error in line 1363397\n",
      "Error in line 1363399\n",
      "Error in line 1366642\n",
      "Error in line 1366643\n",
      "Error in line 1370710\n",
      "Error in line 1370738\n",
      "Error in line 1370739\n",
      "Error in line 1370740\n",
      "Error in line 1370752\n",
      "Error in line 1370753\n",
      "Error in line 1370755\n",
      "Error in line 1370756\n",
      "Error in line 1370758\n",
      "Error in line 1370759\n",
      "Error in line 1371626\n",
      "Error in line 1371627\n",
      "Error in line 1373669\n",
      "Error in line 1373670\n",
      "Error in line 1373678\n",
      "Error in line 1373679\n",
      "Error in line 1373680\n",
      "Error in line 1375099\n",
      "Error in line 1376833\n",
      "Error in line 1376834\n",
      "Error in line 1377670\n",
      "Error in line 1377671\n",
      "Error in line 1382713\n",
      "Error in line 1382714\n",
      "Error in line 1383746\n",
      "Error in line 1383747\n",
      "Error in line 1387894\n",
      "Error in line 1387895\n",
      "Error in line 1390562\n",
      "Error in line 1390563\n",
      "Error in line 1393368\n",
      "Error in line 1395327\n",
      "Error in line 1395328\n",
      "Error in line 1397017\n",
      "Error in line 1397035\n",
      "Error in line 1397037\n",
      "Error in line 1397038\n",
      "Error in line 1397053\n",
      "Error in line 1397055\n",
      "Error in line 1397066\n",
      "Error in line 1397345\n",
      "Error in line 1397346\n",
      "Error in line 1397378\n",
      "Error in line 1397379\n",
      "Error in line 1397388\n",
      "Error in line 1397389\n",
      "Error in line 1397390\n",
      "Error in line 1397402\n",
      "Error in line 1397403\n",
      "Error in line 1400485\n",
      "Error in line 1400486\n",
      "Error in line 1401877\n",
      "Error in line 1401879\n",
      "Error in line 1401880\n",
      "Error in line 1401942\n",
      "Error in line 1401943\n",
      "Error in line 1401944\n",
      "Error in line 1401987\n",
      "Error in line 1401988\n",
      "Error in line 1402016\n",
      "Error in line 1402017\n",
      "Error in line 1402018\n",
      "Error in line 1402026\n",
      "Error in line 1402027\n",
      "Error in line 1402041\n",
      "Error in line 1402042\n",
      "Error in line 1402143\n",
      "Error in line 1402144\n",
      "Error in line 1402145\n",
      "Error in line 1402633\n",
      "Error in line 1406085\n",
      "Error in line 1406086\n",
      "Error in line 1409806\n",
      "Error in line 1409807\n",
      "Error in line 1409808\n",
      "Error in line 1411060\n",
      "Error in line 1411061\n",
      "Error in line 1413379\n",
      "Error in line 1413380\n",
      "Error in line 1413405\n",
      "Error in line 1413406\n",
      "Error in line 1416215\n",
      "Error in line 1416216\n",
      "Error in line 1416217\n",
      "Error in line 1416535\n",
      "Error in line 1416536\n",
      "Error in line 1416539\n",
      "Error in line 1416540\n",
      "Error in line 1416541\n",
      "Error in line 1418302\n",
      "Error in line 1418342\n",
      "Error in line 1418343\n",
      "Error in line 1418344\n",
      "Error in line 1419913\n",
      "Error in line 1419914\n",
      "Error in line 1420478\n",
      "Error in line 1420479\n",
      "Error in line 1422057\n",
      "Error in line 1422058\n",
      "Error in line 1426786\n",
      "Error in line 1432896\n",
      "Error in line 1432897\n",
      "Error in line 1437450\n",
      "Error in line 1437496\n",
      "Error in line 1437497\n",
      "Error in line 1437555\n",
      "Error in line 1437556\n",
      "Error in line 1437577\n",
      "Error in line 1437578\n",
      "Error in line 1437636\n",
      "Error in line 1437637\n",
      "Error in line 1437638\n",
      "Error in line 1437655\n",
      "Error in line 1437656\n",
      "Error in line 1439455\n",
      "Error in line 1443748\n",
      "Error in line 1444687\n",
      "Error in line 1444688\n",
      "Error in line 1444698\n",
      "Error in line 1444699\n",
      "Error in line 1444700\n",
      "Error in line 1444738\n",
      "Error in line 1444740\n",
      "Error in line 1444741\n",
      "Error in line 1444742\n",
      "Error in line 1444798\n",
      "Error in line 1444799\n",
      "Error in line 1444800\n",
      "Error in line 1444801\n",
      "Error in line 1444847\n",
      "Error in line 1444883\n",
      "Error in line 1444884\n",
      "Error in line 1446048\n",
      "Error in line 1446049\n",
      "Error in line 1446050\n"
     ]
    }
   ],
   "source": [
    "# Log problematic lines\n",
    "with open(os.path.join(folder_path, rotten_movie_review_file), 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            pd.read_csv(StringIO(line))\n",
    "        except:\n",
    "            print(f\"Error in line {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652aec51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_chunk.csv split.\n",
      "1_chunk.csv split.\n",
      "2_chunk.csv split.\n",
      "3_chunk.csv split.\n",
      "4_chunk.csv split.\n",
      "5_chunk.csv split.\n",
      "6_chunk.csv split.\n",
      "7_chunk.csv split.\n",
      "8_chunk.csv split.\n",
      "9_chunk.csv split.\n",
      "10_chunk.csv split.\n",
      "11_chunk.csv split.\n",
      "12_chunk.csv split.\n",
      "13_chunk.csv split.\n",
      "14_chunk.csv split.\n",
      "15_chunk.csv split.\n",
      "16_chunk.csv split.\n",
      "17_chunk.csv split.\n",
      "18_chunk.csv split.\n",
      "19_chunk.csv split.\n",
      "20_chunk.csv split.\n",
      "21_chunk.csv split.\n",
      "22_chunk.csv split.\n",
      "23_chunk.csv split.\n",
      "24_chunk.csv split.\n",
      "25_chunk.csv split.\n",
      "26_chunk.csv split.\n",
      "27_chunk.csv split.\n",
      "28_chunk.csv split.\n",
      "29_chunk.csv split.\n",
      "30_chunk.csv split.\n",
      "31_chunk.csv split.\n",
      "32_chunk.csv split.\n",
      "33_chunk.csv split.\n",
      "34_chunk.csv split.\n",
      "35_chunk.csv split.\n",
      "36_chunk.csv split.\n",
      "37_chunk.csv split.\n",
      "38_chunk.csv split.\n",
      "39_chunk.csv split.\n",
      "40_chunk.csv split.\n",
      "41_chunk.csv split.\n",
      "42_chunk.csv split.\n",
      "43_chunk.csv split.\n",
      "44_chunk.csv split.\n",
      "45_chunk.csv split.\n",
      "46_chunk.csv split.\n",
      "47_chunk.csv split.\n",
      "48_chunk.csv split.\n",
      "49_chunk.csv split.\n",
      "50_chunk.csv split.\n",
      "51_chunk.csv split.\n",
      "52_chunk.csv split.\n",
      "53_chunk.csv split.\n",
      "54_chunk.csv split.\n",
      "55_chunk.csv split.\n",
      "56_chunk.csv split.\n",
      "57_chunk.csv split.\n",
      "58_chunk.csv split.\n",
      "59_chunk.csv split.\n",
      "60_chunk.csv split.\n",
      "61_chunk.csv split.\n",
      "62_chunk.csv split.\n",
      "63_chunk.csv split.\n",
      "64_chunk.csv split.\n",
      "65_chunk.csv split.\n",
      "66_chunk.csv split.\n",
      "67_chunk.csv split.\n",
      "68_chunk.csv split.\n",
      "69_chunk.csv split.\n",
      "70_chunk.csv split.\n",
      "71_chunk.csv split.\n",
      "72_chunk.csv split.\n",
      "73_chunk.csv split.\n",
      "74_chunk.csv split.\n",
      "75_chunk.csv split.\n",
      "76_chunk.csv split.\n",
      "77_chunk.csv split.\n",
      "78_chunk.csv split.\n",
      "79_chunk.csv split.\n",
      "80_chunk.csv split.\n",
      "81_chunk.csv split.\n",
      "82_chunk.csv split.\n",
      "83_chunk.csv split.\n",
      "84_chunk.csv split.\n",
      "85_chunk.csv split.\n",
      "86_chunk.csv split.\n",
      "87_chunk.csv split.\n",
      "88_chunk.csv split.\n",
      "89_chunk.csv split.\n",
      "90_chunk.csv split.\n",
      "91_chunk.csv split.\n",
      "92_chunk.csv split.\n",
      "93_chunk.csv split.\n",
      "94_chunk.csv split.\n",
      "95_chunk.csv split.\n",
      "96_chunk.csv split.\n",
      "97_chunk.csv split.\n",
      "98_chunk.csv split.\n",
      "99_chunk.csv split.\n"
     ]
    }
   ],
   "source": [
    "# Create the output folder if it does not exist\n",
    "create_folder(split_path)\n",
    "\n",
    "# Read and split the DataFrame into smaller chunks\n",
    "df_reviews = pd.read_csv(os.path.join(folder_path, rotten_movie_review_file), chunksize=chunk_size)\n",
    "for i, chunk in enumerate(df_reviews):\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "    chunk.to_csv(os.path.join(split_path, f'{i}_chunk.csv'), index=False)\n",
    "    print(f\"{f'{i}_chunk.csv'} split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c23f905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_chunk.csv split.\n",
      "1_chunk.csv split.\n",
      "2_chunk.csv split.\n",
      "3_chunk.csv split.\n",
      "4_chunk.csv split.\n",
      "5_chunk.csv split.\n",
      "6_chunk.csv split.\n",
      "7_chunk.csv split.\n",
      "8_chunk.csv split.\n",
      "9_chunk.csv split.\n",
      "10_chunk.csv split.\n",
      "11_chunk.csv split.\n",
      "12_chunk.csv split.\n",
      "13_chunk.csv split.\n",
      "14_chunk.csv split.\n",
      "15_chunk.csv split.\n",
      "16_chunk.csv split.\n",
      "17_chunk.csv split.\n",
      "18_chunk.csv split.\n",
      "19_chunk.csv split.\n",
      "20_chunk.csv split.\n",
      "21_chunk.csv split.\n",
      "22_chunk.csv split.\n",
      "23_chunk.csv split.\n",
      "24_chunk.csv split.\n",
      "25_chunk.csv split.\n",
      "26_chunk.csv split.\n",
      "27_chunk.csv split.\n",
      "28_chunk.csv split.\n",
      "29_chunk.csv split.\n",
      "30_chunk.csv split.\n",
      "31_chunk.csv split.\n",
      "32_chunk.csv split.\n",
      "33_chunk.csv split.\n",
      "34_chunk.csv split.\n",
      "35_chunk.csv split.\n",
      "36_chunk.csv split.\n",
      "37_chunk.csv split.\n",
      "38_chunk.csv split.\n",
      "39_chunk.csv split.\n",
      "40_chunk.csv split.\n",
      "41_chunk.csv split.\n",
      "42_chunk.csv split.\n",
      "43_chunk.csv split.\n",
      "44_chunk.csv split.\n",
      "45_chunk.csv split.\n",
      "46_chunk.csv split.\n",
      "47_chunk.csv split.\n",
      "48_chunk.csv split.\n",
      "49_chunk.csv split.\n",
      "50_chunk.csv split.\n",
      "51_chunk.csv split.\n",
      "52_chunk.csv split.\n",
      "53_chunk.csv split.\n",
      "54_chunk.csv split.\n",
      "55_chunk.csv split.\n",
      "56_chunk.csv split.\n",
      "57_chunk.csv split.\n",
      "58_chunk.csv split.\n",
      "59_chunk.csv split.\n",
      "60_chunk.csv split.\n",
      "61_chunk.csv split.\n",
      "62_chunk.csv split.\n",
      "63_chunk.csv split.\n",
      "64_chunk.csv split.\n",
      "65_chunk.csv split.\n",
      "66_chunk.csv split.\n",
      "67_chunk.csv split.\n",
      "68_chunk.csv split.\n",
      "69_chunk.csv split.\n",
      "70_chunk.csv split.\n",
      "71_chunk.csv split.\n",
      "72_chunk.csv split.\n",
      "73_chunk.csv split.\n",
      "74_chunk.csv split.\n",
      "75_chunk.csv split.\n",
      "76_chunk.csv split.\n",
      "77_chunk.csv split.\n",
      "78_chunk.csv split.\n",
      "79_chunk.csv split.\n",
      "80_chunk.csv split.\n",
      "81_chunk.csv split.\n",
      "82_chunk.csv split.\n",
      "83_chunk.csv split.\n",
      "84_chunk.csv split.\n",
      "85_chunk.csv split.\n",
      "86_chunk.csv split.\n",
      "87_chunk.csv split.\n",
      "88_chunk.csv split.\n",
      "89_chunk.csv split.\n",
      "90_chunk.csv split.\n",
      "91_chunk.csv split.\n",
      "92_chunk.csv split.\n",
      "93_chunk.csv split.\n",
      "94_chunk.csv split.\n",
      "95_chunk.csv split.\n",
      "96_chunk.csv split.\n",
      "97_chunk.csv split.\n",
      "98_chunk.csv split.\n",
      "99_chunk.csv split.\n"
     ]
    }
   ],
   "source": [
    "# Read the first chunk to get the number of columns\n",
    "first_chunk = pd.read_csv(os.path.join(folder_path, rotten_movie_review_file), nrows=5)\n",
    "num_columns = first_chunk.shape[1]\n",
    "\n",
    "# Read and split the DataFrame into smaller chunks\n",
    "df_reviews = pd.read_csv(os.path.join(folder_path, rotten_movie_review_file), chunksize=chunk_size)\n",
    "for i, chunk in enumerate(df_reviews):\n",
    "    if i >= num_chunks:\n",
    "        break\n",
    "    # Check if the chunk has the correct number of columns\n",
    "    if chunk.shape[1] != num_columns:\n",
    "        print(f\"Skipping chunk {i} due to incorrect number of columns.\")\n",
    "        continue\n",
    "    chunk.to_csv(os.path.join(split_path, f'{i}_chunk.csv'), index=False)\n",
    "    print(f\"{f'{i}_chunk.csv'} split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb0a8f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing IDs Count: 465\n",
      "0_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 371\n",
      "1_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 315\n",
      "2_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 354\n",
      "3_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 366\n",
      "4_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 391\n",
      "5_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 345\n",
      "6_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 355\n",
      "7_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 368\n",
      "8_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 354\n",
      "9_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 400\n",
      "10_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 349\n",
      "11_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 327\n",
      "12_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 415\n",
      "13_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 338\n",
      "14_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 328\n",
      "15_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 368\n",
      "16_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 368\n",
      "17_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 400\n",
      "18_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 390\n",
      "19_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 320\n",
      "20_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 320\n",
      "21_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 377\n",
      "22_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 373\n",
      "23_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 360\n",
      "24_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 386\n",
      "25_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 306\n",
      "26_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 375\n",
      "27_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 387\n",
      "28_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 290\n",
      "29_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 439\n",
      "30_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 390\n",
      "31_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 338\n",
      "32_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 314\n",
      "33_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 282\n",
      "34_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 305\n",
      "35_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 365\n",
      "36_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 393\n",
      "37_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 367\n",
      "38_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 347\n",
      "39_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 382\n",
      "40_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 336\n",
      "41_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 320\n",
      "42_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 315\n",
      "43_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 349\n",
      "44_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 352\n",
      "45_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 324\n",
      "46_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 393\n",
      "47_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 342\n",
      "48_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 326\n",
      "49_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 346\n",
      "50_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 385\n",
      "51_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 364\n",
      "52_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 287\n",
      "53_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 354\n",
      "54_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 326\n",
      "55_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 327\n",
      "56_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 388\n",
      "57_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 418\n",
      "58_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 363\n",
      "59_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 343\n",
      "60_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 301\n",
      "61_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 334\n",
      "62_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 335\n",
      "63_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 334\n",
      "64_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 368\n",
      "65_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 397\n",
      "66_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 341\n",
      "67_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 377\n",
      "68_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 320\n",
      "69_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 436\n",
      "70_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 333\n",
      "71_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 337\n",
      "72_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 300\n",
      "73_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 327\n",
      "74_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 348\n",
      "75_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 368\n",
      "77_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 319\n",
      "78_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 373\n",
      "79_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 316\n",
      "80_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 345\n",
      "81_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 390\n",
      "82_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 292\n",
      "83_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 341\n",
      "84_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 390\n",
      "85_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 386\n",
      "86_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 418\n",
      "87_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 390\n",
      "88_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 348\n",
      "89_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 299\n",
      "90_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 354\n",
      "91_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 388\n",
      "92_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 363\n",
      "93_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 353\n",
      "94_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 319\n",
      "95_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 315\n",
      "96_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 377\n",
      "97_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 401\n",
      "98_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n",
      "Missing IDs Count: 226\n",
      "99_chunk.csv cleaned and 'title_id' added to /Users/toniwork/Desktop/Capstone/split_1\n"
     ]
    }
   ],
   "source": [
    "# Initialize i for the while loop\n",
    "i = 0\n",
    "\n",
    "# Loop through the smaller CSV files and apply the add_title_ids_2 function to each one\n",
    "while True:\n",
    "    file_path = os.path.join(split_path, f'{i}_chunk.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        break\n",
    "    chunk_df = pd.read_csv(file_path)\n",
    "    add_title_ids_2(rotten_movie_file, f'{i}_chunk.csv', on_col, split_path, split_path, compare_path)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f17c7384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with blank 'reviewtext' dropped from 0_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 1_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 2_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 3_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 4_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 5_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 6_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 7_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 8_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 9_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 10_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 11_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 12_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 13_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 14_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 15_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 16_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 17_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 18_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 19_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 20_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 21_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 22_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 23_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 24_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 25_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 26_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 27_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 28_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 29_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 30_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 31_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 32_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 33_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 34_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 35_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 36_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 37_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 38_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 39_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 40_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 41_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 42_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 43_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 44_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 45_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 46_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 47_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 48_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 49_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 50_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 51_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 52_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 53_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 54_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 55_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 56_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 57_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 58_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 59_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 60_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 61_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 62_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 63_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 64_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 65_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 66_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 67_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 68_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 69_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 70_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 71_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 72_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 73_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 74_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 75_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 76_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 77_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 78_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 79_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 80_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 81_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 82_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 83_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 84_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 85_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 86_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 87_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 88_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 89_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 90_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 91_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 92_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 93_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 94_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 95_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 96_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 97_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 98_chunk.csv\n",
      "Rows with blank 'reviewtext' dropped from 99_chunk.csv\n"
     ]
    }
   ],
   "source": [
    "# Drop rows were text is blank \n",
    "i = 0\n",
    "\n",
    "# Loop through the smaller CSV files to drop rows where 'reviewtext' is blank\n",
    "while True:\n",
    "    file_path = os.path.join(split_path, f'{i}_chunk.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        break\n",
    "    chunk_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop rows where 'reviewtext' is blank or NaN\n",
    "    chunk_df.dropna(subset=['reviewtext'], inplace=True)\n",
    "    \n",
    "    # Save the cleaned DataFrame back to the file\n",
    "    chunk_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"Rows with blank 'reviewtext' dropped from {f'{i}_chunk.csv'}\")\n",
    "    \n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8911a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_chunk.csv has 7 columns.\n",
      "1_chunk.csv has 7 columns.\n",
      "2_chunk.csv has 7 columns.\n",
      "3_chunk.csv has 7 columns.\n",
      "4_chunk.csv has 7 columns.\n",
      "5_chunk.csv has 7 columns.\n",
      "6_chunk.csv has 7 columns.\n",
      "7_chunk.csv has 7 columns.\n",
      "8_chunk.csv has 7 columns.\n",
      "9_chunk.csv has 7 columns.\n",
      "10_chunk.csv has 7 columns.\n",
      "11_chunk.csv has 7 columns.\n",
      "12_chunk.csv has 7 columns.\n",
      "13_chunk.csv has 7 columns.\n",
      "14_chunk.csv has 7 columns.\n",
      "15_chunk.csv has 7 columns.\n",
      "16_chunk.csv has 7 columns.\n",
      "17_chunk.csv has 7 columns.\n",
      "18_chunk.csv has 7 columns.\n",
      "19_chunk.csv has 7 columns.\n",
      "20_chunk.csv has 7 columns.\n",
      "21_chunk.csv has 7 columns.\n",
      "22_chunk.csv has 7 columns.\n",
      "23_chunk.csv has 7 columns.\n",
      "24_chunk.csv has 7 columns.\n",
      "25_chunk.csv has 7 columns.\n",
      "26_chunk.csv has 7 columns.\n",
      "27_chunk.csv has 7 columns.\n",
      "28_chunk.csv has 7 columns.\n",
      "29_chunk.csv has 7 columns.\n",
      "30_chunk.csv has 7 columns.\n",
      "31_chunk.csv has 7 columns.\n",
      "32_chunk.csv has 7 columns.\n",
      "33_chunk.csv has 7 columns.\n",
      "34_chunk.csv has 7 columns.\n",
      "35_chunk.csv has 7 columns.\n",
      "36_chunk.csv has 7 columns.\n",
      "37_chunk.csv has 7 columns.\n",
      "38_chunk.csv has 7 columns.\n",
      "39_chunk.csv has 7 columns.\n",
      "40_chunk.csv has 7 columns.\n",
      "41_chunk.csv has 7 columns.\n",
      "42_chunk.csv has 7 columns.\n",
      "43_chunk.csv has 7 columns.\n",
      "44_chunk.csv has 7 columns.\n",
      "45_chunk.csv has 7 columns.\n",
      "46_chunk.csv has 7 columns.\n",
      "47_chunk.csv has 7 columns.\n",
      "48_chunk.csv has 7 columns.\n",
      "49_chunk.csv has 7 columns.\n",
      "50_chunk.csv has 7 columns.\n",
      "51_chunk.csv has 7 columns.\n",
      "52_chunk.csv has 7 columns.\n",
      "53_chunk.csv has 7 columns.\n",
      "54_chunk.csv has 7 columns.\n",
      "55_chunk.csv has 7 columns.\n",
      "56_chunk.csv has 7 columns.\n",
      "57_chunk.csv has 7 columns.\n",
      "58_chunk.csv has 7 columns.\n",
      "59_chunk.csv has 7 columns.\n",
      "60_chunk.csv has 7 columns.\n",
      "61_chunk.csv has 7 columns.\n",
      "62_chunk.csv has 7 columns.\n",
      "63_chunk.csv has 7 columns.\n",
      "64_chunk.csv has 7 columns.\n",
      "65_chunk.csv has 7 columns.\n",
      "66_chunk.csv has 7 columns.\n",
      "67_chunk.csv has 7 columns.\n",
      "68_chunk.csv has 7 columns.\n",
      "69_chunk.csv has 7 columns.\n",
      "70_chunk.csv has 7 columns.\n",
      "71_chunk.csv has 7 columns.\n",
      "72_chunk.csv has 7 columns.\n",
      "73_chunk.csv has 7 columns.\n",
      "74_chunk.csv has 7 columns.\n",
      "75_chunk.csv has 7 columns.\n",
      "76_chunk.csv has 7 columns.\n",
      "77_chunk.csv has 7 columns.\n",
      "78_chunk.csv has 7 columns.\n",
      "79_chunk.csv has 7 columns.\n",
      "80_chunk.csv has 7 columns.\n",
      "81_chunk.csv has 7 columns.\n",
      "82_chunk.csv has 7 columns.\n",
      "83_chunk.csv has 7 columns.\n",
      "84_chunk.csv has 7 columns.\n",
      "85_chunk.csv has 7 columns.\n",
      "86_chunk.csv has 7 columns.\n",
      "87_chunk.csv has 7 columns.\n",
      "88_chunk.csv has 7 columns.\n",
      "89_chunk.csv has 7 columns.\n",
      "90_chunk.csv has 7 columns.\n",
      "91_chunk.csv has 7 columns.\n",
      "92_chunk.csv has 7 columns.\n",
      "93_chunk.csv has 7 columns.\n",
      "94_chunk.csv has 7 columns.\n",
      "95_chunk.csv has 7 columns.\n",
      "96_chunk.csv has 7 columns.\n",
      "97_chunk.csv has 7 columns.\n",
      "98_chunk.csv has 7 columns.\n",
      "99_chunk.csv has 7 columns.\n",
      "Number of columns for each chunk: {'0_chunk.csv': 7, '1_chunk.csv': 7, '2_chunk.csv': 7, '3_chunk.csv': 7, '4_chunk.csv': 7, '5_chunk.csv': 7, '6_chunk.csv': 7, '7_chunk.csv': 7, '8_chunk.csv': 7, '9_chunk.csv': 7, '10_chunk.csv': 7, '11_chunk.csv': 7, '12_chunk.csv': 7, '13_chunk.csv': 7, '14_chunk.csv': 7, '15_chunk.csv': 7, '16_chunk.csv': 7, '17_chunk.csv': 7, '18_chunk.csv': 7, '19_chunk.csv': 7, '20_chunk.csv': 7, '21_chunk.csv': 7, '22_chunk.csv': 7, '23_chunk.csv': 7, '24_chunk.csv': 7, '25_chunk.csv': 7, '26_chunk.csv': 7, '27_chunk.csv': 7, '28_chunk.csv': 7, '29_chunk.csv': 7, '30_chunk.csv': 7, '31_chunk.csv': 7, '32_chunk.csv': 7, '33_chunk.csv': 7, '34_chunk.csv': 7, '35_chunk.csv': 7, '36_chunk.csv': 7, '37_chunk.csv': 7, '38_chunk.csv': 7, '39_chunk.csv': 7, '40_chunk.csv': 7, '41_chunk.csv': 7, '42_chunk.csv': 7, '43_chunk.csv': 7, '44_chunk.csv': 7, '45_chunk.csv': 7, '46_chunk.csv': 7, '47_chunk.csv': 7, '48_chunk.csv': 7, '49_chunk.csv': 7, '50_chunk.csv': 7, '51_chunk.csv': 7, '52_chunk.csv': 7, '53_chunk.csv': 7, '54_chunk.csv': 7, '55_chunk.csv': 7, '56_chunk.csv': 7, '57_chunk.csv': 7, '58_chunk.csv': 7, '59_chunk.csv': 7, '60_chunk.csv': 7, '61_chunk.csv': 7, '62_chunk.csv': 7, '63_chunk.csv': 7, '64_chunk.csv': 7, '65_chunk.csv': 7, '66_chunk.csv': 7, '67_chunk.csv': 7, '68_chunk.csv': 7, '69_chunk.csv': 7, '70_chunk.csv': 7, '71_chunk.csv': 7, '72_chunk.csv': 7, '73_chunk.csv': 7, '74_chunk.csv': 7, '75_chunk.csv': 7, '76_chunk.csv': 7, '77_chunk.csv': 7, '78_chunk.csv': 7, '79_chunk.csv': 7, '80_chunk.csv': 7, '81_chunk.csv': 7, '82_chunk.csv': 7, '83_chunk.csv': 7, '84_chunk.csv': 7, '85_chunk.csv': 7, '86_chunk.csv': 7, '87_chunk.csv': 7, '88_chunk.csv': 7, '89_chunk.csv': 7, '90_chunk.csv': 7, '91_chunk.csv': 7, '92_chunk.csv': 7, '93_chunk.csv': 7, '94_chunk.csv': 7, '95_chunk.csv': 7, '96_chunk.csv': 7, '97_chunk.csv': 7, '98_chunk.csv': 7, '99_chunk.csv': 7}\n"
     ]
    }
   ],
   "source": [
    "# Count columns to ensure all have 7 column prior to merge\n",
    "i = 0\n",
    "\n",
    "# Initialize an empty dictionary to store the number of columns for each chunk\n",
    "chunk_columns_count = {}\n",
    "\n",
    "# Loop through the smaller CSV files to count the number of columns\n",
    "while True:\n",
    "    file_path = os.path.join(split_path, f'{i}_chunk.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        break\n",
    "    chunk_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count the number of columns and store it in the dictionary\n",
    "    chunk_columns_count[f'{i}_chunk.csv'] = chunk_df.shape[1]\n",
    "    \n",
    "    print(f\"{f'{i}_chunk.csv'} has {chunk_df.shape[1]} columns.\")\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Print the dictionary to see the number of columns for each chunk\n",
    "print(\"Number of columns for each chunk:\", chunk_columns_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfab84c5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunk files concatenated and saved to /Users/toniwork/Desktop/Capstone/step_2\n"
     ]
    }
   ],
   "source": [
    "# Takes all the \"chunk.csv\" files from the input folder, groups them into three equal parts, and concatenates these parts into three separate CSV files, saving them to the output folder.\n",
    "input_folder = '/Users/toniwork/Desktop/Capstone/split_1'\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "# Get all files in the input folder that end with \"chunk.csv\"\n",
    "all_files = [f for f in os.listdir(input_folder) if f.endswith(\"chunk.csv\")]\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated chunks\n",
    "concatenated_df = pd.DataFrame()\n",
    "\n",
    "# Loop through all chunk files\n",
    "for i, file_name in enumerate(all_files):\n",
    "    # Read in chunk file as a DataFrame\n",
    "    chunk_df = pd.read_csv(os.path.join(input_folder, file_name))\n",
    "    \n",
    "    # Append chunk DataFrame to the concatenated DataFrame\n",
    "    concatenated_df = pd.concat([concatenated_df, chunk_df], ignore_index=True)\n",
    "\n",
    "# Split the concatenated DataFrame into three equal parts\n",
    "dfs = np.array_split(concatenated_df, 3)\n",
    "\n",
    "# Save each part as a separate CSV file\n",
    "for i, df in enumerate(dfs):\n",
    "    output_file = os.path.join(output_folder, f\"rotten_tomatoes_movie_reviews_concat{i+1}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"All chunk files concatenated and saved to {output_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aecbbf3f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7256 entries, 0 to 7255\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     7256 non-null   object \n",
      " 1   domestic_box_office       7256 non-null   float64\n",
      " 2   international_box_office  5040 non-null   float64\n",
      " 3   genre                     7220 non-null   object \n",
      " 4   worldwide_box_office      5040 non-null   float64\n",
      " 5   release_year              7255 non-null   float64\n",
      " 6   release_month             7254 non-null   float64\n",
      " 7   release_day               7250 non-null   float64\n",
      " 8   release_date              7255 non-null   object \n",
      " 9   id                        7256 non-null   int64  \n",
      " 10  languages                 7256 non-null   object \n",
      " 11  title_id                  7256 non-null   int64  \n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 680.4+ KB\n",
      "movies_dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9345 entries, 0 to 9344\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           9345 non-null   object \n",
      " 1   rating          8560 non-null   object \n",
      " 2   user_rating     7674 non-null   float64\n",
      " 3   website_rating  9345 non-null   int64  \n",
      " 4   release_date    9345 non-null   object \n",
      " 5   release_year    9345 non-null   int64  \n",
      " 6   release_month   9345 non-null   int64  \n",
      " 7   release_day     9345 non-null   int64  \n",
      " 8   title_id        9345 non-null   int64  \n",
      "dtypes: float64(1), int64(5), object(3)\n",
      "memory usage: 657.2+ KB\n",
      "metacritic-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4112 entries, 0 to 4111\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   title          4112 non-null   object \n",
      " 1   review         3183 non-null   object \n",
      " 2   review_date    3913 non-null   object \n",
      " 3   comment_count  3823 non-null   float64\n",
      " 4   like_count     2978 non-null   float64\n",
      " 5   release_year   4096 non-null   float64\n",
      " 6   title_id       4112 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(3)\n",
      "memory usage: 225.0+ KB\n",
      "letterboxd-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361424 entries, 0 to 361423\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   reviewtext      361424 non-null  object\n",
      " 1   scoresentiment  361424 non-null  object\n",
      " 2   id              361424 non-null  object\n",
      " 3   istopcritic     361424 non-null  bool  \n",
      " 4   originalscore   254035 non-null  object\n",
      " 5   creationdate    361424 non-null  object\n",
      " 6   title_id        361424 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(5)\n",
      "memory usage: 16.9+ MB\n",
      "rotten_tomatoes_movie_reviews_concat2.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361423 entries, 0 to 361422\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   reviewtext      361423 non-null  object\n",
      " 1   scoresentiment  361423 non-null  object\n",
      " 2   id              361423 non-null  object\n",
      " 3   istopcritic     361423 non-null  bool  \n",
      " 4   originalscore   253040 non-null  object\n",
      " 5   creationdate    361423 non-null  object\n",
      " 6   title_id        361423 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(5)\n",
      "memory usage: 16.9+ MB\n",
      "rotten_tomatoes_movie_reviews_concat3.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361424 entries, 0 to 361423\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   reviewtext      361424 non-null  object\n",
      " 1   scoresentiment  361424 non-null  object\n",
      " 2   id              361424 non-null  object\n",
      " 3   istopcritic     361424 non-null  bool  \n",
      " 4   originalscore   252972 non-null  object\n",
      " 5   creationdate    361424 non-null  object\n",
      " 6   title_id        361424 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(5)\n",
      "memory usage: 16.9+ MB\n",
      "rotten_tomatoes_movie_reviews_concat1.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56213 entries, 0 to 56212\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   title                  56213 non-null  object \n",
      " 1   audience_score         36808 non-null  float64\n",
      " 2   tomato_meter           21458 non-null  float64\n",
      " 3   director               55559 non-null  object \n",
      " 4   id                     56213 non-null  object \n",
      " 5   box_office             66 non-null     float64\n",
      " 6   release_date_theaters  24231 non-null  object \n",
      " 7   original_language      56213 non-null  object \n",
      " 8   title_id               56213 non-null  int64  \n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 3.9+ MB\n",
      "rotten_tomatoes_movies.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221683 entries, 0 to 221682\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        221683 non-null  int64 \n",
      " 1   credits   144631 non-null  object\n",
      " 2   title_id  221683 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 5.1+ MB\n",
      "movies_extras.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10293 entries, 0 to 10292\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   movie_title  10293 non-null  object \n",
      " 1   rating       10028 non-null  float64\n",
      " 2   user_rating  2393 non-null   float64\n",
      " 3   director     10293 non-null  object \n",
      " 4   top_5_casts  10293 non-null  object \n",
      " 5   year         10293 non-null  int64  \n",
      " 6   title_id     10293 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 563.0+ KB\n",
      "25k IMDb movie Dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222729 entries, 0 to 222728\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   title              222729 non-null  object \n",
      " 1   genres             152291 non-null  object \n",
      " 2   popularity         222729 non-null  float64\n",
      " 3   budget             222729 non-null  float64\n",
      " 4   revenue            222729 non-null  float64\n",
      " 5   vote_average       222729 non-null  float64\n",
      " 6   vote_count         222729 non-null  float64\n",
      " 7   id                 222729 non-null  int64  \n",
      " 8   release_date       217449 non-null  object \n",
      " 9   original_language  222729 non-null  object \n",
      " 10  title_id           222729 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(4)\n",
      "memory usage: 18.7+ MB\n",
      "movies.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print file info from folder to verify \n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5e282",
   "metadata": {},
   "source": [
    "### Delete files, drop columns, rename columns\n",
    "- Delete files/folders to clean up space utilization\n",
    "- Drop columns that are no longer needed, such as release date information\n",
    "- Rename columns in order to prepare for merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d5a46eb",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Delete folder/files\n",
    "folder_path1 = '/Users/toniwork/Desktop/Capstone/step_1'\n",
    "folder_path2 = '/Users/toniwork/Desktop/Capstone/split_1'\n",
    "\n",
    "# Remove the first folder and its contents\n",
    "# shutil.rmtree(folder_path1)\n",
    "\n",
    "# Remove the second folder and its contents\n",
    "shutil.rmtree(folder_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "271fc693",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_dataset.csv updated successfully!\n",
      "metacritic-reviews.csv updated successfully!\n",
      "letterboxd-reviews.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat2.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat3.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat1.csv updated successfully!\n",
      "rotten_tomatoes_movies.csv updated successfully!\n",
      "movies_extras.csv updated successfully!\n",
      "25k IMDb movie Dataset.csv updated successfully!\n",
      "movies.csv updated successfully!\n",
      "movies_dataset.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat1.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat2.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat3.csv updated successfully!\n",
      "rotten_tomatoes_movies.csv updated successfully!\n",
      "25k IMDb movie Dataset.csv updated successfully!\n",
      "movies.csv updated successfully!\n",
      "movies_extras.csv updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Drop columns using drop_columns_by_keyword_or_name function\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "drop_columns_by_keyword_or_name(folder_path, keyword='release')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id', 'languages', 'production_companies'], file_name='movies_dataset.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id'], file_name='rotten_tomatoes_movie_reviews_concat1.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id'], file_name='rotten_tomatoes_movie_reviews_concat2.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id'], file_name='rotten_tomatoes_movie_reviews_concat3.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id', 'original_language'], file_name='rotten_tomatoes_movies.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['year'], file_name='25k IMDb movie Dataset.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id', 'original_language'], file_name='movies.csv')\n",
    "drop_columns_by_keyword_or_name(folder_path, column_names=['id'], file_name='movies_extras.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdc7eb2b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Rename rating column to mpaa_rating in metacritic-reviews.csv\n",
    "df = pd.read_csv('/Users/toniwork/Desktop/Capstone/step_2/metacritic-reviews.csv')\n",
    "\n",
    "# Define a dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    'rating': 'mpaa_rating'\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "df.to_csv('/Users/toniwork/Desktop/Capstone/step_2/metacritic-reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cab9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename rating column to mpaa_rating in metacritic-reviews.csv\n",
    "df = pd.read_csv('/Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv')\n",
    "\n",
    "# Define a dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    'movie_title': 'title'\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "df.to_csv('/Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb21a098",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Rename columns from the rotten_tomatoes_movie_reviews_concat1-3.csv's\n",
    "new_col_names = {\n",
    "    'reviewtext': 'review_text',\n",
    "    'creationdate': 'creation_date',\n",
    "    'istopcritic': 'rt_is_top_critic',\n",
    "    'originalscore': 'rt_original_score',\n",
    "    'scoresentiment' : 'rt_score_sentiment'\n",
    "}\n",
    "\n",
    "# loop through the file paths\n",
    "for file_path in ['/Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movie_reviews_concat1.csv', \n",
    "                  '/Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movie_reviews_concat2.csv', \n",
    "                  '/Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movie_reviews_concat3.csv']:\n",
    "    \n",
    "    # load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # update the column names\n",
    "    df.rename(columns=new_col_names, inplace=True)\n",
    "    \n",
    "    # save the updated DataFrame to the same file path\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8fabe",
   "metadata": {},
   "source": [
    "I am dropping these specific rows from various CSV files to ensure data quality and consistency across my dataset. By removing rows with missing or null values in key columns, I am eliminating potential inaccuracies and focusing on the data that is most relevant to my analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee81d63f",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed movies_dataset.csv\n",
      "Processed metacritic-reviews.csv\n",
      "Processed letterboxd-reviews.csv\n",
      "Processed rotten_tomatoes_movie_reviews_concat2.csv\n",
      "Processed rotten_tomatoes_movie_reviews_concat3.csv\n",
      "Processed rotten_tomatoes_movie_reviews_concat1.csv\n",
      "Processed rotten_tomatoes_movies.csv\n",
      "Processed movies_extras.csv\n",
      "Processed 25k IMDb movie Dataset.csv\n",
      "Processed movies.csv\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows for data consistency\n",
    "\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "file_patterns_to_columns = {\n",
    "    'rotten_tomatoes_movie_reviews_concat': ['review_text'],\n",
    "    'movies_extras.csv': ['credits'],\n",
    "    'letterboxd-reviews.csv': ['review'],\n",
    "    'rotten_tomatoes_movies.csv': {'subset': ['audience_score', 'tomato_meter', 'director'], 'how': 'all'}\n",
    "}\n",
    "\n",
    "# Iterate through the files and apply drop_rows function\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        drop_rows(file_path, file_patterns_to_columns)\n",
    "        print(f\"Processed {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5046e36b",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: movies_dataset.csv, Title duplicates: 122\n",
      "File: movies_dataset.csv, Title_id duplicates: 122\n",
      "File: metacritic-reviews.csv, Title duplicates: 283\n",
      "File: metacritic-reviews.csv, Title_id duplicates: 283\n",
      "File: letterboxd-reviews.csv, Title duplicates: 2773\n",
      "File: letterboxd-reviews.csv, Title_id duplicates: 2848\n",
      "File: rotten_tomatoes_movie_reviews_concat2.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat2.csv, Title_id duplicates: 359162\n",
      "File: rotten_tomatoes_movie_reviews_concat3.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat3.csv, Title_id duplicates: 354250\n",
      "File: rotten_tomatoes_movie_reviews_concat1.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat1.csv, Title_id duplicates: 356110\n",
      "File: rotten_tomatoes_movies.csv, Title duplicates: 12188\n",
      "File: rotten_tomatoes_movies.csv, Title_id duplicates: 12197\n",
      "File: movies_extras.csv, does not have a 'title' column.\n",
      "File: movies_extras.csv, Title_id duplicates: 19574\n",
      "File: 25k IMDb movie Dataset.csv, Title duplicates: 494\n",
      "File: 25k IMDb movie Dataset.csv, Title_id duplicates: 494\n",
      "File: movies.csv, Title duplicates: 76212\n",
      "File: movies.csv, Title_id duplicates: 76227\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and get count for all updated files in folder\n",
    "folder_path = \"/Users/toniwork/Desktop/Capstone/step_2\"\n",
    "file_pattern = \".csv\" \n",
    "drop_duplicate_rows_and_report(folder_path, file_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2d4990e",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6009 entries, 0 to 6008\n",
      "Data columns (total 6 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     6009 non-null   object \n",
      " 1   domestic_box_office       6009 non-null   float64\n",
      " 2   international_box_office  4029 non-null   float64\n",
      " 3   genre                     5974 non-null   object \n",
      " 4   worldwide_box_office      4029 non-null   float64\n",
      " 5   title_id                  6009 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 281.8+ KB\n",
      "movies_dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9345 entries, 0 to 9344\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           9345 non-null   object \n",
      " 1   mpaa_rating     8560 non-null   object \n",
      " 2   user_rating     7674 non-null   float64\n",
      " 3   website_rating  9345 non-null   int64  \n",
      " 4   title_id        9345 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 365.2+ KB\n",
      "metacritic-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3183 entries, 0 to 3182\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   title          3183 non-null   object \n",
      " 1   review         3183 non-null   object \n",
      " 2   review_date    3024 non-null   object \n",
      " 3   comment_count  2957 non-null   float64\n",
      " 4   like_count     2315 non-null   float64\n",
      " 5   title_id       3183 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 149.3+ KB\n",
      "letterboxd-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361212 entries, 0 to 361211\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         361212 non-null  object\n",
      " 1   rt_score_sentiment  361212 non-null  object\n",
      " 2   rt_is_top_critic    361212 non-null  bool  \n",
      " 3   rt_original_score   253876 non-null  object\n",
      " 4   creation_date       361212 non-null  object\n",
      " 5   title_id            361212 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 14.1+ MB\n",
      "rotten_tomatoes_movie_reviews_concat2.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356302 entries, 0 to 356301\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         356302 non-null  object\n",
      " 1   rt_score_sentiment  356302 non-null  object\n",
      " 2   rt_is_top_critic    356302 non-null  bool  \n",
      " 3   rt_original_score   249477 non-null  object\n",
      " 4   creation_date       356302 non-null  object\n",
      " 5   title_id            356302 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 13.9+ MB\n",
      "rotten_tomatoes_movie_reviews_concat3.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 358121 entries, 0 to 358120\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         358121 non-null  object\n",
      " 1   rt_score_sentiment  358121 non-null  object\n",
      " 2   rt_is_top_critic    358121 non-null  bool  \n",
      " 3   rt_original_score   250652 non-null  object\n",
      " 4   creation_date       358121 non-null  object\n",
      " 5   title_id            358121 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 14.0+ MB\n",
      "rotten_tomatoes_movie_reviews_concat1.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55282 entries, 0 to 55281\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           55282 non-null  object \n",
      " 1   audience_score  36504 non-null  float64\n",
      " 2   tomato_meter    21267 non-null  float64\n",
      " 3   director        55070 non-null  object \n",
      " 4   box_office      66 non-null     float64\n",
      " 5   title_id        55282 non-null  int64  \n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 2.5+ MB\n",
      "rotten_tomatoes_movies.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124128 entries, 0 to 124127\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   credits   124128 non-null  object\n",
      " 1   title_id  124128 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.9+ MB\n",
      "movies_extras.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10293 entries, 0 to 10292\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        10293 non-null  object \n",
      " 1   rating       10028 non-null  float64\n",
      " 2   user_rating  2393 non-null   float64\n",
      " 3   director     10293 non-null  object \n",
      " 4   top_5_casts  10293 non-null  object \n",
      " 5   title_id     10293 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 482.6+ KB\n",
      "25k IMDb movie Dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219371 entries, 0 to 219370\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   title         219371 non-null  object \n",
      " 1   genres        151841 non-null  object \n",
      " 2   popularity    219371 non-null  float64\n",
      " 3   budget        219371 non-null  float64\n",
      " 4   revenue       219371 non-null  float64\n",
      " 5   vote_average  219371 non-null  float64\n",
      " 6   vote_count    219371 non-null  float64\n",
      " 7   title_id      219371 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 13.4+ MB\n",
      "movies.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print file info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccf93f",
   "metadata": {},
   "source": [
    "## Create reference files for easier comparison\n",
    "\n",
    "### Creating a file for directors, actors, and genres will make the analysis easier\n",
    "We will create files with a id, the data, and then a comma separated list of the title_ids they appear in\n",
    "- Director\n",
    "    - director_id\n",
    "    - director\n",
    "    - title_ids\n",
    "    - <b>rotten_tomatoes_movies.csv</b> \n",
    "        - director (currently comma separated)\n",
    "    - <b>25k IMDb movie Dataset.csv</b> \n",
    "        - director\n",
    "\n",
    "- Actor\n",
    "    - actor_id\n",
    "    - actor\n",
    "    - title_ids\n",
    "    - <b>movies_extras.csv</b> \n",
    "        - credits (currently separated by -)\n",
    "    - <b>25k IMDb movie Dataset.csv</b> \n",
    "        - top_5_casts (currently in [] and comma separated)\n",
    "\n",
    "- Genre\n",
    "    - genre_id\n",
    "    - genre\n",
    "    - title_ids\n",
    "    - <b>movies_dataset.csv</b> \n",
    "        - genre\n",
    "    - <b>movies.csv</b> \n",
    "        - genres (currently comma separated)\n",
    "        \n",
    "Normalization and Consistency: By separating these entities into individual reference files, we ensure that the information is stored in a normalized and consistent manner. This eliminates redundancy and inconsistencies in the data representation, especially when the information is scattered across different files and formats.\n",
    "\n",
    "Ease of Analysis and Integration: Having dedicated reference files with unique identifiers (director_id, actor_id, genre_id) and associated title_ids allows for more efficient querying, analysis, and integration with other datasets. It simplifies the process of understanding relationships between movies, directors, actors, and genres, and facilitates data-driven insights and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd56e04f",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file /Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movies.csv:\n",
      "Processed DataFrame for file /Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movies.csv:\n",
      "Concatenated result DataFrame after file /Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movies.csv:\n",
      "Read file /Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv:\n",
      "Concatenated result DataFrame after file /Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv:\n",
      "directors.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create directors.csv for reference\n",
    "\n",
    "input_files = ['/Users/toniwork/Desktop/Capstone/step_2/rotten_tomatoes_movies.csv', '/Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv']\n",
    "column_names = ['director', 'director']\n",
    "delimiters = [',', None]\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/reference'\n",
    "output_file = 'directors.csv'\n",
    "merge_columns = 'title_ids'\n",
    "target_column_name = 'directors'\n",
    "\n",
    "create_folder(output_folder)\n",
    "\n",
    "process_data(input_files, column_names, target_column_name, delimiters, output_folder, output_file, merge_columns, num_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f6a1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicated directors found.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate director names\n",
    "directors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv')\n",
    "\n",
    "# Check for duplicates in the 'directors' column\n",
    "duplicated_directors = directors_df[directors_df.duplicated(subset=['directors'], keep=False)]\n",
    "\n",
    "# If there are duplicates, print them\n",
    "if not duplicated_directors.empty:\n",
    "    print(\"Found duplicated directors:\")\n",
    "    print(duplicated_directors)\n",
    "else:\n",
    "    print(\"No duplicated directors found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d225ead",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Make directors title_ids into set, removing duplicates within the set\n",
    "directors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv')\n",
    "\n",
    "# Apply the function to the 'title_ids' column\n",
    "directors_df['title_ids'] = directors_df['title_ids'].apply(remove_duplicates_in_set)\n",
    "\n",
    "# Save the updated DataFrame back to CSV \n",
    "directors_df.to_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36cfdabf",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess 25k IMDb movie Dataset.csv to remove [] from one file\n",
    "df = pd.read_csv('/Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv')\n",
    "\n",
    "# Strip brackets\n",
    "df['top_5_casts'] = df['top_5_casts'].str.strip('[]')\n",
    "\n",
    "# Save the updated DataFrame back to CSV \n",
    "df.to_csv('/Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1408d9b1",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file /Users/toniwork/Desktop/Capstone/step_2/movies_extras.csv:\n",
      "Processed DataFrame for file /Users/toniwork/Desktop/Capstone/step_2/movies_extras.csv:\n",
      "Concatenated result DataFrame after file /Users/toniwork/Desktop/Capstone/step_2/movies_extras.csv:\n",
      "Read file /Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv:\n",
      "Processed DataFrame for file /Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv:\n",
      "Concatenated result DataFrame after file /Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv:\n",
      "actors.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create actors.csv for reference\n",
    "\n",
    "input_files = ['/Users/toniwork/Desktop/Capstone/step_2/movies_extras.csv', '/Users/toniwork/Desktop/Capstone/step_2/25k IMDb movie Dataset.csv']\n",
    "column_names = ['credits', 'top_5_casts']\n",
    "delimiters = ['-', ',']\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/reference'\n",
    "output_file = 'actors.csv'\n",
    "merge_columns = 'title_ids'\n",
    "target_column_name = 'actors'\n",
    "\n",
    "create_folder(output_folder)\n",
    "\n",
    "process_data(input_files, column_names, target_column_name, delimiters, output_folder, output_file, merge_columns, remove_quotes=True, num_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b07ab38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicated actors:\n",
      "                    actors                                          title_ids  \\\n",
      "0                      NaN                                              70573   \n",
      "9                  50 cent  78775,188000,197343,152132,8453,68259,8755,115025   \n",
      "12      a. michael baldwin                                              26407   \n",
      "14              a.a. milne                                              27918   \n",
      "15               a.j. cook                          94565,143250,40406,176171   \n",
      "...                    ...                                                ...   \n",
      "478953       émilien néron                                             193358   \n",
      "478955        éric besnard                                             208574   \n",
      "478973         éric rohmer                                              16138   \n",
      "478976       éric toledano                                             200221   \n",
      "479040       óscar jaenada  190087,92343,178431,17305,182117,211212,167399...   \n",
      "\n",
      "        actors_id  \n",
      "0               1  \n",
      "9              10  \n",
      "12             13  \n",
      "14             15  \n",
      "15             16  \n",
      "...           ...  \n",
      "478953     478954  \n",
      "478955     478956  \n",
      "478973     478974  \n",
      "478976     478977  \n",
      "479040     479041  \n",
      "\n",
      "[30423 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate director names\n",
    "actors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv')\n",
    "\n",
    "# Remove leading and trailing whitespaces from the 'actors' column\n",
    "actors_df['actors'] = actors_df['actors'].str.strip()\n",
    "\n",
    "# Check for duplicates in the 'directors' column\n",
    "duplicated_actor = actors_df[actors_df.duplicated(subset=['actors'], keep=False)]\n",
    "\n",
    "# If there are duplicates, print them\n",
    "if not duplicated_actor.empty:\n",
    "    print(\"Found duplicated actors:\")\n",
    "    print(duplicated_actor)\n",
    "else:\n",
    "    print(\"No duplicated actors found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0246ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            actors title_ids  actors_id\n",
      "0       !aru ikhuisi piet berendse     31911      21107\n",
      "1              (prince) kunle remi     12917      21108\n",
      "2                      .38 special     41127      21109\n",
      "3                        070 shake    185326      21110\n",
      "4                     0rphan drift    154519      21111\n",
      "...                            ...       ...        ...\n",
      "464095                          迟蓬    163164     479310\n",
      "464096                         陈慧娴     18548     479311\n",
      "464097                      하비 카이텔     60320     479312\n",
      "464098         ﻿ar﻿chie lanfranc﻿o    211295     479313\n",
      "464099       ﻿roshanak morrowatian    170694     479314\n",
      "\n",
      "[464100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing whitespaces from the 'actors' column\n",
    "actors_df['actors'] = actors_df['actors'].str.strip()\n",
    "\n",
    "# Group by 'actors' and aggregate 'title_ids' and 'actors_id'\n",
    "agg_funcs = {\n",
    "    'title_ids': lambda x: ','.join(x.dropna()),\n",
    "    'actors_id': 'first'\n",
    "}\n",
    "grouped_actors_df = actors_df.groupby('actors').agg(agg_funcs).reset_index()\n",
    "\n",
    "# Display the DataFrame\n",
    "grouped_actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ce826a9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Make actors title_ids into set, removing duplicates within the set\n",
    "#actors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv')\n",
    "\n",
    "# Apply the function to the 'title_ids' column\n",
    "grouped_actors_df['title_ids'] = grouped_actors_df['title_ids'].apply(remove_duplicates_in_set)\n",
    "\n",
    "# Save the updated DataFrame back to CSV \n",
    "grouped_actors_df.to_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a8ccc0c",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file /Users/toniwork/Desktop/Capstone/step_2/movies.csv:\n",
      "Processed DataFrame for file /Users/toniwork/Desktop/Capstone/step_2/movies.csv:\n",
      "Concatenated result DataFrame after file /Users/toniwork/Desktop/Capstone/step_2/movies.csv:\n",
      "Read file /Users/toniwork/Desktop/Capstone/step_2/movies_dataset.csv:\n",
      "Concatenated result DataFrame after file /Users/toniwork/Desktop/Capstone/step_2/movies_dataset.csv:\n",
      "genres.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create genres.csv for reference \n",
    "\n",
    "input_files = ['/Users/toniwork/Desktop/Capstone/step_2/movies.csv', '/Users/toniwork/Desktop/Capstone/step_2/movies_dataset.csv']\n",
    "column_names = ['genres','genre']\n",
    "delimiters = ['-',None]\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/reference'\n",
    "output_file = 'genres.csv'\n",
    "merge_columns = 'title_ids'\n",
    "target_column_name = 'genres'\n",
    "\n",
    "create_folder(output_folder)\n",
    "\n",
    "process_data(input_files, column_names, target_column_name, delimiters, output_folder, output_file, merge_columns, num_files=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0350aa",
   "metadata": {},
   "source": [
    "### Removing rows where there is only 1 title_id\n",
    "\n",
    "In this analysis, we are interested in studying how reviews of one movie impact the next for a given director or actor. To make meaningful comparisons, we need to have at least two movies associated with each director or actor.\n",
    "\n",
    "The 'directors.csv' and 'actors.csv' files contain information about directors and actors, respectively, along with the associated movie title IDs. Some directors and actors may only be associated with a single movie, and in those cases, there would be no \"next\" movie to compare with.\n",
    "\n",
    "To focus our analysis on the entities that have multiple movies, we filter out the rows where there is only one title ID for the director or actor. Specifically, we look for rows where the 'title_ids' column contains at least one comma, indicating that there are multiple title IDs.\n",
    "\n",
    "By doing this, we ensure that our dataset includes only the directors and actors with at least two movies, allowing us to proceed with the comparative analysis between consecutive movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3065701",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Make genre title_ids into set, removing duplicates within the set\n",
    "genres_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/genres.csv')\n",
    "\n",
    "# Apply the function to the 'title_ids' column\n",
    "genres_df['title_ids'] = genres_df['title_ids'].apply(remove_duplicates_in_set)\n",
    "\n",
    "# Save the updated DataFrame back to CSV \n",
    "genres_df.to_csv('/Users/toniwork/Desktop/Capstone/reference/genres.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "445a1d5d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Turn the title_ids column on directors.csv into a set, this will get rid of any duplicates within the row\n",
    "directors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv')\n",
    "\n",
    "# Keep only the rows where there is more than one title ID (i.e., at least one comma in the 'title_ids' column)\n",
    "directors_df = directors_df[directors_df['title_ids'].str.count(',') > 0]\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "directors_df.to_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4c65fd8",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Turn the title_ids column on actors.csv into a set, this will get rid of any duplicates within the row\n",
    "actors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv')\n",
    "\n",
    "# Keep only the rows where there is more than one title ID\n",
    "actors_df = actors_df[actors_df['title_ids'].str.count(',') > 0]\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "actors_df.to_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "879a41a4",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_dataset.csv updated successfully!\n",
      "metacritic-reviews.csv updated successfully!\n",
      "letterboxd-reviews.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat2.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat3.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat1.csv updated successfully!\n",
      "rotten_tomatoes_movies.csv updated successfully!\n",
      "movies_extras.csv has only 2 columns. Deleting the entire file.\n",
      "25k IMDb movie Dataset.csv updated successfully!\n",
      "movies.csv updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Drop columns used to create the directors, actors, and genres csv\n",
    "path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "columns_to_drop = ['director', 'credits', 'top_5_casts', 'genres', 'genre']\n",
    "\n",
    "# Iterate through all the CSV files in the specified path\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if the DataFrame has more than 2 columns\n",
    "        if df.shape[1] > 2:\n",
    "            # Drop the specified columns if they exist\n",
    "            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "            # Save the updated DataFrame back to the CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"{filename} updated successfully!\")\n",
    "        else:\n",
    "            print(f\"{filename} has only 2 columns. Deleting the entire file.\")\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebbb990a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6009 entries, 0 to 6008\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     6009 non-null   object \n",
      " 1   domestic_box_office       6009 non-null   float64\n",
      " 2   international_box_office  4029 non-null   float64\n",
      " 3   worldwide_box_office      4029 non-null   float64\n",
      " 4   title_id                  6009 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 234.9+ KB\n",
      "movies_dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9345 entries, 0 to 9344\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           9345 non-null   object \n",
      " 1   mpaa_rating     8560 non-null   object \n",
      " 2   user_rating     7674 non-null   float64\n",
      " 3   website_rating  9345 non-null   int64  \n",
      " 4   title_id        9345 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 365.2+ KB\n",
      "metacritic-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3183 entries, 0 to 3182\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   title          3183 non-null   object \n",
      " 1   review         3183 non-null   object \n",
      " 2   review_date    3024 non-null   object \n",
      " 3   comment_count  2957 non-null   float64\n",
      " 4   like_count     2315 non-null   float64\n",
      " 5   title_id       3183 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 149.3+ KB\n",
      "letterboxd-reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361212 entries, 0 to 361211\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         361212 non-null  object\n",
      " 1   rt_score_sentiment  361212 non-null  object\n",
      " 2   rt_is_top_critic    361212 non-null  bool  \n",
      " 3   rt_original_score   253876 non-null  object\n",
      " 4   creation_date       361212 non-null  object\n",
      " 5   title_id            361212 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 14.1+ MB\n",
      "rotten_tomatoes_movie_reviews_concat2.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356302 entries, 0 to 356301\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         356302 non-null  object\n",
      " 1   rt_score_sentiment  356302 non-null  object\n",
      " 2   rt_is_top_critic    356302 non-null  bool  \n",
      " 3   rt_original_score   249477 non-null  object\n",
      " 4   creation_date       356302 non-null  object\n",
      " 5   title_id            356302 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 13.9+ MB\n",
      "rotten_tomatoes_movie_reviews_concat3.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 358121 entries, 0 to 358120\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         358121 non-null  object\n",
      " 1   rt_score_sentiment  358121 non-null  object\n",
      " 2   rt_is_top_critic    358121 non-null  bool  \n",
      " 3   rt_original_score   250652 non-null  object\n",
      " 4   creation_date       358121 non-null  object\n",
      " 5   title_id            358121 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 14.0+ MB\n",
      "rotten_tomatoes_movie_reviews_concat1.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55282 entries, 0 to 55281\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           55282 non-null  object \n",
      " 1   audience_score  36504 non-null  float64\n",
      " 2   tomato_meter    21267 non-null  float64\n",
      " 3   box_office      66 non-null     float64\n",
      " 4   title_id        55282 non-null  int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 2.1+ MB\n",
      "rotten_tomatoes_movies.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10293 entries, 0 to 10292\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        10293 non-null  object \n",
      " 1   rating       10028 non-null  float64\n",
      " 2   user_rating  2393 non-null   float64\n",
      " 3   title_id     10293 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 321.8+ KB\n",
      "25k IMDb movie Dataset.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219371 entries, 0 to 219370\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   title         219371 non-null  object \n",
      " 1   popularity    219371 non-null  float64\n",
      " 2   budget        219371 non-null  float64\n",
      " 3   revenue       219371 non-null  float64\n",
      " 4   vote_average  219371 non-null  float64\n",
      " 5   vote_count    219371 non-null  float64\n",
      " 6   title_id      219371 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 11.7+ MB\n",
      "movies.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print file info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "091e2898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   genres     27 non-null     object\n",
      " 1   title_ids  27 non-null     object\n",
      " 2   genres_id  27 non-null     int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 776.0+ bytes\n",
      "genres.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148592 entries, 0 to 148591\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   actors     148592 non-null  object\n",
      " 1   title_ids  148592 non-null  object\n",
      " 2   actors_id  148592 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.4+ MB\n",
      "actors.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9941 entries, 0 to 9940\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   directors     9940 non-null   object\n",
      " 1   title_ids     9941 non-null   object\n",
      " 2   directors_id  9941 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 233.1+ KB\n",
      "directors.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print file info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/reference'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1472b",
   "metadata": {},
   "source": [
    "## Create an updated title_id_key.csv\n",
    "\n",
    "I can then use this title_key_updated DataFrame to merge with other datasets, ensuring that only the rows with matching title_ids are included in the analysis. By doing this, I'll be focusing on the movies that are associated with directors and actors who have at least two movies, aligning with the criteria I've established earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8368f457",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_key_updated.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create an updated title_id (title_key_updated.csv)\n",
    "# Read the directors.csv and actors.csv files\n",
    "directors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv')\n",
    "actors_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv')\n",
    "\n",
    "# Apply the function to the title_ids column\n",
    "directors_df['title_ids'] = directors_df['title_ids'].apply(convert_to_list)\n",
    "actors_df['title_ids'] = actors_df['title_ids'].apply(convert_to_list)\n",
    "\n",
    "# Explode the title_ids column\n",
    "director_title_ids = directors_df.explode('title_ids')['title_ids'].drop_duplicates()\n",
    "actor_title_ids = actors_df.explode('title_ids')['title_ids'].drop_duplicates()\n",
    "\n",
    "# Combine the title_ids and remove duplicates\n",
    "all_title_ids = pd.concat([director_title_ids, actor_title_ids]).drop_duplicates()\n",
    "\n",
    "# Create a DataFrame for the title_id_key\n",
    "title_id_key_df = pd.DataFrame({'title_id': all_title_ids})\n",
    "\n",
    "# Read the title_key.csv file\n",
    "title_key_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key.csv')\n",
    "\n",
    "# Convert the title_id column to string in both DataFrames\n",
    "title_key_df['title_id'] = title_key_df['title_id'].astype(str)\n",
    "title_id_key_df['title_id'] = title_id_key_df['title_id'].astype(str)\n",
    "\n",
    "# Merge with title_id_key_df using an inner join on the title_id column\n",
    "title_key_updated_df = pd.merge(title_key_df, title_id_key_df, on='title_id', how='inner')\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "title_key_updated_df.to_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_updated.csv', index=False)\n",
    "\n",
    "print(\"title_key_updated.csv saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4af9fb8d",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12942 entries, 0 to 12941\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   title          12942 non-null  object\n",
      " 1   release_date   12942 non-null  object\n",
      " 2   release_year   12942 non-null  int64 \n",
      " 3   release_month  12942 non-null  int64 \n",
      " 4   release_day    12942 non-null  int64 \n",
      " 5   title_id       12942 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 606.8+ KB\n",
      "title_key_final.csv : None                  title release_date  release_year  release_month  release_day  \\\n",
      "0                1 day   2009-10-21          2009             10           21   \n",
      "1       10 cent pistol   2014-07-24          2014              7           24   \n",
      "2  10 cloverfield lane   2016-03-10          2016              3           10   \n",
      "3     10 items or less   2006-09-11          2006              9           11   \n",
      "4      10 minutes gone   2019-09-27          2019              9           27   \n",
      "\n",
      "   title_id  \n",
      "0     30706  \n",
      "1    150558  \n",
      "2    133480  \n",
      "3     13136  \n",
      "4     46302          release_year  release_month   release_day       title_id\n",
      "count  12942.000000   12942.000000  12942.000000   12942.000000\n",
      "mean    2008.276541       6.409597     14.759620  107221.297790\n",
      "std       14.912954       3.463005      8.779517   62615.373206\n",
      "min     1913.000000       1.000000      1.000000       2.000000\n",
      "25%     2005.000000       3.000000      8.000000   53144.250000\n",
      "50%     2012.000000       6.000000     14.000000  106659.500000\n",
      "75%     2017.000000       9.000000     22.000000  161733.750000\n",
      "max     2023.000000      12.000000     31.000000  215799.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111189 entries, 0 to 111188\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   title          111189 non-null  object\n",
      " 1   release_date   111189 non-null  object\n",
      " 2   release_year   111189 non-null  int64 \n",
      " 3   release_month  111189 non-null  int64 \n",
      " 4   release_day    111189 non-null  int64 \n",
      " 5   title_id       111189 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 5.1+ MB\n",
      "title_key_updated.csv : None                   title release_date  release_year  release_month  \\\n",
      "0                     1   2006-01-01          2006              1   \n",
      "1              1  1  11   2015-06-21          2015              6   \n",
      "2                 1 2 3   2015-10-05          2015             10   \n",
      "3  1 2 3 all eyes on me   2020-10-24          2020             10   \n",
      "4            1 a minute   2010-10-06          2010             10   \n",
      "\n",
      "   release_day  title_id  \n",
      "0            1     57221  \n",
      "1           21    154843  \n",
      "2            5    206567  \n",
      "3           24    153167  \n",
      "4            6    128292           release_year  release_month    release_day       title_id\n",
      "count  111189.000000  111189.000000  111189.000000  111189.000000\n",
      "mean     2009.734731       6.416255      14.076860  108080.647897\n",
      "std        15.147916       3.630270       9.433962   62356.766543\n",
      "min      1909.000000       1.000000       1.000000       1.000000\n",
      "25%      2007.000000       3.000000       5.000000   54132.000000\n",
      "50%      2014.000000       6.000000      14.000000  108109.000000\n",
      "75%      2018.000000      10.000000      22.000000  162130.000000\n",
      "max      2028.000000      12.000000      31.000000  215824.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 184432 entries, 0 to 184431\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   title          184432 non-null  object\n",
      " 1   release_date   184432 non-null  object\n",
      " 2   release_year   184432 non-null  int64 \n",
      " 3   release_month  184432 non-null  int64 \n",
      " 4   release_day    184432 non-null  int64 \n",
      " 5   title_id       184432 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 8.4+ MB\n",
      "title_key.csv : None                   title release_date  release_year  release_month  \\\n",
      "0                     1   2006-01-01          2006              1   \n",
      "1              1  1  11   2015-06-21          2015              6   \n",
      "2                   1 2   2007-01-01          2007              1   \n",
      "3                 1 2 3   2015-10-05          2015             10   \n",
      "4  1 2 3 all eyes on me   2020-10-24          2020             10   \n",
      "\n",
      "   release_day  title_id  \n",
      "0            1     57221  \n",
      "1           21    154843  \n",
      "2            1    155977  \n",
      "3            5    206567  \n",
      "4           24    153167           release_year  release_month    release_day       title_id\n",
      "count  184432.000000  184432.000000  184432.000000  184432.000000\n",
      "mean     2011.180159       6.115571      13.272897  107977.925035\n",
      "std        12.733736       3.721201       9.683587   62310.670684\n",
      "min      1909.000000       1.000000       1.000000       1.000000\n",
      "25%      2008.000000       3.000000       4.000000   54034.750000\n",
      "50%      2014.000000       6.000000      13.000000  108006.500000\n",
      "75%      2018.000000      10.000000      22.000000  161914.250000\n",
      "max      2028.000000      12.000000      31.000000  215824.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12945 entries, 0 to 12944\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title_id              12945 non-null  int64  \n",
      " 1   ranking_count         12945 non-null  float64\n",
      " 2   weekend_count         12945 non-null  float64\n",
      " 3   rt_count_x            12945 non-null  float64\n",
      " 4   movies_dataset_count  12945 non-null  float64\n",
      " 5   rt_count_y            12945 non-null  float64\n",
      " 6   total_count           12945 non-null  float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 708.1 KB\n",
      "movie_data_summary_key.csv : None    title_id  ranking_count  weekend_count  rt_count_x  movies_dataset_count  \\\n",
      "0    170914            4.0            3.0         0.0                   0.0   \n",
      "1    167026            2.0            0.0         0.0                   0.0   \n",
      "2    187549            2.0            0.0         0.0                   1.0   \n",
      "3    115214            2.0            0.0         0.0                   0.0   \n",
      "4     65879            2.0            1.0         0.0                   2.0   \n",
      "\n",
      "   rt_count_y  total_count  \n",
      "0         2.0          9.0  \n",
      "1         1.0          3.0  \n",
      "2         2.0          5.0  \n",
      "3         1.0          3.0  \n",
      "4         2.0          7.0               title_id  ranking_count  weekend_count    rt_count_x  \\\n",
      "count   12945.000000   12945.000000   12945.000000  12945.000000   \n",
      "mean   107225.967864       0.203090       0.065199      0.002858   \n",
      "std     62608.952779       0.408413       0.390810      0.053388   \n",
      "min         2.000000       0.000000       0.000000      0.000000   \n",
      "25%     53163.000000       0.000000       0.000000      0.000000   \n",
      "50%    106680.000000       0.000000       0.000000      0.000000   \n",
      "75%    161733.000000       0.000000       0.000000      0.000000   \n",
      "max    215799.000000       4.000000       8.000000      1.000000   \n",
      "\n",
      "       movies_dataset_count    rt_count_y   total_count  \n",
      "count          12945.000000  12945.000000  12945.000000  \n",
      "mean               0.378525      1.186945      1.836616  \n",
      "std                0.494346      0.534328      1.153723  \n",
      "min                0.000000      0.000000      1.000000  \n",
      "25%                0.000000      1.000000      1.000000  \n",
      "50%                0.000000      1.000000      1.000000  \n",
      "75%                1.000000      1.000000      2.000000  \n",
      "max                2.000000      9.000000     11.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111108 entries, 0 to 111107\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   title_id      111108 non-null  int64  \n",
      " 1   file1_count   111108 non-null  float64\n",
      " 2   file4_count   111108 non-null  float64\n",
      " 3   file5_count   111108 non-null  float64\n",
      " 4   file6_count   111108 non-null  float64\n",
      " 5   file8_count   111108 non-null  float64\n",
      " 6   file9_count   111108 non-null  float64\n",
      " 7   file10_count  111108 non-null  float64\n",
      " 8   file11_count  111108 non-null  float64\n",
      " 9   total_count   111108 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 8.5 MB\n",
      "review_summary_key.csv : None    title_id  file1_count  file4_count  file5_count  file6_count  file8_count  \\\n",
      "0     96875        597.0          1.0          1.0          7.0          0.0   \n",
      "1    154389        564.0          2.0          0.0         25.0          0.0   \n",
      "2    182720        522.0          1.0          0.0          4.0          0.0   \n",
      "3     22838        507.0          1.0          1.0         22.0          0.0   \n",
      "4    200560        484.0          1.0          1.0          0.0          0.0   \n",
      "\n",
      "   file9_count  file10_count  file11_count  total_count  \n",
      "0          0.0           1.0           1.0        608.0  \n",
      "1          0.0           0.0           1.0        592.0  \n",
      "2          0.0           0.0           1.0        528.0  \n",
      "3          0.0           0.0           1.0        532.0  \n",
      "4          0.0           1.0           1.0        488.0               title_id    file1_count    file4_count    file5_count  \\\n",
      "count  111108.000000  111108.000000  111108.000000  111108.000000   \n",
      "mean   107813.492377       2.955215       0.445449       0.091965   \n",
      "std     62284.866806      20.501029       0.634839       0.296843   \n",
      "min         2.000000       0.000000       0.000000       0.000000   \n",
      "25%     53948.250000       0.000000       0.000000       0.000000   \n",
      "50%    107719.500000       0.000000       0.000000       0.000000   \n",
      "75%    161721.500000       0.000000       1.000000       0.000000   \n",
      "max    215823.000000     597.000000      11.000000       3.000000   \n",
      "\n",
      "         file6_count    file8_count    file9_count   file10_count  \\\n",
      "count  111108.000000  111108.000000  111108.000000  111108.000000   \n",
      "mean        0.027613       3.279674       3.281501       0.068753   \n",
      "std         0.465766      21.522760      21.727626       0.257827   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max        28.000000     561.000000     580.000000       3.000000   \n",
      "\n",
      "        file11_count    total_count  \n",
      "count  111108.000000  111108.000000  \n",
      "mean        1.246112      11.396281  \n",
      "std         0.890805      37.408944  \n",
      "min         0.000000       1.000000  \n",
      "25%         1.000000       1.000000  \n",
      "50%         1.000000       1.000000  \n",
      "75%         1.000000       3.000000  \n",
      "max        25.000000     625.000000  \n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/keys'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info(), df.head(), df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ee2ac2a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: title_key_final.csv, Title duplicates: 0\n",
      "File: title_key_final.csv, Title_id duplicates: 0\n",
      "File: title_key_updated.csv, Title duplicates: 0\n",
      "File: title_key_updated.csv, Title_id duplicates: 0\n",
      "File: title_key.csv, Title duplicates: 0\n",
      "File: title_key.csv, Title_id duplicates: 0\n",
      "File: movie_data_summary_key.csv, does not have a 'title' column.\n",
      "File: movie_data_summary_key.csv, Title_id duplicates: 0\n",
      "File: review_summary_key.csv, does not have a 'title' column.\n",
      "File: review_summary_key.csv, Title_id duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove/check for duplicates in title_key files\n",
    "folder_path = \"/Users/toniwork/Desktop/Capstone/keys\"\n",
    "file_pattern = \".csv\" # Or any specific pattern you want to match\n",
    "drop_duplicate_rows_and_report(folder_path, file_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14891dcd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres file updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read the updated key file and update the genre title_ids removing any title_ids in the sets that are not in the title_key_updated.csv\n",
    "title_id_key_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_updated.csv')\n",
    "\n",
    "# Read the genres file\n",
    "genres_file_path = '/Users/toniwork/Desktop/Capstone/reference/genres.csv'\n",
    "genres_df = pd.read_csv(genres_file_path)\n",
    "\n",
    "# Convert the title_ids in the key file to a set for faster lookup\n",
    "title_id_key_set = set(title_id_key_df['title_id'].astype(str))\n",
    "\n",
    "# Apply the function to the 'title_ids' column\n",
    "genres_df['title_ids'] = genres_df['title_ids'].apply(lambda x: filter_title_ids(x, title_id_key_set))\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "genres_df.to_csv(genres_file_path, index=False)\n",
    "print(\"Genres file updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60110a31",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_dataset.csv updated successfully!\n",
      "metacritic-reviews.csv updated successfully!\n",
      "letterboxd-reviews.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat2.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat3.csv updated successfully!\n",
      "rotten_tomatoes_movie_reviews_concat1.csv updated successfully!\n",
      "rotten_tomatoes_movies.csv updated successfully!\n",
      "25k IMDb movie Dataset.csv updated successfully!\n",
      "movies.csv updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Compare title_key_updated to the files in folder step_2 dropping any rows that do not match on title_id \n",
    "title_id_key_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_updated.csv')\n",
    "path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "# Extract the title_ids from title_id_key_df into a set for faster lookup\n",
    "title_ids_set = set(title_id_key_df['title_id'])\n",
    "\n",
    "# Specify the path where your step_2 CSV files are located\n",
    "path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "\n",
    "# Iterate through all the CSV files in the specified path\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows where title_id is not in title_ids_set\n",
    "        df_filtered = df[df['title_id'].isin(title_ids_set)]\n",
    "        \n",
    "        # Save the filtered DataFrame back to the CSV file\n",
    "        df_filtered.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"{filename} updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20e7c20",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: movies_dataset.csv, Title duplicates: 122\n",
      "File: movies_dataset.csv, Title_id duplicates: 122\n",
      "File: metacritic-reviews.csv, Title duplicates: 283\n",
      "File: metacritic-reviews.csv, Title_id duplicates: 283\n",
      "File: letterboxd-reviews.csv, Title duplicates: 2671\n",
      "File: letterboxd-reviews.csv, Title_id duplicates: 2745\n",
      "File: rotten_tomatoes_movie_reviews_concat2.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat2.csv, Title_id duplicates: 352735\n",
      "File: rotten_tomatoes_movie_reviews_concat3.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat3.csv, Title_id duplicates: 347948\n",
      "File: rotten_tomatoes_movie_reviews_concat1.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat1.csv, Title_id duplicates: 349219\n",
      "File: rotten_tomatoes_movies.csv, Title duplicates: 10640\n",
      "File: rotten_tomatoes_movies.csv, Title_id duplicates: 10649\n",
      "File: 25k IMDb movie Dataset.csv, Title duplicates: 485\n",
      "File: 25k IMDb movie Dataset.csv, Title_id duplicates: 485\n",
      "File: movies.csv, Title duplicates: 60353\n",
      "File: movies.csv, Title_id duplicates: 60363\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and get count for all updated files in step_2 folder\n",
    "folder_path = \"/Users/toniwork/Desktop/Capstone/step_2\"\n",
    "file_pattern = \".csv\" # Or any specific pattern you want to match\n",
    "drop_duplicate_rows_and_report(folder_path, file_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1b87a",
   "metadata": {},
   "source": [
    "## Data File Split \n",
    "\n",
    "### For each dataset split the files to prepare for completed datasets:\n",
    "\n",
    "- Movie Revenues: Split files, make sure each file has the title and title_id columns, add 'revenue' to the end of the file name. \n",
    "    - title_key_id\n",
    "    - title\n",
    "    - <b>movies_dataset.csv</b> \n",
    "        - domestic_box_office\n",
    "        - international_box_office\n",
    "        - worldwide_box_office\n",
    "    - <b>rotten_tomatoes_movies.csv</b> \n",
    "        - box_office\n",
    "    - <b>movies.csv</b> \n",
    "        - budget\n",
    "        - revenue\n",
    "\n",
    "- Movie Reviews: Split files, make sure each file has the title and title_id columns, add 'reviews' to the end of the file name. \n",
    "    - title_key_id\n",
    "    - title\n",
    "    - <b>metacritic-reviews.csv</b>\n",
    "        - mpaa_rating\n",
    "        - user_rating\n",
    "    - <b>rotten_tomatoes_movies.csv</b> \n",
    "        - audience_score\n",
    "        - tomato_meter\n",
    "    - <b>letterboxd-reviews.csv</b> \n",
    "        - review\n",
    "        - review_date\n",
    "        - comment_count\n",
    "        - like_count\n",
    "    - <b>rotten_tomatoes_movie_reviews_concat1-3.csv</b> \n",
    "        - rt_review_text\n",
    "        - rt_score_sentiment\n",
    "        - rt_is_top_critic\n",
    "        - rt_original_score\n",
    "        - rt_creation_date\n",
    "    - <b>25k IMDb movie Dataset.csv</b> \n",
    "        - rating\n",
    "        - user_rating\n",
    "    - <b>movies.csv</b> \n",
    "        - vote_average\n",
    "        - vote_count\n",
    "        - popularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daab3d83",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping movies_dataset.csv for reviews as it contains insufficient columns.\n",
      "Skipping movies_dataset.csv for mpaa as it contains insufficient columns.\n",
      "Skipping metacritic-reviews.csv for revenue as it contains insufficient columns.\n",
      "Skipping letterboxd-reviews.csv for revenue as it contains insufficient columns.\n",
      "Skipping letterboxd-reviews.csv for mpaa as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movie_reviews_concat2.csv for revenue as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movie_reviews_concat2.csv for mpaa as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movie_reviews_concat3.csv for revenue as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movie_reviews_concat3.csv for mpaa as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movie_reviews_concat1.csv for revenue as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movie_reviews_concat1.csv for mpaa as it contains insufficient columns.\n",
      "Skipping rotten_tomatoes_movies.csv for mpaa as it contains insufficient columns.\n",
      "Skipping 25k IMDb movie Dataset.csv for revenue as it contains insufficient columns.\n",
      "Skipping 25k IMDb movie Dataset.csv for mpaa as it contains insufficient columns.\n",
      "Skipping movies.csv for mpaa as it contains insufficient columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Successfully split the files. Check the output folder: /Users/toniwork/Desktop/Capstone/final'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove/check for duplicates \n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/step_2'\n",
    "output_folder_path = \"/Users/toniwork/Desktop/Capstone/final\"\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "create_folder(output_folder_path)\n",
    "\n",
    "split_movie_files(folder_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342a7285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 349788 entries, 0 to 349787\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         349788 non-null  object\n",
      " 1   rt_score_sentiment  349788 non-null  object\n",
      " 2   rt_is_top_critic    349788 non-null  bool  \n",
      " 3   rt_original_score   245387 non-null  object\n",
      " 4   creation_date       349788 non-null  object\n",
      " 5   title_id            349788 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 13.7+ MB\n",
      "rotten_tomatoes_movie_reviews_concat3_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49500 entries, 0 to 49499\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           49500 non-null  object \n",
      " 1   audience_score  34655 non-null  float64\n",
      " 2   tomato_meter    20191 non-null  float64\n",
      " 3   title_id        49500 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 1.5+ MB\n",
      "rotten_tomatoes_movies_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10250 entries, 0 to 10249\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        10250 non-null  object \n",
      " 1   rating       9988 non-null   float64\n",
      " 2   user_rating  2374 non-null   float64\n",
      " 3   title_id     10250 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 320.4+ KB\n",
      "25k IMDb movie Dataset_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3068 entries, 0 to 3067\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   title          3068 non-null   object \n",
      " 1   review         3068 non-null   object \n",
      " 2   review_date    2915 non-null   object \n",
      " 3   comment_count  2851 non-null   float64\n",
      " 4   like_count     2231 non-null   float64\n",
      " 5   title_id       3068 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 143.9+ KB\n",
      "letterboxd-reviews_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354555 entries, 0 to 354554\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         354555 non-null  object\n",
      " 1   rt_score_sentiment  354555 non-null  object\n",
      " 2   rt_is_top_critic    354555 non-null  bool  \n",
      " 3   rt_original_score   249813 non-null  object\n",
      " 4   creation_date       354555 non-null  object\n",
      " 5   title_id            354555 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 13.9+ MB\n",
      "rotten_tomatoes_movie_reviews_concat2_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351031 entries, 0 to 351030\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         351031 non-null  object\n",
      " 1   rt_score_sentiment  351031 non-null  object\n",
      " 2   rt_is_top_critic    351031 non-null  bool  \n",
      " 3   rt_original_score   246369 non-null  object\n",
      " 4   creation_date       351031 non-null  object\n",
      " 5   title_id            351031 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 13.7+ MB\n",
      "rotten_tomatoes_movie_reviews_concat1_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7659 entries, 0 to 7658\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        7659 non-null   object \n",
      " 1   user_rating  6526 non-null   float64\n",
      " 2   title_id     7659 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 179.6+ KB\n",
      "metacritic-reviews_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139391 entries, 0 to 139390\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   title         139391 non-null  object \n",
      " 1   popularity    139391 non-null  float64\n",
      " 2   vote_average  139391 non-null  float64\n",
      " 3   vote_count    139391 non-null  float64\n",
      " 4   title_id      139391 non-null  int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 5.3+ MB\n",
      "movies_reviews.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/final'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('reviews.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453e6a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49500 entries, 0 to 49499\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   title       49500 non-null  object \n",
      " 1   box_office  53 non-null     float64\n",
      " 2   title_id    49500 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "rotten_tomatoes_movies_revenue.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5633 entries, 0 to 5632\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     5633 non-null   object \n",
      " 1   domestic_box_office       5633 non-null   float64\n",
      " 2   international_box_office  3881 non-null   float64\n",
      " 3   worldwide_box_office      3881 non-null   float64\n",
      " 4   title_id                  5633 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 220.2+ KB\n",
      "movies_dataset_revenue.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139391 entries, 0 to 139390\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   title     139391 non-null  object \n",
      " 1   budget    139391 non-null  float64\n",
      " 2   revenue   139391 non-null  float64\n",
      " 3   title_id  139391 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 4.3+ MB\n",
      "movies_revenue.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/final'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('revenue.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02590ca0",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_specific_rows(file_path, output_path):\n",
    "    # Read in the CSV file as a DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Define a condition to filter rows where both specified columns are NA or zero\n",
    "    condition = ((df['box_office'].isna() | (df['box_office'] == 0)))\n",
    "    # Drop rows that meet the condition\n",
    "    df.drop(df[condition].index, inplace=True)\n",
    "\n",
    "    # Save the DataFrame back to the file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# File path to process\n",
    "file_path = '/Users/toniwork/Desktop/Capstone/final/rotten_tomatoes_movies_revenue.csv'\n",
    "output_path = '/Users/toniwork/Desktop/Capstone/final/rotten_tomatoes_movies_revenue.csv'\n",
    "drop_specific_rows(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94d5c84",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mpaa_rating                                          title_ids\n",
      "0            g  {172928, 65665, 43393, 186243, 182916, 38279, ...\n",
      "1            m                                           {129355}\n",
      "2        nc-17  {77582, 8847, 5008, 10128, 162069, 21912, 1427...\n",
      "3    not rated  {106496, 155651, 106503, 172047, 204817, 10651...\n",
      "4         open                                     {24355, 65543}\n",
      "5           pg  {161797, 61450, 188434, 204821, 110616, 153630...\n",
      "6        pg-13  {114689, 188419, 73738, 40977, 147478, 155681,...\n",
      "7            r  {49152, 155651, 204805, 40969, 90122, 73739, 1...\n",
      "8        tv-14  {132611, 90373, 192518, 56712, 183438, 69649, ...\n",
      "9         tv-g    {150726, 215015, 62344, 182132, 150812, 124349}\n",
      "10       tv-ma  {167426, 32772, 31238, 133638, 197643, 171533,...\n",
      "11       tv-pg  {141505, 104226, 83746, 124324, 213603, 121862...\n",
      "12       tv-y7                                           {132876}\n"
     ]
    }
   ],
   "source": [
    "# Convert the string data to a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/mpaa.csv')\n",
    "\n",
    "df['mpaa_rating'].fillna('not rated', inplace=True)\n",
    "df['mpaa_rating'].replace('unrated', 'not rated', inplace=True)\n",
    "df['mpaa_rating'].replace('nr', 'not rated', inplace=True)\n",
    "df['mpaa_rating'].replace('m/pg', 'pg', inplace=True)\n",
    "df['mpaa_rating'].replace('pg--13', 'pg-13', inplace=True)\n",
    "df['mpaa_rating'].replace('pg-13`', 'pg-13', inplace=True)\n",
    "\n",
    "# Drop the 'title' column\n",
    "df.drop('title', axis=1, inplace=True)\n",
    "\n",
    "# Group by 'mpaa_rating' and aggregate 'title_id' into sets\n",
    "result = df.groupby('mpaa_rating')['title_id'].apply(set).reset_index()\n",
    "\n",
    "# Rename the columns to match your desired output\n",
    "result.columns = ['mpaa_rating', 'title_ids']\n",
    "result.to_csv('/Users/toniwork/Desktop/Capstone/reference/mpaa.csv')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4befca4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: rotten_tomatoes_movie_reviews_concat3_reviews.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat3_reviews.csv, Title_id duplicates: 347948\n",
      "File: rotten_tomatoes_movies_revenue.csv, Title duplicates: 0\n",
      "File: rotten_tomatoes_movies_revenue.csv, Title_id duplicates: 0\n",
      "File: movies_dataset_revenue.csv, Title duplicates: 122\n",
      "File: movies_dataset_revenue.csv, Title_id duplicates: 122\n",
      "File: rotten_tomatoes_movies_reviews.csv, Title duplicates: 10639\n",
      "File: rotten_tomatoes_movies_reviews.csv, Title_id duplicates: 10648\n",
      "File: 25k IMDb movie Dataset_reviews.csv, Title duplicates: 485\n",
      "File: 25k IMDb movie Dataset_reviews.csv, Title_id duplicates: 485\n",
      "File: letterboxd-reviews_reviews.csv, Title duplicates: 2671\n",
      "File: letterboxd-reviews_reviews.csv, Title_id duplicates: 2745\n",
      "File: movies_revenue.csv, Title duplicates: 8188\n",
      "File: movies_revenue.csv, Title_id duplicates: 8199\n",
      "File: rotten_tomatoes_movie_reviews_concat2_reviews.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat2_reviews.csv, Title_id duplicates: 352735\n",
      "File: rotten_tomatoes_movie_reviews_concat1_reviews.csv, does not have a 'title' column.\n",
      "File: rotten_tomatoes_movie_reviews_concat1_reviews.csv, Title_id duplicates: 349219\n",
      "File: metacritic-reviews_reviews.csv, Title duplicates: 263\n",
      "File: metacritic-reviews_reviews.csv, Title_id duplicates: 263\n",
      "File: movies_reviews.csv, Title duplicates: 59293\n",
      "File: movies_reviews.csv, Title_id duplicates: 59303\n"
     ]
    }
   ],
   "source": [
    "# Remove/check for duplicates in new movie_data.csv \n",
    "folder_path = \"/Users/toniwork/Desktop/Capstone/final\"\n",
    "file_pattern = \".csv\" \n",
    "drop_duplicate_rows_and_report(folder_path, file_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2204946",
   "metadata": {},
   "source": [
    "## Cleaning groups of data from csv files date columns and formats\n",
    "\n",
    "#### fixing dates and addressing money and percent columns for the summary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576f0cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/1363925846.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n"
     ]
    }
   ],
   "source": [
    "# set path to folder containing CSV files\n",
    "folder_path = '/Users/toniwork/Desktop/AUBEC - 3 Projects/to_clean_summary'\n",
    "output_folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_1'\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "create_folder(output_folder_path)\n",
    "\n",
    "# loop over all files in folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # read in CSV file\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        # perform cleaning and wrangling operations on the dataframe\n",
    "        if 'ranking_summary' in filename:\n",
    "            #print('Converting columns to numeric values and removing dollar sign and comma...')\n",
    "            # convert columns to numeric values and remove dollar sign and comma\n",
    "            df[['worldwide', 'domestic', 'foreign']] = df[['worldwide', 'domestic', 'foreign']].apply(\n",
    "                lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
    "            df[['domestic_pct', 'foreign_pct']] = df[['domestic_pct', 'foreign_pct']].apply(\n",
    "                lambda x: pd.to_numeric(x.astype(str).replace('%', '', regex=True), errors='coerce')).round(4) / 100.0\n",
    "            \n",
    "            # extract year from filename and create Year column\n",
    "            year = int(filename[-8:-4])\n",
    "            df['Year'] = year\n",
    "\n",
    "        elif 'weekend_summary' in filename:\n",
    "            # update top_release column \n",
    "            df['top_release_title'] = df['top_release']\n",
    "            \n",
    "            #print('Converting columns to numeric values and removing dollar sign and comma...')\n",
    "            # convert columns to numeric values and remove dollar sign and comma\n",
    "            df[['top10_gross', 'overall_gross']] = df[['top10_gross', 'overall_gross']].apply(\n",
    "                lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce')).round(2)\n",
    "            df[['top10_wow_change', 'overall_wow_change']] = df[['top10_wow_change', 'overall_wow_change']].apply(\n",
    "                lambda x: pd.to_numeric(x.astype(str).replace('%', '', regex=True), errors='coerce')).round(4) / 100.0\n",
    "    \n",
    "            #print('Splitting date column and cleaning date values...')\n",
    "            # clean and split date column\n",
    "            df = clean_split_date(df, 'date', filename)\n",
    "            \n",
    "        # save cleaned dataframe to new file\n",
    "        new_filename = filename.split('.')[0] + '_date.csv'\n",
    "        df.to_csv(os.path.join(output_folder_path, new_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a0a3c5",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
      "/var/folders/wf/_x5mxls94p92t_39895yg9pm0000gq/T/ipykernel_85360/3873049472.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_1'\n",
    "output_folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_2'\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "create_folder(output_folder_path)\n",
    "\n",
    "ranking_data = pd.DataFrame()  # Empty DataFrame to store ranking data\n",
    "weekend_data = pd.DataFrame()  # Empty DataFrame to store weekend data\n",
    "\n",
    "# Loop over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'ranking' in filename.lower():\n",
    "            ranking_data = ranking_data.append(df)  # Append data to ranking_data DataFrame\n",
    "        elif 'weekend' in filename.lower():\n",
    "            weekend_data = weekend_data.append(df)  # Append data to weekend_data DataFrame\n",
    "\n",
    "# Save the combined data to new files\n",
    "ranking_data.to_csv(os.path.join(output_folder_path, 'combined_ranking_data.csv'), index=False)\n",
    "weekend_data.to_csv(os.path.join(output_folder_path, 'combined_weekend_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8950fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_weekend_data.csv updated successfully!\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_2'\n",
    "\n",
    "drop_columns(folder_path, 'combined_weekend_data.csv', ['occasion','top_release_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c00d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_ranking_data.csv cleaned and saved as combined_ranking_data.csv.\n",
      "combined_weekend_data.csv cleaned and saved as combined_weekend_data.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "# Run clean_movie_data function\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_2'\n",
    "output_folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_2'  \n",
    "\n",
    "clean_movie_data(folder_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d39a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4200 entries, 0 to 4199\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   rank          4200 non-null   int64  \n",
      " 1   title         4200 non-null   object \n",
      " 2   worldwide     4200 non-null   int64  \n",
      " 3   domestic      3171 non-null   float64\n",
      " 4   domestic_pct  3125 non-null   float64\n",
      " 5   foreign       4177 non-null   float64\n",
      " 6   foreign_pct   4171 non-null   float64\n",
      " 7   year          4200 non-null   int64  \n",
      "dtypes: float64(4), int64(3), object(1)\n",
      "memory usage: 262.6+ KB\n",
      "combined_ranking_data.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1291 entries, 0 to 1290\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   top10_gross         1291 non-null   float64\n",
      " 1   top10_wow_change    1094 non-null   float64\n",
      " 2   overall_gross       1291 non-null   float64\n",
      " 3   overall_wow_change  1094 non-null   float64\n",
      " 4   num_releases        1291 non-null   int64  \n",
      " 5   top_release         1291 non-null   object \n",
      " 6   week_no             1291 non-null   int64  \n",
      " 7   start_date          1291 non-null   object \n",
      " 8   start_month         1291 non-null   object \n",
      " 9   start_day           1291 non-null   int64  \n",
      " 10  start_year          1291 non-null   int64  \n",
      " 11  end_month           1291 non-null   object \n",
      " 12  end_day             1291 non-null   int64  \n",
      " 13  end_year            1291 non-null   int64  \n",
      " 14  end_date            1291 non-null   object \n",
      "dtypes: float64(4), int64(6), object(5)\n",
      "memory usage: 151.4+ KB\n",
      "combined_weekend_data.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/summary_step_2'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb2aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename rating column to mpaa_rating in metacritic-reviews.csv\n",
    "df = pd.read_csv('/Users/toniwork/Desktop/Capstone/summary_step_2/combined_weekend_data.csv')\n",
    "\n",
    "# Define a dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    'top_release': 'title'\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new file\n",
    "df.to_csv('/Users/toniwork/Desktop/Capstone/summary_step_2/combined_weekend_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c04954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first folder and its contents\n",
    "shutil.rmtree('/Users/toniwork/Desktop/Capstone/summary_step_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bfc342b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_ranking_data.csv cleaned and 'title_id' added.    rank                                    title  worldwide     domestic  \\\n",
      "0     1     the lord of the rings the two towers  936689735  339789881.0   \n",
      "1     2  harry potter and the chamber of secrets  878979634  261988482.0   \n",
      "\n",
      "   domestic_pct      foreign  foreign_pct  year title_id  \n",
      "0         0.363  596899854.0        0.637  2002   131768  \n",
      "1         0.298  616991152.0        0.702  2002   150726  \n",
      "combined_weekend_data.csv cleaned and 'title_id' added.    top10_gross  top10_wow_change  overall_gross  overall_wow_change  \\\n",
      "0  137343062.0             0.106    159345245.0               0.139   \n",
      "1  124133496.0            -0.007    139938201.0               0.039   \n",
      "\n",
      "   num_releases           title  week_no  start_date start_month  start_day  \\\n",
      "0            99  little fockers       53  2010-12-31         dec         31   \n",
      "1            97  little fockers       52  2010-12-24         dec         24   \n",
      "\n",
      "   start_year end_month  end_day  end_year    end_date title_id  \n",
      "0        2010       jan        2      2011  2011-01-02   207612  \n",
      "1        2010       dec       26      2010  2010-12-26   207612  \n"
     ]
    }
   ],
   "source": [
    "# Loop through file in folder to add the new title_id from the title_key.csv using add_title_ids_main function\n",
    "input_folder = '/Users/toniwork/Desktop/Capstone/summary_step_2'\n",
    "title_key_path = '/Users/toniwork/Desktop/Capstone/keys/'\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/summary_id'\n",
    "split_folder = '/Users/toniwork/Desktop/Capstone/split_1'\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "create_folder(output_folder)\n",
    "    \n",
    "# Create the split folder if it does not exist\n",
    "create_folder(split_folder)\n",
    "\n",
    "add_title_ids_main(input_folder, title_key_path, split_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13fe1fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3040 entries, 0 to 3039\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   rank          3040 non-null   int64  \n",
      " 1   title         3040 non-null   object \n",
      " 2   worldwide     3040 non-null   int64  \n",
      " 3   domestic      2843 non-null   float64\n",
      " 4   domestic_pct  2821 non-null   float64\n",
      " 5   foreign       3018 non-null   float64\n",
      " 6   foreign_pct   3012 non-null   float64\n",
      " 7   year          3040 non-null   int64  \n",
      " 8   title_id      3040 non-null   int64  \n",
      "dtypes: float64(4), int64(4), object(1)\n",
      "memory usage: 213.9+ KB\n",
      "combined_ranking_data.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 897 entries, 0 to 896\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   top10_gross         897 non-null    float64\n",
      " 1   top10_wow_change    764 non-null    float64\n",
      " 2   overall_gross       897 non-null    float64\n",
      " 3   overall_wow_change  765 non-null    float64\n",
      " 4   num_releases        897 non-null    int64  \n",
      " 5   title               897 non-null    object \n",
      " 6   week_no             897 non-null    int64  \n",
      " 7   start_date          897 non-null    object \n",
      " 8   start_month         897 non-null    object \n",
      " 9   start_day           897 non-null    int64  \n",
      " 10  start_year          897 non-null    int64  \n",
      " 11  end_month           897 non-null    object \n",
      " 12  end_day             897 non-null    int64  \n",
      " 13  end_year            897 non-null    int64  \n",
      " 14  end_date            897 non-null    object \n",
      " 15  title_id            897 non-null    int64  \n",
      "dtypes: float64(4), int64(7), object(5)\n",
      "memory usage: 112.2+ KB\n",
      "combined_weekend_data.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/summary_id'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adbab185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_ranking_data.csv updated successfully!\n",
      "combined_weekend_data.csv updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Compare title_key_updated to the files in folder summary_title_id dropping any rows that do not match on title_id \n",
    "title_id_key_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_updated.csv')\n",
    "path = '/Users/toniwork/Desktop/Capstone/summary_id'\n",
    "\n",
    "# Extract the title_ids from title_id_key_df into a set for faster lookup\n",
    "title_ids_set = set(title_id_key_df['title_id'])\n",
    "\n",
    "# Iterate through all the CSV files in the specified path\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows where title_id is not in title_ids_set\n",
    "        df_filtered = df[df['title_id'].isin(title_ids_set)]\n",
    "        \n",
    "        # Save the filtered DataFrame back to the CSV file\n",
    "        df_filtered.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"{filename} updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88d50a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: combined_ranking_data.csv, Title duplicates: 64\n",
      "File: combined_ranking_data.csv, Title_id duplicates: 64\n",
      "File: combined_weekend_data.csv, Title duplicates: 578\n",
      "File: combined_weekend_data.csv, Title_id duplicates: 578\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and get count for all updated files\n",
    "folder_path = \"/Users/toniwork/Desktop/Capstone/summary_id\"\n",
    "file_pattern = \".csv\" \n",
    "drop_duplicate_rows_and_report(folder_path, file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b928459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2902 entries, 0 to 2901\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   rank          2902 non-null   int64  \n",
      " 1   title         2902 non-null   object \n",
      " 2   worldwide     2902 non-null   int64  \n",
      " 3   domestic      2730 non-null   float64\n",
      " 4   domestic_pct  2721 non-null   float64\n",
      " 5   foreign       2880 non-null   float64\n",
      " 6   foreign_pct   2874 non-null   float64\n",
      " 7   year          2902 non-null   int64  \n",
      " 8   title_id      2902 non-null   int64  \n",
      "dtypes: float64(4), int64(4), object(1)\n",
      "memory usage: 204.2+ KB\n",
      "combined_ranking_data.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 895 entries, 0 to 894\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   top10_gross         895 non-null    float64\n",
      " 1   top10_wow_change    762 non-null    float64\n",
      " 2   overall_gross       895 non-null    float64\n",
      " 3   overall_wow_change  763 non-null    float64\n",
      " 4   num_releases        895 non-null    int64  \n",
      " 5   title               895 non-null    object \n",
      " 6   week_no             895 non-null    int64  \n",
      " 7   start_date          895 non-null    object \n",
      " 8   start_month         895 non-null    object \n",
      " 9   start_day           895 non-null    int64  \n",
      " 10  start_year          895 non-null    int64  \n",
      " 11  end_month           895 non-null    object \n",
      " 12  end_day             895 non-null    int64  \n",
      " 13  end_year            895 non-null    int64  \n",
      " 14  end_date            895 non-null    object \n",
      " 15  title_id            895 non-null    int64  \n",
      "dtypes: float64(4), int64(7), object(5)\n",
      "memory usage: 112.0+ KB\n",
      "combined_weekend_data.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/summary_id'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da04fe8",
   "metadata": {},
   "source": [
    "## Final Preprocessing\n",
    "\n",
    "### 1. Create Review Summary Key (review_summary_key.csv):\n",
    "- Columns: title_id, and then a column for each review file with the count of how many times a title_id appears in that file, and a final column with the sum of the other column counts.\n",
    "    - Review the average count to help determine minimum threshold of reviews needed to be included\n",
    "- Purpose: To identify title_ids that have a minimum number of reviews, ensuring that I have enough review data for meaningful analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "065ea12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average count of reviews per title_id is 11.37144861452122\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 111189 entries, 0 to 111188\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   title_id      111189 non-null  int64  \n",
      " 1   file1_count   111189 non-null  float64\n",
      " 2   file4_count   111189 non-null  float64\n",
      " 3   file5_count   111189 non-null  float64\n",
      " 4   file6_count   111189 non-null  float64\n",
      " 5   file8_count   111189 non-null  float64\n",
      " 6   file9_count   111189 non-null  float64\n",
      " 7   file10_count  111189 non-null  float64\n",
      " 8   file11_count  111189 non-null  float64\n",
      " 9   total_count   111189 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 9.3 MB\n",
      "            title_id    file1_count    file4_count    file5_count  \\\n",
      "count  111189.000000  111189.000000  111189.000000  111189.000000   \n",
      "mean   108080.647897       3.145887       0.445179       0.092185   \n",
      "std     62356.766543      20.961976       0.634726       0.297141   \n",
      "min         1.000000       0.000000       0.000000       0.000000   \n",
      "25%     54132.000000       0.000000       0.000000       0.000000   \n",
      "50%    108109.000000       0.000000       0.000000       0.000000   \n",
      "75%    162130.000000       0.000000       1.000000       0.000000   \n",
      "max    215824.000000     580.000000      11.000000       3.000000   \n",
      "\n",
      "         file6_count    file8_count    file9_count   file10_count  \\\n",
      "count  111189.000000  111189.000000  111189.000000  111189.000000   \n",
      "mean        0.027593       3.188760       3.157066       0.068793   \n",
      "std         0.465596      21.669823      21.060309       0.257890   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max        28.000000     568.000000     583.000000       3.000000   \n",
      "\n",
      "        file11_count    total_count  \n",
      "count  111189.000000  111189.000000  \n",
      "mean        1.245987      11.371449  \n",
      "std         0.890682      37.341455  \n",
      "min         0.000000       1.000000  \n",
      "25%         1.000000       1.000000  \n",
      "50%         1.000000       1.000000  \n",
      "75%         1.000000       3.000000  \n",
      "max        25.000000     625.000000   None\n"
     ]
    }
   ],
   "source": [
    "# Create Review Summary Key (review_summary_key.csv)\n",
    "\n",
    "# File paths\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/final/'\n",
    "\n",
    "# Initialize an empty list to store counts\n",
    "counts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for i, file_name in enumerate(os.listdir(folder_path), 1):\n",
    "    if file_name.endswith('reviews.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read the file and count the occurrences of title_id\n",
    "        df = pd.read_csv(file_path)\n",
    "        counts.append(df['title_id'].value_counts().reset_index().rename(columns={'index': 'title_id', 'title_id': f'file{i}_count'}))\n",
    "\n",
    "# Merge the counts into a summary DataFrame\n",
    "summary_key = counts[0]\n",
    "for count in counts[1:]:\n",
    "    summary_key = summary_key.merge(count, on='title_id', how='outer')\n",
    "\n",
    "# Fill NaN values and add a sum column\n",
    "summary_key = summary_key.fillna(0)\n",
    "summary_key['total_count'] = summary_key.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Calculate the average count\n",
    "average_count = summary_key['total_count'].mean()\n",
    "print(f\"The average count of reviews per title_id is {average_count}\")\n",
    "\n",
    "# Optional: Save to CSV\n",
    "summary_key.to_csv('review_summary_key.csv', index=False)\n",
    "print(summary_key.describe(), summary_key.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68ef00b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15943 entries, 0 to 44996\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title_id      15943 non-null  int64  \n",
      " 1   file1_count   15943 non-null  float64\n",
      " 2   file4_count   15943 non-null  float64\n",
      " 3   file5_count   15943 non-null  float64\n",
      " 4   file6_count   15943 non-null  float64\n",
      " 5   file8_count   15943 non-null  float64\n",
      " 6   file9_count   15943 non-null  float64\n",
      " 7   file10_count  15943 non-null  float64\n",
      " 8   file11_count  15943 non-null  float64\n",
      " 9   total_count   15943 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 1.3 MB\n",
      "            title_id   file1_count   file4_count   file5_count   file6_count  \\\n",
      "count   15943.000000  15943.000000  15943.000000  15943.000000  15943.000000   \n",
      "mean   107886.206799     21.217525      1.297121      0.424324      0.185034   \n",
      "std     62222.874646     51.774041      0.711288      0.520948      1.208949   \n",
      "min        41.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%     54030.000000      0.000000      1.000000      0.000000      0.000000   \n",
      "50%    107381.000000      0.000000      1.000000      0.000000      0.000000   \n",
      "75%    161822.500000     15.000000      1.000000      1.000000      0.000000   \n",
      "max    215803.000000    580.000000     11.000000      3.000000     28.000000   \n",
      "\n",
      "        file8_count   file9_count  file10_count  file11_count   total_count  \n",
      "count  15943.000000  15943.000000  15943.000000  15943.000000  15943.000000  \n",
      "mean      21.503168     21.308599      0.427523      1.477325     67.840620  \n",
      "std       53.671405     52.019891      0.511442      1.673657     77.378824  \n",
      "min        0.000000      0.000000      0.000000      0.000000     10.000000  \n",
      "25%        0.000000      0.000000      0.000000      1.000000     17.000000  \n",
      "50%        0.000000      0.000000      0.000000      1.000000     34.000000  \n",
      "75%       13.000000     15.000000      1.000000      2.000000     92.000000  \n",
      "max      568.000000    583.000000      3.000000     25.000000    625.000000   None\n"
     ]
    }
   ],
   "source": [
    "# Keep only the rows where total_count is 15 or more\n",
    "filtered_summary_key = summary_key[summary_key['total_count'] >= 10]\n",
    "\n",
    "# You can now proceed with the filtered_summary_key DataFrame, which contains only the title_ids with 15 or more reviews\n",
    "print(filtered_summary_key.describe(), filtered_summary_key.info())\n",
    "filtered_summary_key.to_csv('/Users/toniwork/Desktop/Capstone/keys/review_summary_key_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa363d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_ranking_data.csv moved to EDA folder\n",
      "combined_weekend_data.csv moved to EDA folder\n",
      "rotten_tomatoes_movie_reviews_concat3_reviews.csv moved to EDA folder\n",
      "rotten_tomatoes_movies_revenue.csv moved to EDA folder\n",
      "movies_dataset_revenue.csv moved to EDA folder\n",
      "rotten_tomatoes_movies_reviews.csv moved to EDA folder\n",
      "25k IMDb movie Dataset_reviews.csv moved to EDA folder\n",
      "letterboxd-reviews_reviews.csv moved to EDA folder\n",
      "movies_revenue.csv moved to EDA folder\n",
      "rotten_tomatoes_movie_reviews_concat2_reviews.csv moved to EDA folder\n",
      "rotten_tomatoes_movie_reviews_concat1_reviews.csv moved to EDA folder\n",
      "metacritic-reviews_reviews.csv moved to EDA folder\n",
      "movies_reviews.csv moved to EDA folder\n"
     ]
    }
   ],
   "source": [
    "folder_path_a = '/Users/toniwork/Desktop/Capstone/summary_id'\n",
    "folder_path_b = '/Users/toniwork/Desktop/Capstone/final'\n",
    "\n",
    "output_folder = '/Users/toniwork/Desktop/Capstone/EDA_Prep'\n",
    "\n",
    "create_folder(output_folder)\n",
    "\n",
    "for file_name in os.listdir(folder_path_a):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path_a, file_name)\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(folder_path_a, file_name))\n",
    "        df.to_csv(os.path.join(output_folder, file_name), index=False)\n",
    "        print(f\"{file_name} moved to EDA folder\")\n",
    "        \n",
    "for file_name in os.listdir(folder_path_b):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path_b, file_name)\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(folder_path_b, file_name))\n",
    "        df.to_csv(os.path.join(output_folder, file_name), index=False)\n",
    "        print(f\"{file_name} moved to EDA folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4290cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_revenue.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_dataset_revenue.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_reviews.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/25k IMDb movie Dataset_reviews.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/letterboxd-reviews_reviews.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_ranking_data.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_revenue.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_weekend_data.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/metacritic-reviews_reviews.csv\n",
      "Saved filtered file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_reviews.csv\n",
      "Saved concatenated file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movie_reviews_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Filter all files and add to EDA_Filtered folder\n",
    "filtered_title_ids = set(filtered_summary_key['title_id'])\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep/'\n",
    "filtered_folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/'\n",
    "\n",
    "create_folder(filtered_folder_path)\n",
    "\n",
    "# DataFrame to store concatenated rotten tomatoes reviews\n",
    "concat_reviews_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        dataset = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if 'title_id' column exists in the dataset\n",
    "        if 'title_id' in dataset.columns:\n",
    "            filtered_dataset = dataset[dataset['title_id'].isin(filtered_title_ids)]\n",
    "            \n",
    "            # If the file is one of the rotten tomatoes reviews, concatenate it\n",
    "            if filename.startswith('rotten_tomatoes_movie_reviews_concat'):\n",
    "                concat_reviews_df = pd.concat([concat_reviews_df, filtered_dataset], ignore_index=True)\n",
    "            else:\n",
    "                # Save the filtered dataset to the new folder\n",
    "                filtered_file_path = os.path.join(filtered_folder_path, filename)\n",
    "                filtered_dataset.to_csv(filtered_file_path, index=False)\n",
    "                print(f\"Saved filtered file: {filtered_file_path}\")\n",
    "\n",
    "# Save the concatenated rotten tomatoes reviews to the new folder\n",
    "concat_file_path = os.path.join(filtered_folder_path, 'rotten_tomatoes_movie_reviews_combined.csv')\n",
    "concat_reviews_df.to_csv(concat_file_path, index=False)\n",
    "print(f\"Saved concatenated file: {concat_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8207e7e",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_text rt_score_sentiment  \\\n",
      "0                       the best film of the summer.           positive   \n",
      "1                    [a] solidly old-fashioned film.           positive   \n",
      "2  like a ken burns documentary relieved of the b...           positive   \n",
      "3  a handsome respectful beautifully designed fil...           negative   \n",
      "4  a lush romantic drama that echoes with hero wo...           positive   \n",
      "\n",
      "   rt_is_top_critic rt_original_score creation_date  title_id  \n",
      "0             False               4/4    2003-07-25    160985  \n",
      "1             False               3/5    2003-07-25    160985  \n",
      "2              True               3/5    2003-07-25    160985  \n",
      "3             False               2/4    2003-07-25    160985  \n",
      "4             False             3.5/4    2003-07-25    160985  \n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movie_reviews_combined.csv'\n",
    "output_file_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movie_reviews_combined_cleaned.csv'\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    csv_reader = csv.reader(infile)\n",
    "    csv_writer = csv.writer(outfile)\n",
    "    \n",
    "    # Read and write the header\n",
    "    header = next(csv_reader)\n",
    "    csv_writer.writerow(header)\n",
    "    \n",
    "    # Read each row from the input file\n",
    "    for row in csv_reader:\n",
    "        # Assuming 'rt_review_text' is the first field, clean it\n",
    "        row[0] = row[0].replace('\"', '').replace(';', '').replace(',', '')\n",
    "        \n",
    "        # Write the cleaned row to the output file\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "# Verify by reading the first few rows of the cleaned CSV into a DataFrame\n",
    "df_verify = pd.read_csv(output_file_path)\n",
    "print(df_verify.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfcd5b8",
   "metadata": {},
   "source": [
    "### 2. Create Movie Data Summary Key (movie_data_summary_key.csv):\n",
    "- Columns: title_id, and then a column for each of the files combined_ranking_data.csv, combined_weekend_data.csv, and movie_data.csv, with the count of how many times a title_id appears in those files.\n",
    "    - Drop rows that do not have a revenue \n",
    "- Purpose: To ensure that I have consistent data across these files for each title_id.\n",
    "\n",
    "Consideration: The goal is to maintain a dataset no less than 4500 unique titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5c6be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12936 entries, 0 to 12935\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title_id              12936 non-null  int64  \n",
      " 1   ranking_count         12936 non-null  float64\n",
      " 2   weekend_count         12936 non-null  float64\n",
      " 3   rt_count_x            12936 non-null  float64\n",
      " 4   movies_dataset_count  12936 non-null  float64\n",
      " 5   rt_count_y            12936 non-null  float64\n",
      " 6   total_count           12936 non-null  float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 808.5 KB\n",
      "            title_id  ranking_count  weekend_count    rt_count_x  \\\n",
      "count   12936.000000   12936.000000   12936.000000  12936.000000   \n",
      "mean   107980.272805       0.203231       0.065244      0.002860   \n",
      "std     62106.624041       0.408520       0.390942      0.053407   \n",
      "min        53.000000       0.000000       0.000000      0.000000   \n",
      "25%     54345.500000       0.000000       0.000000      0.000000   \n",
      "50%    107251.000000       0.000000       0.000000      0.000000   \n",
      "75%    161692.500000       0.000000       0.000000      0.000000   \n",
      "max    215803.000000       4.000000       8.000000      1.000000   \n",
      "\n",
      "       movies_dataset_count    rt_count_y   total_count  \n",
      "count          12936.000000  12936.000000  12936.000000  \n",
      "mean               0.378711      1.186998      1.837044  \n",
      "std                0.494398      0.534446      1.153958  \n",
      "min                0.000000      0.000000      1.000000  \n",
      "25%                0.000000      1.000000      1.000000  \n",
      "50%                0.000000      1.000000      1.000000  \n",
      "75%                1.000000      1.000000      2.000000  \n",
      "max                2.000000      9.000000     11.000000   None\n"
     ]
    }
   ],
   "source": [
    "# Create Movie Data Summary Key (movie_data_summary_key.csv)\n",
    "\n",
    "# Load the data\n",
    "combined_ranking_data = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_ranking_data.csv')\n",
    "combined_weekend_data = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_weekend_data.csv')\n",
    "rt_revenue = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_revenue.csv')\n",
    "movies_dataset_revenue = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_dataset_revenue.csv')\n",
    "movies_revenue = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_revenue.csv')\n",
    "\n",
    "# Filter rows with revenue\n",
    "combined_ranking_data = combined_ranking_data[combined_ranking_data[['worldwide', 'domestic']].notna().any(axis=1)]\n",
    "combined_weekend_data = combined_weekend_data[combined_weekend_data[['top10_gross', 'overall_gross']].notna().any(axis=1)]\n",
    "rt_revenue = rt_revenue[rt_revenue[['box_office']].notna().any(axis=1)]\n",
    "movies_dataset_revenue = movies_dataset_revenue[movies_dataset_revenue[['domestic_box_office', 'worldwide_box_office']].notna().any(axis=1)]\n",
    "movies_revenue = movies_revenue[movies_revenue[['budget', 'revenue']].notna().any(axis=1)]\n",
    "\n",
    "# Count title_id occurrences\n",
    "count_ranking = combined_ranking_data['title_id'].value_counts()\n",
    "count_weekend = combined_weekend_data['title_id'].value_counts()\n",
    "count_rt = rt_revenue['title_id'].value_counts()\n",
    "count_movies_dataset = movies_dataset_revenue['title_id'].value_counts()\n",
    "count_movies = movies_revenue['title_id'].value_counts()\n",
    "\n",
    "# Merge the counts\n",
    "summary_key_md = pd.DataFrame({'title_id': count_ranking.index, 'ranking_count': count_ranking.values})\n",
    "summary_key_md = summary_key_md.merge(pd.DataFrame({'title_id': count_weekend.index, 'weekend_count': count_weekend.values}), on='title_id', how='outer')\n",
    "summary_key_md = summary_key_md.merge(pd.DataFrame({'title_id': count_rt.index, 'rt_count': count_rt.values}), on='title_id', how='outer')\n",
    "summary_key_md = summary_key_md.merge(pd.DataFrame({'title_id': count_movies_dataset.index, 'movies_dataset_count': count_movies_dataset.values}), on='title_id', how='outer')\n",
    "summary_key_md = summary_key_md.merge(pd.DataFrame({'title_id': count_movies.index, 'rt_count': count_movies.values}), on='title_id', how='outer')\n",
    "\n",
    "\n",
    "summary_key_md = summary_key_md.fillna(0)\n",
    "summary_key_md['total_count'] = summary_key_md.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Optional: Apply additional filtering\n",
    "# summary_key = summary_key[summary_key['total_count'] >= threshold]\n",
    "\n",
    "# Save to CSV\n",
    "summary_key_md.to_csv('/Users/toniwork/Desktop/Capstone/keys/revenue_summary_key.csv', index=False)\n",
    "print(summary_key_md.describe(), summary_key_md.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbb684a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movie_reviews_combined.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_revenue.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_dataset_revenue.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movie_reviews_combined_cleaned.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_reviews.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/25k IMDb movie Dataset_reviews.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/letterboxd-reviews_reviews.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_ranking_data.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_revenue.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_weekend_data.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/metacritic-reviews_reviews.csv\n",
      "Filtered and saved file: /Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the movie_data_summary_key.csv\n",
    "summary_key_path = '/Users/toniwork/Desktop/Capstone/keys/revenue_summary_key.csv'\n",
    "summary_key = pd.read_csv(summary_key_path)\n",
    "filtered_title_ids = set(summary_key['title_id'])\n",
    "\n",
    "# Path to the filtered folder\n",
    "filtered_folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/'\n",
    "\n",
    "# Iterate through all CSV files in the folder\n",
    "for filename in os.listdir(filtered_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(filtered_folder_path, filename)\n",
    "        dataset = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if 'title_id' column exists in the dataset\n",
    "        if 'title_id' in dataset.columns:\n",
    "            filtered_dataset = dataset[dataset['title_id'].isin(filtered_title_ids)]\n",
    "\n",
    "            # Save the filtered dataset back to the same file\n",
    "            filtered_dataset.to_csv(file_path, index=False)\n",
    "            print(f\"Filtered and saved file: {file_path}\")\n",
    "        else:\n",
    "            print(f\"File: {filename} does not contain 'title_id' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4324eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 902457 entries, 0 to 902456\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         902457 non-null  object\n",
      " 1   rt_score_sentiment  902457 non-null  object\n",
      " 2   rt_is_top_critic    902457 non-null  bool  \n",
      " 3   rt_original_score   647558 non-null  object\n",
      " 4   creation_date       902457 non-null  object\n",
      " 5   title_id            902457 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 35.3+ MB\n",
      "rotten_tomatoes_movie_reviews_combined.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37 entries, 0 to 36\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   title       37 non-null     object \n",
      " 1   box_office  37 non-null     float64\n",
      " 2   title_id    37 non-null     int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1016.0+ bytes\n",
      "rotten_tomatoes_movies_revenue.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4899 entries, 0 to 4898\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   title                     4899 non-null   object \n",
      " 1   domestic_box_office       4899 non-null   float64\n",
      " 2   international_box_office  3624 non-null   float64\n",
      " 3   worldwide_box_office      3624 non-null   float64\n",
      " 4   title_id                  4899 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 191.5+ KB\n",
      "movies_dataset_revenue.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 902457 entries, 0 to 902456\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   review_text         902457 non-null  object\n",
      " 1   rt_score_sentiment  902457 non-null  object\n",
      " 2   rt_is_top_critic    902457 non-null  bool  \n",
      " 3   rt_original_score   647558 non-null  object\n",
      " 4   creation_date       902457 non-null  object\n",
      " 5   title_id            902457 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 35.3+ MB\n",
      "rotten_tomatoes_movie_reviews_combined_cleaned.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17345 entries, 0 to 17344\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           17345 non-null  object \n",
      " 1   audience_score  15069 non-null  float64\n",
      " 2   tomato_meter    13779 non-null  float64\n",
      " 3   title_id        17345 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 542.2+ KB\n",
      "rotten_tomatoes_movies_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6710 entries, 0 to 6709\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        6710 non-null   object \n",
      " 1   rating       6567 non-null   float64\n",
      " 2   user_rating  737 non-null    float64\n",
      " 3   title_id     6710 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 209.8+ KB\n",
      "25k IMDb movie Dataset_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2520 entries, 0 to 2519\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   title          2520 non-null   object \n",
      " 1   review         2520 non-null   object \n",
      " 2   review_date    2398 non-null   object \n",
      " 3   comment_count  2340 non-null   float64\n",
      " 4   like_count     1846 non-null   float64\n",
      " 5   title_id       2520 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 118.2+ KB\n",
      "letterboxd-reviews_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2629 entries, 0 to 2628\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   rank          2629 non-null   int64  \n",
      " 1   title         2629 non-null   object \n",
      " 2   worldwide     2629 non-null   int64  \n",
      " 3   domestic      2535 non-null   float64\n",
      " 4   domestic_pct  2530 non-null   float64\n",
      " 5   foreign       2613 non-null   float64\n",
      " 6   foreign_pct   2607 non-null   float64\n",
      " 7   year          2629 non-null   int64  \n",
      " 8   title_id      2629 non-null   int64  \n",
      "dtypes: float64(4), int64(4), object(1)\n",
      "memory usage: 185.0+ KB\n",
      "combined_ranking_data.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15355 entries, 0 to 15354\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   title     15355 non-null  object \n",
      " 1   budget    15355 non-null  float64\n",
      " 2   revenue   15355 non-null  float64\n",
      " 3   title_id  15355 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 480.0+ KB\n",
      "movies_revenue.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 844 entries, 0 to 843\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   top10_gross         844 non-null    float64\n",
      " 1   top10_wow_change    719 non-null    float64\n",
      " 2   overall_gross       844 non-null    float64\n",
      " 3   overall_wow_change  720 non-null    float64\n",
      " 4   num_releases        844 non-null    int64  \n",
      " 5   title               844 non-null    object \n",
      " 6   week_no             844 non-null    int64  \n",
      " 7   start_date          844 non-null    object \n",
      " 8   start_month         844 non-null    object \n",
      " 9   start_day           844 non-null    int64  \n",
      " 10  start_year          844 non-null    int64  \n",
      " 11  end_month           844 non-null    object \n",
      " 12  end_day             844 non-null    int64  \n",
      " 13  end_year            844 non-null    int64  \n",
      " 14  end_date            844 non-null    object \n",
      " 15  title_id            844 non-null    int64  \n",
      "dtypes: float64(4), int64(7), object(5)\n",
      "memory usage: 105.6+ KB\n",
      "combined_weekend_data.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6612 entries, 0 to 6611\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        6612 non-null   object \n",
      " 1   user_rating  5742 non-null   float64\n",
      " 2   title_id     6612 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 155.1+ KB\n",
      "metacritic-reviews_reviews.csv : None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23553 entries, 0 to 23552\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         23553 non-null  object \n",
      " 1   popularity    23553 non-null  float64\n",
      " 2   vote_average  23553 non-null  float64\n",
      " 3   vote_count    23553 non-null  float64\n",
      " 4   title_id      23553 non-null  int64  \n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 920.2+ KB\n",
      "movies_reviews.csv : None\n"
     ]
    }
   ],
   "source": [
    "# Print files info from folder to verify\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read in the CSV file as a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_name,':',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "995dffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_specific_rows(file_path, output_path):\n",
    "    # Read in the CSV file as a DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Define a condition to filter rows where both specified columns are NA or zero\n",
    "    condition = ((df['rating'].isna() | (df['rating'] == 0)))\n",
    "    # Drop rows that meet the condition\n",
    "    df.drop(df[condition].index, inplace=True)\n",
    "\n",
    "    # Save the DataFrame back to the file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# File path to process\n",
    "file_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/25k IMDb movie Dataset_reviews.csv'\n",
    "output_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/25k IMDb movie Dataset_reviews.csv'\n",
    "drop_specific_rows(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e21b6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_specific_rows(file_path, output_path):\n",
    "    # Read in the CSV file as a DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Define a condition to filter rows where both specified columns are NA or zero\n",
    "    condition = ((df['domestic'].isna() | (df['domestic'] == 0)))\n",
    "    # Drop rows that meet the condition\n",
    "    df.drop(df[condition].index, inplace=True)\n",
    "\n",
    "    # Save the DataFrame back to the file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# File path to process\n",
    "file_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_ranking_data.csv'\n",
    "output_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/combined_ranking_data.csv'\n",
    "drop_specific_rows(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19a2bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_specific_rows(file_path, output_path):\n",
    "    # Read in the CSV file as a DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Define a condition to filter rows where both specified columns are NA or zero\n",
    "    condition = ((df['user_rating'].isna() | (df['user_rating'] == 0)))\n",
    "    # Drop rows that meet the condition\n",
    "    df.drop(df[condition].index, inplace=True)\n",
    "\n",
    "    # Save the DataFrame back to the file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# File path to process\n",
    "file_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/metacritic-reviews_reviews.csv'\n",
    "output_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/metacritic-reviews_reviews.csv'\n",
    "drop_specific_rows(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b520a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title key file has been updated.\n"
     ]
    }
   ],
   "source": [
    "# Load the title key file\n",
    "title_key_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_updated.csv')\n",
    "\n",
    "# Initialize sets to keep track of title_ids found in the specified types of files\n",
    "title_ids_in_revenue_data = set()\n",
    "title_ids_in_reviews_combined = set()\n",
    "\n",
    "# Directory path\n",
    "folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/'\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(('revenue.csv', 'data.csv')) or file_name.endswith(('reviews.csv', 'combined.csv')):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if 'title_id' column exists in the DataFrame\n",
    "        if 'title_id' in df.columns:\n",
    "            title_ids = set(df['title_id'])\n",
    "            \n",
    "            # Update the sets based on the type of file\n",
    "            if file_name.endswith(('revenue.csv', 'data.csv')):\n",
    "                title_ids_in_revenue_data.update(title_ids)\n",
    "            else:\n",
    "                title_ids_in_reviews_combined.update(title_ids)\n",
    "\n",
    "# Filter the title_key_df DataFrame based on the conditions\n",
    "filtered_title_key_df = title_key_df[title_key_df['title_id'].isin(title_ids_in_revenue_data) & title_key_df['title_id'].isin(title_ids_in_reviews_combined)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_title_key_df.to_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_final.csv', index=False)\n",
    "\n",
    "print(\"Title key file has been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e82e4b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12933 entries, 0 to 12932\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   title          12933 non-null  object\n",
      " 1   release_date   12933 non-null  object\n",
      " 2   release_year   12933 non-null  int64 \n",
      " 3   release_month  12933 non-null  int64 \n",
      " 4   release_day    12933 non-null  int64 \n",
      " 5   title_id       12933 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 606.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_final.csv')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bb4ae",
   "metadata": {},
   "source": [
    "### Filter directors, actors, and genres reference datasets \n",
    "\n",
    "Filtering the reference datasets in the title_ids column sets to remove any title_ids not in title_key_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a78376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the directors.csv\n",
    "\n",
    "# Load the review summary key file\n",
    "title_id_key_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/keys/title_key_final.csv')\n",
    "\n",
    "# Load the directors file\n",
    "directors = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/directors.csv')\n",
    "\n",
    "# Create a set of valid title IDs for faster lookup\n",
    "valid_title_ids = set(title_id_key_df['title_id'].astype(int))\n",
    "\n",
    "# Apply the filtering function\n",
    "directors['title_ids'] = directors['title_ids'].apply(lambda x: filter_title_ids_2(x, valid_title_ids))\n",
    "\n",
    "# Drop rows where the title_ids set is empty\n",
    "directors = directors[directors['title_ids'] != 'set()']\n",
    "\n",
    "# Save the filtered data\n",
    "directors.to_csv('/Users/toniwork/Desktop/Capstone/reference/directors_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0fa3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the actors.csv\n",
    "\n",
    "# Load the actors file\n",
    "actors = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/actors.csv')\n",
    "\n",
    "# Apply the filtering function\n",
    "actors['title_ids'] = actors['title_ids'].apply(lambda x: filter_title_ids_2(x, valid_title_ids))\n",
    "\n",
    "# Drop rows where the title_ids set is empty\n",
    "actors = actors[actors['title_ids'] != 'set()']\n",
    "\n",
    "# Save the filtered data\n",
    "actors.to_csv('/Users/toniwork/Desktop/Capstone/reference/actors_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b10174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter genres.csv\n",
    "\n",
    "# Load the genres file\n",
    "genres = pd.read_csv('/Users/toniwork/Desktop/Capstone/reference/genres.csv')\n",
    "\n",
    "# Apply the filtering function\n",
    "genres['title_ids'] = genres['title_ids'].apply(lambda x: filter_title_ids_2(x, valid_title_ids))\n",
    "\n",
    "# Drop rows where the title_ids set is empty\n",
    "genres = genres[genres['title_ids'] != 'set()']\n",
    "\n",
    "# Save the filtered data\n",
    "genres.to_csv('/Users/toniwork/Desktop/Capstone/reference/genres_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79958c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20291 entries, 0 to 15354\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title                 20291 non-null  object \n",
      " 1   title_id              20291 non-null  int64  \n",
      " 2   standardized_revenue  20291 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 634.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files\n",
    "df1 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_revenue.csv')\n",
    "df2 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_dataset_revenue.csv')\n",
    "df3 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_revenue.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "df2 = df2.drop(columns=['international_box_office', 'worldwide_box_office'])\n",
    "df3 = df3.drop(columns=['budget'])\n",
    "\n",
    "# Rename the columns to a standardized name\n",
    "df1.rename(columns={'box_office': 'standardized_revenue'}, inplace=True)\n",
    "df2.rename(columns={'domestic_box_office': 'standardized_revenue'}, inplace=True)\n",
    "df3.rename(columns={'revenue': 'standardized_revenue'}, inplace=True)\n",
    "\n",
    "# Drop the original columns to avoid duplication\n",
    "df1 = df1[['title', 'title_id', 'standardized_revenue']]\n",
    "df2 = df2[['title', 'title_id', 'standardized_revenue']]\n",
    "df3 = df3[['title', 'title_id', 'standardized_revenue']]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "final_df = pd.concat([df1, df2, df3])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/revenue_simplified.csv', index=False)\n",
    "\n",
    "# Print DataFrame info\n",
    "print(final_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c748c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28278 entries, 0 to 28277\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title                 28278 non-null  object \n",
      " 1   title_id              28278 non-null  int64  \n",
      " 2   standardized_revenue  28278 non-null  float64\n",
      " 3   budget                28155 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the existing revenue_simplified.csv into a DataFrame\n",
    "final_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/revenue_simplified.csv')\n",
    "\n",
    "# Read the movies_revenue.csv into another DataFrame\n",
    "df3 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_revenue.csv')\n",
    "\n",
    "# Merge the 'budget' column from df3 onto final_df based on 'title_id'\n",
    "final_df = pd.merge(final_df, df3[['title_id', 'budget']], on='title_id', how='left')\n",
    "\n",
    "# Save the updated DataFrame back to revenue_simplified.csv\n",
    "final_df.to_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/revenue_simplified.csv', index=False)\n",
    "\n",
    "# Print DataFrame info to confirm the merge\n",
    "print(final_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f291331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: revenue_simplified.csv, Title duplicates: 16152\n",
      "File: revenue_simplified.csv, Title_id duplicates: 16158\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered'\n",
    "drop_duplicate_rows_and_report(folder_path, 'simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843faf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12914 entries, 6181 to 23582\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   title                 12914 non-null  object \n",
      " 1   title_id              12914 non-null  int64  \n",
      " 2   standardized_revenue  12914 non-null  float64\n",
      " 3   budget                12793 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 504.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the existing revenue_simplified.csv into a DataFrame\n",
    "final_df = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/revenue_simplified.csv')\n",
    "\n",
    "# Remove duplicates based on 'title_id', keeping the first occurrence\n",
    "final_df = final_df.drop_duplicates(subset='title_id', keep='first')\n",
    "\n",
    "# Sort the DataFrame by 'standardized_revenue' in descending order\n",
    "final_df = final_df.sort_values(by='standardized_revenue', ascending=False)\n",
    "\n",
    "# Save the updated DataFrame back to revenue_simplified.csv\n",
    "final_df.to_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/revenue_simplified.csv', index=False)\n",
    "\n",
    "# Print DataFrame info to confirm the changes\n",
    "print(final_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f67e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29654 entries, 0 to 29653\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        29654 non-null  object \n",
      " 1   title_id     29654 non-null  int64  \n",
      " 2   user_rating  21405 non-null  float64\n",
      " 3   rating       20346 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 926.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read datasets\n",
    "df1 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/rotten_tomatoes_movies_reviews.csv')[['title', 'title_id', 'audience_score', 'tomato_meter']]\n",
    "df2 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/25k IMDb movie Dataset_reviews.csv')[['title', 'title_id', 'rating', 'user_rating']]\n",
    "df3 = pd.read_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/metacritic-reviews_reviews.csv')[['title', 'title_id', 'user_rating']]\n",
    "\n",
    "# Rename columns for consistency\n",
    "df1.rename(columns={'audience_score': 'user_rating'}, inplace=True)\n",
    "df1.rename(columns={'tomato_meter': 'rating'}, inplace=True)\n",
    "\n",
    "# Scale IMDb ratings to match Rotten Tomatoes scale\n",
    "df2['rating'] = df2['rating'] * 10  # Scaling from 10-point to 100-point scale\n",
    "df2['user_rating'] = df2['user_rating'] / 10  # Scaling from 1000-point to 100-point scale\n",
    "\n",
    "# Concatenate\n",
    "ratings_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "ratings_df.to_csv('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/ratings.csv', index=False)\n",
    "\n",
    "# Print DataFrame info to confirm the changes\n",
    "print(ratings_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04377961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ratings.csv, Title duplicates: 25842\n",
      "File: ratings.csv, Title_id duplicates: 25853\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered'\n",
    "drop_duplicate_rows_and_report(folder_path, 'ratings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948b1c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/popularity.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/letterboxd-reviews_reviews.csv', '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/lr_reviews.csv')\n",
    "shutil.move('/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/movies_reviews.csv', '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/popularity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a6f4501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved : ratings.csv\n",
      "Moved : lr_reviews.csv\n",
      "Moved : popularity.csv\n",
      "Moved : rotten_tomatoes_movie_reviews_combined.csv\n",
      "Moved : combined_ranking_data.csv\n",
      "Moved : combined_weekend_data.csv\n",
      "Moved : revenue_simplified.csv\n"
     ]
    }
   ],
   "source": [
    "# Source folder\n",
    "src_folder = '/Users/toniwork/Desktop/Capstone/EDA_Prep_Filtered/'\n",
    "\n",
    "# Destination folder\n",
    "dst_folder = '/Users/toniwork/Desktop/Capstone/EDA/'\n",
    "\n",
    "# Create destination folder if it doesn't exist\n",
    "if not os.path.exists(dst_folder):\n",
    "    os.makedirs(dst_folder)\n",
    "\n",
    "# List of files to move\n",
    "files_to_move = ['ratings.csv', 'lr_reviews.csv', 'popularity.csv', 'rotten_tomatoes_movie_reviews_combined.csv', \n",
    "                 'combined_ranking_data.csv', 'combined_weekend_data.csv', 'revenue_simplified.csv']\n",
    "\n",
    "# Loop to move each file\n",
    "for file_name in files_to_move:\n",
    "    # construct full file path\n",
    "    source = src_folder + file_name\n",
    "    destination = dst_folder + file_name\n",
    "    \n",
    "    try:\n",
    "        shutil.move(source, destination)\n",
    "        print(f\"Moved : {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_name} not found in {src_folder}\")\n",
    "    except shutil.Error:\n",
    "        print(f\"Error occurred while moving {file_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f1989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da66b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (6.4.4)\n",
      "Requirement already satisfied: traitlets>=5.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (5.1.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (5.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (0.5.13)\n",
      "Requirement already satisfied: testpath in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (0.4)\n",
      "Requirement already satisfied: jupyter-core in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: jinja2>=2.4 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (2.11.3)\n",
      "Requirement already satisfied: bleach in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (2.11.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from jinja2>=2.4->nbconvert) (2.0.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.5.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (7.3.4)\n",
      "Requirement already satisfied: fastjsonschema in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbformat>=4.4->nbconvert) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from nbformat>=4.4->nbconvert) (4.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: packaging in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert) (21.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (21.4.0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (23.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: tornado>=6.0 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/toniwork/opt/anaconda3/lib/python3.9/site-packages (from packaging->bleach->nbconvert) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "# Load the notebook you want to run\n",
    "with open(\"/Users/toniwork/Desktop/Capstone/Capstone - EDA.ipynb\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Set up the notebook execution configuration\n",
    "ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "\n",
    "# Execute the notebook\n",
    "ep.preprocess(notebook, {'metadata': {'path': '/Users/toniwork/Desktop/Capstone/'}})\n",
    "\n",
    "# Save the executed notebook to a new file\n",
    "with open(\"/Users/toniwork/Desktop/Capstone/Capstone - EDA.ipynb\", 'wt') as f:\n",
    "    nbformat.write(notebook, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "196px",
    "width": "161px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "371.906px",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
