{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3b5c93",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-Functions\" data-toc-modified-id=\"Cleaning-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning Functions</a></span></li><li><span><a href=\"#General-Functions\" data-toc-modified-id=\"General-Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>General Functions</a></span></li><li><span><a href=\"#Process-and-Merge-Functions\" data-toc-modified-id=\"Process-and-Merge-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Process and Merge Functions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adb5e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from common_imports.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from common_imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45879c",
   "metadata": {},
   "source": [
    "## Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a06f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s, patterns_replacements):\n",
    "    for pattern, replacement in patterns_replacements:\n",
    "        s = re.sub(pattern, replacement, s)\n",
    "    return s\n",
    "\n",
    "def clean_money_columns(df, columns):\n",
    "    # Define the patterns and replacements for cleaning money strings\n",
    "    money_patterns_replacements = [('[$.]', ''), ('K', '000'), ('M', '000000')]\n",
    "\n",
    "    # Clean the specified columns using the clean_string function\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).apply(lambda x: clean_string(x, money_patterns_replacements))\n",
    "            # Convert to numeric, handling non-numeric values\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else:\n",
    "            print(f\"Column {col} not found in the DataFrame. Skipping.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8224898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_language(df):\n",
    "    # Check for language columns and filter rows accordingly\n",
    "    language_cols = [col for col in df.columns if 'language' in col.lower() or 'languages' in col.lower()]\n",
    "    for col in language_cols:\n",
    "        if df[col].str.len().max() == 2:\n",
    "            df = df[df[col] == 'en']\n",
    "        else:\n",
    "            df = df[df[col] == 'english']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebe2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_names(df):\n",
    "    df.columns = df.columns.str.lower().str.replace(r'[\\s]', '_', regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa9ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_with_removed_zeros = set()\n",
    "\n",
    "\n",
    "def save_titles_with_removed_zeros(output_folder_path):\n",
    "    with open(os.path.join(output_folder_path, 'titles_with_removed_zeros.txt'), 'w') as f:\n",
    "        for title in titles_with_removed_zeros:\n",
    "            f.write(f\"{title}\\n\")\n",
    "    print(\"Titles with removed zeros saved as 'titles_with_removed_zeros.txt'.\")\n",
    "\n",
    "\n",
    "\n",
    "def clean_title_columns(df):\n",
    "    title_cols = [col for col in df.columns if 'title' in col.lower()]\n",
    "    for col in title_cols:\n",
    "        if df[col].dtype == object:\n",
    "            original_titles = df[col].copy()\n",
    "            df[col] = df[col].str.lower().str.replace('[^\\w\\s]', '').apply(lambda x: x.lstrip('0') if isinstance(x, str) else x)\n",
    "            \n",
    "            # Update the set with titles that had leading zeros removed\n",
    "            titles_with_removed_zeros.update(df.loc[original_titles != df[col], col].dropna().unique())\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ff592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_release_dates(df, date_columns=None):\n",
    "    if date_columns is None:\n",
    "        date_columns = ['release_date']\n",
    "    \n",
    "    for date_column in date_columns:\n",
    "        if date_column in df.columns:\n",
    "            try:\n",
    "                df[date_column] = pd.to_datetime(df[date_column], errors='coerce', format='%Y-%m-%d')\n",
    "                df['release_year'] = df[date_column].dt.year.astype(int)   # Converted to integer\n",
    "                df['release_month'] = df[date_column].dt.month.astype(int) # Converted to integer\n",
    "                df['release_day'] = df[date_column].dt.day.astype(int)     # Converted to integer\n",
    "            except Exception as e:\n",
    "                print(f\"Error: '{date_column}' column not in 'YYYY-MM-DD' format. {str(e)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f716d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, columns_to_check):\n",
    "    if set(columns_to_check).issubset(df.columns):\n",
    "        duplicates = df[df.duplicated(columns_to_check, keep=False)]\n",
    "        if not duplicates.empty:\n",
    "            null_counts = duplicates.isnull().sum(axis=1)\n",
    "            min_null_count = null_counts.min()\n",
    "            df = df.drop(duplicates[null_counts > min_null_count].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0268b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_date_ranges(df, date_columns=None, year_column=None, start_date=None, end_date=None, start_year=None, end_year=None):\n",
    "    if date_columns is not None:\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                if start_date and end_date:\n",
    "                    df = df[((df[col].isnull()) | ((df[col] >= start_date) & (df[col] <= end_date)))]\n",
    "    if year_column is not None and start_year is not None and end_year is not None:\n",
    "        if year_column in df.columns:\n",
    "            df = df[((df[year_column].isnull()) | ((df[year_column] >= start_year) & (df[year_column] <= end_year)))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76eb811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_columns_to_lowercase(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.lower()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3037001",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c31f43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37f4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "                print(f\"{filename} has been deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e00d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_by_keyword_or_name(folder_path, keyword=None, column_names=None, file_name=None):\n",
    "    if file_name:\n",
    "        file_paths = [os.path.join(folder_path, file_name)]\n",
    "    else:\n",
    "        file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"{file_path} not found, skipping\")\n",
    "            continue\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Drop columns containing the keyword in their names\n",
    "        if keyword:\n",
    "            cols_to_drop = [col for col in df.columns if keyword in col]\n",
    "            df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        # Drop the columns with the exact specified names\n",
    "        if column_names:\n",
    "            cols_to_drop = [col for col in column_names if col in df.columns]\n",
    "            df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"{os.path.basename(file_path)} updated successfully!\")\n",
    "        \n",
    "def drop_columns(folder_path, file_name, columns):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"{file_path} not found, skipping\")\n",
    "        return\n",
    "    df = pd.read_csv(file_path)\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        else:\n",
    "            print(f\"Column {col} not found in {file_name}, skipping\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"{file_name} updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3dc95d1",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def drop_rows(file_path, file_patterns_to_columns):\n",
    "    # Read in the CSV file as a DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Apply specific conditions based on the file name\n",
    "    for pattern, columns in file_patterns_to_columns.items():\n",
    "        if pattern in file_path:\n",
    "            if isinstance(columns, list):\n",
    "                df.dropna(subset=columns, inplace=True)\n",
    "            elif isinstance(columns, dict):\n",
    "                df.dropna(subset=columns['subset'], how=columns.get('how', 'any'), inplace=True)\n",
    "\n",
    "    # Save the DataFrame back to the file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b7f7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_extra(folder_path=None, file_path=None, column_name=None, delimiter=','):\n",
    "    if file_path:\n",
    "        process_file(file_path, column_name, delimiter)\n",
    "    elif folder_path:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                process_file(file_path, column_name, delimiter)\n",
    "\n",
    "def process_file(file_path, column_name, delimiter):\n",
    "    # Read in the CSV file as a DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    if column_name and column_name in df.columns:\n",
    "        # Apply the unique processing to the specified column\n",
    "        df[column_name] = df[column_name].apply(lambda x: delimiter.join(set(str(x).split(delimiter))))\n",
    "\n",
    "        # Save the DataFrame back to the CSV file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Removed duplicates from {column_name} in {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0de6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_duplicates_and_nulls(df, duplicate_column):\n",
    "    # Identify duplicates in the specified column\n",
    "    duplicates = df.duplicated(subset=duplicate_column, keep=False)\n",
    "    duplicate_df = df[duplicates]\n",
    "\n",
    "    # Print the number of duplicates\n",
    "    print(f\"Number of duplicates in {duplicate_column}: {len(duplicate_df)}\")\n",
    "\n",
    "    # Check for null or zero values in all columns\n",
    "    for column in duplicate_df.columns:\n",
    "        null_count = duplicate_df[column].isnull().sum()\n",
    "        zero_count = (duplicate_df[column] == 0).sum()\n",
    "        print(f\"Number of null values in {column} among duplicates: {null_count}\")\n",
    "        print(f\"Number of zero values in {column} among duplicates: {zero_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50faf576",
   "metadata": {},
   "source": [
    "## Process and Merge Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f85ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title_key_file(folder_path, output_folder_path, specific_files):\n",
    "    # Loop over all files in folder\n",
    "    title_data_set = set()\n",
    "    title_files = {} # Dictionary to keep track of which files each title appears in\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Read in CSV file\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {filename} not found.\")\n",
    "                continue\n",
    "\n",
    "            # Convert all string columns to lowercase\n",
    "            df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "            # Check if file has any columns with 'title' in the name\n",
    "            title_cols = [col for col in df.columns if 'title' in col.lower()]\n",
    "            if len(title_cols) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Check for language columns and filter rows accordingly\n",
    "            language_cols = [col for col in df.columns if 'language' in col.lower() or 'languages' in col.lower()]\n",
    "            for col in language_cols:\n",
    "                if df[col].str.len().max() == 2:\n",
    "                    df = df[df[col] == 'en']\n",
    "                else:\n",
    "                    df = df[df[col] == 'english']\n",
    "\n",
    "            # Check if file has release date columns\n",
    "            release_date_cols = [col for col in df.columns if 'release_date' in col.lower()]\n",
    "            has_release_date = len(release_date_cols) > 0\n",
    "            \n",
    "            print(f\"Duplicate titles in {filename}: {df.duplicated(subset=title_cols).sum()}\")\n",
    "\n",
    "\n",
    "            # Loop through each row in the DataFrame and compare titles\n",
    "            for _, row in df.iterrows():\n",
    "                title = ''.join([str(row[col]) for col in title_cols]).strip()\n",
    "\n",
    "                # Add the file to the list of files for this title\n",
    "                if title not in title_files:\n",
    "                    title_files[title] = [filename]\n",
    "                else:\n",
    "                    title_files[title].append(filename)\n",
    "                    \n",
    "                # Add release date information if available\n",
    "                if has_release_date:\n",
    "                    release_dates = [row[col] for col in release_date_cols if pd.notnull(row[col])]\n",
    "                    if len(release_dates) > 0:\n",
    "                        release_dates = [datetime.strptime(date.split(' ')[0], '%Y-%m-%d').date() for date in release_dates]\n",
    "                        release_dates.sort()\n",
    "                        release_date = release_dates[0]\n",
    "                        release_year = int(release_date.year) # Ensure integer\n",
    "                        release_month = int(release_date.month) # Ensure integer\n",
    "                        release_day = int(release_date.day) # Ensure integer\n",
    "\n",
    "                       # Create a tuple with the title and release date information\n",
    "                        title_info_tuple = (title, release_date, release_year, release_month, release_day)\n",
    "\n",
    "                        # Append the dictionary to the list\n",
    "                        title_data_set.add(title_info_tuple)\n",
    "\n",
    "    # Convert the set to a list of dictionaries and then to a DataFrame\n",
    "    title_data_set = [dict(zip(['title', 'release_date', 'release_year', 'release_month', 'release_day'], tpl)) for tpl in title_data_set]\n",
    "    title_count_df = pd.DataFrame(title_data_set)\n",
    "\n",
    "    # Reset the index and add 'title_id' column\n",
    "    title_count_df.reset_index(drop=True, inplace=True)\n",
    "    title_count_df['title_id'] = title_count_df.index + 1\n",
    "\n",
    "    # Save DataFrame to CSV file\n",
    "    title_count_df.to_csv(os.path.join(output_folder_path, 'title_key.csv'), index=False)\n",
    "    print(f\"Title key file saved as 'title_key.csv'.\")\n",
    "    df = pd.read_csv(os.path.join(output_folder_path, 'title_key.csv'))\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b186e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title_ids_main(input_folder, title_key_path, split_folder, output_folder):\n",
    "    # Read in the title key CSV file as a DataFrame\n",
    "    title_key_df = pd.read_csv(os.path.join(title_key_path, 'title_key.csv'))\n",
    "\n",
    "    # Loop through each file in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv') and 'extra' not in file_name:\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "            # Read in the CSV file as a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Get the title columns\n",
    "            title_cols = [col for col in df.columns if 'title' in col.lower()]\n",
    "\n",
    "            # Check if the file has any title columns\n",
    "            if len(title_cols) == 0:\n",
    "                continue\n",
    "\n",
    "            # Split the file if it has more than 50,000 rows\n",
    "            if len(df) > 25000:\n",
    "                split_files = []\n",
    "                split_num = 1\n",
    "                split_size = 25000\n",
    "                for i in range(0, len(df), split_size):\n",
    "                    split_file_name = f\"{os.path.splitext(file_name)[0]}_split{split_num}.csv\"\n",
    "                    split_num += 1\n",
    "                    split_file_path = os.path.join(split_folder, split_file_name)\n",
    "                    split_files.append(split_file_path)\n",
    "                    df.iloc[i:i+split_size].to_csv(split_file_path, index=False)\n",
    "\n",
    "                # Loop through each split file\n",
    "                for split_file_path in split_files:\n",
    "                    # Read in the CSV file as a DataFrame\n",
    "                    df_split = pd.read_csv(split_file_path)\n",
    "\n",
    "                    # Add a new column for the title ID\n",
    "                    df_split['title_id'] = None\n",
    "\n",
    "                    # Loop through each row in the DataFrame and add the title ID\n",
    "                    for i, row in df_split.iterrows():\n",
    "                        title = ' '.join([str(row[col]) for col in title_cols]).strip()\n",
    "                        title_id = title_key_df.loc[title_key_df['title'] == title, 'title_id'].values\n",
    "\n",
    "                        # If a title ID was found, add it to the DataFrame\n",
    "                        if len(title_id) > 0:\n",
    "                            df_split.at[i, 'title_id'] = title_id[0]\n",
    "                        else:\n",
    "                            # If no title ID was found, drop the row\n",
    "                            df_split.drop(i, inplace=True)\n",
    "\n",
    "                    # Save the updated CSV file to the split folder\n",
    "                    output_file = os.path.join(split_folder, os.path.basename(split_file_path))\n",
    "                    df_split.to_csv(output_file, index=False)\n",
    "                    print(f\"{split_file_path} cleaned and 'title_id' added. On {output_file}\")\n",
    "\n",
    "                # Concatenate the split files and save to the output_folder\n",
    "                split_files_df = [pd.read_csv(split_file) for split_file in split_files]\n",
    "                concat_df = pd.concat(split_files_df)\n",
    "                output_file = os.path.join(output_folder, file_name)\n",
    "                concat_df.to_csv(output_file, index=False)\n",
    "                print(f\"{len(split_files)} files concatenated and saved to {output_file}\")\n",
    "\n",
    "            else:\n",
    "                # Add a new column for the title ID\n",
    "                df['title_id'] = None\n",
    "\n",
    "                # Loop through each row in the DataFrame and add the title ID\n",
    "                for i, row in df.iterrows():\n",
    "                    title = ' '.join([str(row[col]) for col in title_cols]).strip()\n",
    "                    title_id = title_key_df.loc[title_key_df['title'] == title, 'title_id'].values\n",
    "                    # If a title ID was found, add it to the DataFrame\n",
    "                    if len(title_id) > 0:\n",
    "                        df.at[i, 'title_id'] = title_id[0]\n",
    "                    else:\n",
    "                        # If no title ID was found, drop the row\n",
    "                        df.drop(i, inplace=True)\n",
    "                \n",
    "                # Save the updated CSV file to the output path\n",
    "                output_file = os.path.join(output_folder, file_name)\n",
    "                output_file2 = os.path.join(input_folder, file_name)\n",
    "                df.to_csv(output_file, index=False)\n",
    "                #df.to_csv(output_file2, index=False)\n",
    "                print(f\"{file_name} cleaned and 'title_id' added.\",df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbefa5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_title_ids_2(df1_file, df2_file, on_col, folder_path, output_path, compare_path):\n",
    "    # Load the data from df1\n",
    "    df1 = pd.read_csv(os.path.join(compare_path, df1_file))\n",
    "\n",
    "    # Load the data from df2\n",
    "    df2 = pd.read_csv(os.path.join(folder_path, df2_file))\n",
    "    \n",
    "    # Check if on_col exists in df1\n",
    "    if on_col not in df1.columns:\n",
    "        raise ValueError(f\"{on_col} not found in {df1_file}\")\n",
    "    \n",
    "    # Check if on_col exists in df2\n",
    "    if on_col not in df2.columns:\n",
    "        raise ValueError(f\"{on_col} not found in {df2_file}\")\n",
    "    \n",
    "    # Create a new column for the title_id in df2\n",
    "    df2['title_id'] = ''\n",
    "    \n",
    "    # Keep only the rows in df2 that have a matching value in df1[on_col]\n",
    "    df2 = df2[df2[on_col].isin(df1[on_col])]\n",
    "    \n",
    "    # Loop through each row in df2 and add the corresponding title_id from df1\n",
    "    for index, row in df2.iterrows():\n",
    "        title_id = df1.loc[df1[on_col] == row[on_col], 'title_id'].iloc[0]\n",
    "        df2.loc[index, 'title_id'] = title_id\n",
    "    \n",
    "    # Write the updated df2 to a new CSV file in the specified output path\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    output_file = os.path.join(output_path, df2_file)\n",
    "    df2.to_csv(output_file, index=False)\n",
    "    print(f\"{df2_file} cleaned and 'title_id' added to {output_path}\")\n",
    "    \n",
    "    # Return the updated df2\n",
    "    return df2_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f5c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_files, column_names, target_column_name, delimiters, output_folder, output_file, merge_columns, remove_quotes=False, numeric=False, num_files=None):\n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    # Process the specified number of files or all files if num_files is None\n",
    "    for file_index, file_path in enumerate(input_files[:num_files]):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Read file {file_path}:\")\n",
    "        \n",
    "        # Remove leading and trailing spaces from all string columns\n",
    "        for col in df.select_dtypes(['object']).columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "        # Rename the specific column to the target_column_name\n",
    "        df.rename(columns={column_names[file_index]: target_column_name}, inplace=True)\n",
    "\n",
    "        # Get the specified delimiter for this file\n",
    "        delimiter = delimiters[file_index]\n",
    "\n",
    "        # Split and explode the column if a delimiter is specified\n",
    "        if delimiter:\n",
    "            df[target_column_name] = df[target_column_name].str.split(delimiter)\n",
    "            df = df.explode(target_column_name)\n",
    "            print(f\"Processed DataFrame for file {file_path}:\")\n",
    "\n",
    "        # Remove single or double quotes if specified\n",
    "        if remove_quotes:\n",
    "            df[target_column_name] = df[target_column_name].str.replace('\"', '').str.replace(\"'\", \"\")\n",
    "\n",
    "        # Concatenate with the result DataFrame\n",
    "        result_df = pd.concat([result_df, df[['title_id', target_column_name]]])\n",
    "        print(f\"Concatenated result DataFrame after file {file_path}:\")\n",
    "\n",
    "    # Handle missing values\n",
    "    result_df.dropna(subset=[target_column_name], inplace=True)\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert to numeric if specified\n",
    "    if numeric:\n",
    "        result_df[target_column_name] = result_df[target_column_name].astype(float)\n",
    "\n",
    "    # Group by the specified column and aggregate title_ids\n",
    "    unique_df = result_df.groupby(target_column_name)['title_id'].apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "    unique_df.columns = [target_column_name, merge_columns]\n",
    "\n",
    "    # Save to the specified output file\n",
    "    file_path = os.path.join(output_folder, output_file)\n",
    "    unique_df.to_csv(file_path, index=False)\n",
    "\n",
    "    # Add an ID column\n",
    "    unique_df[f'{target_column_name}_id'] = unique_df.index + 1\n",
    "    unique_df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"{output_file} saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05c7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_in_set(title_ids_str):\n",
    "    # Split the string by commas\n",
    "    title_ids_list = title_ids_str.split(',')\n",
    "    \n",
    "    # Convert the list to a set to remove duplicates\n",
    "    title_ids_set = set(title_ids_list)\n",
    "    \n",
    "    return title_ids_set\n",
    "\n",
    "def filter_title_ids(title_ids_str, title_id_key_set):\n",
    "    title_ids_list = title_ids_str[1:-1].replace(\"'\", \"\").split(', ')\n",
    "    matching_title_ids = [title_id for title_id in title_ids_list if title_id in title_id_key_set]\n",
    "    return \"{\" + \", \".join(matching_title_ids) + \"}\"\n",
    "\n",
    "def convert_to_list(title_ids_str):\n",
    "    return list(ast.literal_eval(title_ids_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c5e53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_rows_and_report(folder_path, file_pattern):\n",
    "    # Iterate through the files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(file_pattern):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Read in the CSV file as a DataFrame\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "            # Drop any fully duplicate rows\n",
    "            df.drop_duplicates(inplace=True)\n",
    "\n",
    "            # Find the counts of duplicated title_id and title values\n",
    "            duplicate_counts = df['title_id'].duplicated(keep=False).sum()\n",
    "            \n",
    "            if 'title' in df.columns:\n",
    "                duplicate_counts1 = df['title'].duplicated(keep=False).sum()\n",
    "                print(f\"File: {file_name}, Title duplicates: {duplicate_counts1}\")\n",
    "            else: print(f\"File: {file_name}, does not have a 'title' column.\")\n",
    "            # You can also remove all but the first occurrence of each duplicate title_id if desired\n",
    "            # df.drop_duplicates(subset=['title_id'], keep='first', inplace=True)\n",
    "\n",
    "            # Save the DataFrame back to the file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            \n",
    "            print(f\"File: {file_name}, Title_id duplicates: {duplicate_counts}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "672e848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_combine(main_file, combine_columns_mapping, rename_combine_column, output_file):\n",
    "    # Read only the title_id from the main file\n",
    "    main_df = pd.read_csv(main_file, usecols=['title_id'])\n",
    "    \n",
    "    for file_path, combine_columns in combine_columns_mapping.items():\n",
    "        input_df = pd.read_csv(file_path, usecols=['title_id'] + combine_columns)\n",
    "\n",
    "        # Rename the specific columns to the common name\n",
    "        for column_name in combine_columns:\n",
    "            input_df.rename(columns={column_name: rename_combine_column}, inplace=True)\n",
    "\n",
    "        # Merge only on the specified columns\n",
    "        if rename_combine_column in main_df.columns:\n",
    "            main_df = main_df.merge(input_df[['title_id', rename_combine_column]], on='title_id')\n",
    "            main_df[rename_combine_column] = main_df[[rename_combine_column + '_x', rename_combine_column + '_y']].apply(\n",
    "                lambda row: ','.join(row.dropna().astype(str)), axis=1)\n",
    "            main_df.drop(columns=[rename_combine_column + '_x', rename_combine_column + '_y'], inplace=True)\n",
    "        else:\n",
    "            main_df = pd.merge(main_df, input_df[['title_id', rename_combine_column]], on='title_id')\n",
    "\n",
    "    main_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3821cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_split_date(df, date_column, filename):\n",
    "    # Split date column into separate columns\n",
    "    df[['Start_Date', 'End_Date_ft']] = df['date'].str.split('-', expand=True)\n",
    "    \n",
    "    # Extract year from filename\n",
    "    year = str(filename[-8:-4])\n",
    "\n",
    "    # Clean start date column\n",
    "    df[['Start_Month', 'Start_Day']] = df['Start_Date'].str.split(' ', expand=True)\n",
    "    df['Start_Year'] = year\n",
    "    \n",
    "    # Clean end date column\n",
    "    # if month is present in End_Date\n",
    "    df[['End_Month', 'End_Day']] = df['End_Date_ft'].str.split(' ', expand=True)\n",
    "    \n",
    "    # If End_Day is null, then set End_Month == Start_Month and End_Day == End_Date_ft\n",
    "    if df['End_Day'].isnull().all():\n",
    "        df['End_Month'] = df['Start_Month']\n",
    "        df['End_Day'] = df['End_Date_ft']\n",
    "        df['End_Year'] = year\n",
    "    else:\n",
    "        # If End_Day is not null, set End_Day to the corresponding values in End_Date_ft where it is null\n",
    "        null_mask = df['End_Day'].isnull()\n",
    "        df.loc[null_mask, 'End_Day'] = df.loc[null_mask, 'End_Date_ft']\n",
    "        # Set End_Month to Start_Month for the corresponding rows where End_Day was null\n",
    "        df.loc[null_mask, 'End_Month'] = df.loc[null_mask, 'Start_Month']\n",
    "        df['End_Year'] = year\n",
    "    \n",
    "    # Check if end year should be incremented by 1\n",
    "    if ((df['Start_Month'] == 'Dec') & (df['End_Month'] == 'Jan')).any():\n",
    "        # Only update rows where Start_Month is Dec and End_Month is Jan\n",
    "        mask = (df['Start_Month'] == 'Dec') & (df['End_Month'] == 'Jan')\n",
    "        df.loc[mask, 'End_Year'] = int(year) + 1\n",
    "          \n",
    "    df['End_Year'] = df['End_Year'].astype(str) \n",
    "    \n",
    "    #print('date columns:',df[['End_Month', 'End_Day', 'End_Year', 'Start_Year']].head())\n",
    "    #print('dec and jan:',((df['Start_Month'] == 'Dec') & (df['End_Month'] == 'Jan')).any())\n",
    "    \n",
    "    # Convert date columns to datetime format\n",
    "    df['Start_Date'] = pd.to_datetime(df['Start_Month'] + ' ' + df['Start_Day'] + ' ' + df['Start_Year'])\n",
    "    df['End_Date'] = pd.to_datetime(df['End_Month'] + ' ' + df['End_Day'] + ' ' + df['End_Year'], errors='coerce')\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(columns=['End_Date_ft', 'date'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b52afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_movie_data(folder_path, output_folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {filename} not found.\")\n",
    "                continue\n",
    "\n",
    "        # Call the cleaning functions\n",
    "        df = convert_column_names(df)\n",
    "        df = clean_title_columns(df)\n",
    "        df = convert_string_columns_to_lowercase(df)\n",
    "        \n",
    "        # Save cleaned DataFrame\n",
    "        new_filename = filename\n",
    "        df.to_csv(os.path.join(output_folder_path, new_filename), index=False)\n",
    "        print(f\"{filename} cleaned and saved as {new_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter title IDs\n",
    "def filter_title_ids_2(title_ids_str, valid_title_ids):\n",
    "    title_ids_set = ast.literal_eval(title_ids_str)\n",
    "    return str(set(title_id for title_id in title_ids_set if int(title_id) in valid_title_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614be814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_movie_files(folder_path, output_folder_path, file_type='csv'):\n",
    "    # List all files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(f'.{file_type}')]\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type == 'xlsx':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            return 'Unsupported file type'\n",
    "\n",
    "        # Columns for Movie Revenues, Movie Reviews, and MPAA Ratings\n",
    "        revenue_columns = ['title_id', 'title', 'domestic_box_office', 'international_box_office', 'worldwide_box_office', 'box_office', 'budget', 'revenue']\n",
    "        review_columns = ['title_id', 'title', 'user_rating', 'audience_score', 'tomato_meter', 'review', 'review_date', 'comment_count', 'like_count', 'review_text', 'rt_score_sentiment', 'rt_is_top_critic', 'rt_original_score', 'creation_date', 'rating', 'user_rating', 'vote_average', 'vote_count', 'popularity']\n",
    "        mpaa_columns = ['title_id', 'title', 'mpaa_rating']\n",
    "\n",
    "        # Splitting the DataFrame\n",
    "        revenue_df = df[[col for col in df.columns if col in revenue_columns]]\n",
    "        review_df = df[[col for col in df.columns if col in review_columns]]\n",
    "        mpaa_df = df[[col for col in df.columns if col in mpaa_columns]]\n",
    "\n",
    "        # File names for the split files\n",
    "        base_name = os.path.basename(file_path).split('.')[0]\n",
    "        revenue_file_name = f'{base_name}_revenue.csv'\n",
    "        review_file_name = f'{base_name}_reviews.csv'\n",
    "        # mpaa_file_name = f'{base_name}_mpaa.csv'\n",
    "\n",
    "        # Full path for the output files\n",
    "        revenue_file_path = os.path.join(output_folder_path, revenue_file_name)\n",
    "        review_file_path = os.path.join(output_folder_path, review_file_name)\n",
    "        mpaa_file_path = '/Users/toniwork/Desktop/Capstone/reference/mpaa.csv'\n",
    "\n",
    "        # Check and save the DataFrames based on their columns\n",
    "        if set(revenue_df.columns) not in [{'title'}, {'title_id'}, {'title', 'title_id'}]:\n",
    "            revenue_df.to_csv(revenue_file_path, index=False)\n",
    "        else:\n",
    "            print(f\"Skipping {file} for revenue as it contains insufficient columns.\")\n",
    "\n",
    "        if set(review_df.columns) not in [{'title'}, {'title_id'}, {'title', 'title_id'}]:\n",
    "            review_df.to_csv(review_file_path, index=False)\n",
    "        else:\n",
    "            print(f\"Skipping {file} for reviews as it contains insufficient columns.\")\n",
    "\n",
    "        if set(mpaa_df.columns) not in [{'title'}, {'title_id'}, {'title', 'title_id'}]:\n",
    "            mpaa_df.to_csv(mpaa_file_path, index=False)\n",
    "        else:\n",
    "            print(f\"Skipping {file} for mpaa as it contains insufficient columns.\")\n",
    "\n",
    "    return f'Successfully split the files. Check the output folder: {output_folder_path}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "170.836px",
    "left": "10px",
    "top": "150px",
    "width": "329.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
